#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥çµŒ225å…¨225éŠ˜æŸ„å®Œå…¨ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚’ä½¿ç”¨ã—ã¦æ—¥çµŒ225å…¨225éŠ˜æŸ„ã®10å¹´åˆ†æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ã§é«˜é€Ÿå–å¾—
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from pathlib import Path
import logging
from typing import List, Optional, Dict, Tuple
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

JQUANTS_BASE_URL = "https://api.jquants.com/v1"

class Nikkei225CompleteParallelFetcher:
    """æ—¥çµŒ225å…¨225éŠ˜æŸ„å®Œå…¨ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        self.token_lock = threading.Lock()
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿
        self.company_mapping = self._load_company_mapping()
        logger.info(f"æ—¥çµŒ225å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°éŠ˜æŸ„æ•°: {len(self.company_mapping)}éŠ˜æŸ„")
        
        # ä¸¦åˆ—å®Ÿè¡Œç”¨ã®å…±æœ‰ãƒ‡ãƒ¼ã‚¿
        self.results_queue = Queue()
        self.progress_count = 0
        self.progress_lock = threading.Lock()
    
    def _load_company_mapping(self) -> Dict[str, Dict[str, str]]:
        """æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚’èª­ã¿è¾¼ã¿"""
        try:
            # æœ€æ–°ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            mapping_files = list(Path("docment/ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±").glob("nikkei225_matched_companies_*.csv"))
            if not mapping_files:
                raise FileNotFoundError("ãƒãƒƒãƒ”ãƒ³ã‚°çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            latest_file = max(mapping_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {latest_file}")
            
            df = pd.read_csv(latest_file)
            
            # è¾æ›¸å½¢å¼ã«å¤‰æ›
            mapping = {}
            for _, row in df.iterrows():
                mapping[row['target_code']] = {
                    "api_code": row['api_code'],
                    "target_name": row['target_name'],
                    "api_name": row['api_name'],
                    "match_method": row['match_method']
                }
            
            logger.info(f"æ—¥çµŒ225å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿å®Œäº†: {len(mapping)}éŠ˜æŸ„")
            return mapping
            
        except Exception as e:
            logger.error(f"ãƒãƒƒãƒ”ãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        with self.token_lock:
            if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
                return self.id_token
            
            logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
            time.sleep(1)  # èªè¨¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“éš”èª¿æ•´
            
            try:
                # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
                auth_payload = {
                    "mailaddress": self.mail_address,
                    "password": self.password
                }
                
                resp = requests.post(
                    f"{JQUANTS_BASE_URL}/token/auth_user",
                    data=json.dumps(auth_payload),
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
                
                if resp.status_code == 429:
                    logger.warning("èªè¨¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    return self._get_id_token()
                    
                resp.raise_for_status()
                refresh_token = resp.json().get("refreshToken")
                
                if not refresh_token:
                    raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                time.sleep(1)
                resp = requests.post(
                    f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                    timeout=30
                )
                
                if resp.status_code == 429:
                    logger.warning("èªè¨¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    return self._get_id_token()
                    
                resp.raise_for_status()
                self.id_token = resp.json().get("idToken")
                
                if not self.id_token:
                    raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                self.token_expires_at = datetime.now() + timedelta(hours=1)
                
                logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
                return self.id_token
                
            except Exception as e:
                logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
                raise
    
    def fetch_stock_data_worker(self, company_info: Tuple[str, Dict[str, str]], 
                               from_date: str, to_date: str) -> Optional[pd.DataFrame]:
        """å€‹åˆ¥éŠ˜æŸ„ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼‰"""
        target_code, info = company_info
        api_code = info["api_code"]
        company_name = info["target_name"]
        match_method = info.get("match_method", "ä¸æ˜")
        
        try:
            token = self._get_id_token()
            headers = {"Authorization": f"Bearer {token}"}
            
            url = f"{JQUANTS_BASE_URL}/prices/daily_quotes"
            params = {
                "code": api_code,
                "from": from_date,
                "to": to_date
            }
            
            all_data = []
            pagination_key = None
            
            while True:
                if pagination_key:
                    params["pagination_key"] = pagination_key
                
                try:
                    time.sleep(0.3)  # ä¸¦åˆ—å®Ÿè¡Œç”¨ã®ã‚ˆã‚ŠçŸ­ã„é–“éš”
                    response = requests.get(url, headers=headers, params=params, timeout=30)
                    
                    if response.status_code == 429:
                        logger.warning(f"éŠ˜æŸ„ {target_code}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š30ç§’å¾…æ©Ÿ...")
                        time.sleep(30)
                        continue
                    
                    if response.status_code == 404:
                        logger.warning(f"éŠ˜æŸ„ {target_code}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (API: {api_code})")
                        return None
                    
                    response.raise_for_status()
                    data = response.json()
                    
                    if "daily_quotes" in data:
                        all_data.extend(data["daily_quotes"])
                    
                    pagination_key = data.get("pagination_key")
                    if not pagination_key:
                        break
                        
                except Exception as e:
                    logger.error(f"éŠ˜æŸ„ {target_code} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(2)
                    return None
            
            if all_data:
                df = pd.DataFrame(all_data)
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã¨ä¼æ¥­åã‚’è¿½åŠ 
                df['Code'] = target_code
                df['CompanyName'] = company_name
                df['MatchMethod'] = match_method
                df['ApiCode'] = api_code
                
                # é€²æ—æ›´æ–°
                with self.progress_lock:
                    self.progress_count += 1
                    logger.info(f"âœ… {target_code} ({company_name}): {len(df)}ä»¶å–å¾— [{self.progress_count}/225] - {match_method}")
                
                return df
            
            return None
            
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ {target_code} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—ï¼ˆè­¦å‘Šã‚’æŠ‘åˆ¶ï¼‰"""
        if df.empty:
            return df
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values(['Code', 'Date'])
        
        # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è¨ˆç®—
        result_dfs = []
        for code, group in df.groupby('Code'):
            group = group.sort_values('Date').copy()
            
            # ç§»å‹•å¹³å‡
            group['MA_5'] = group['Close'].rolling(window=5).mean()
            group['MA_20'] = group['Close'].rolling(window=20).mean()
            
            # ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆè­¦å‘ŠæŠ‘åˆ¶ï¼‰
            group['Returns'] = group['Close'].pct_change(fill_method=None)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ20æ—¥é–“ï¼‰
            group['Volatility'] = group['Returns'].rolling(window=20).std()
            
            # RSIè¨ˆç®—
            delta = group['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            group['RSI'] = 100 - (100 / (1 + rs))
            
            result_dfs.append(group)
        
        result = pd.concat(result_dfs, ignore_index=True)
        logger.info(f"ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—å®Œäº†: {len(result)}ä»¶")
        
        return result
    
    def fetch_all_nikkei225_complete_data(self, years: int = 10, max_workers: int = 10) -> pd.DataFrame:
        """æ—¥çµŒ225å…¨225éŠ˜æŸ„ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ã§å–å¾—"""
        if not self.company_mapping:
            logger.error("ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return pd.DataFrame()
        
        # æœŸé–“è¨­å®š
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=years * 365)
        
        from_date = start_date.strftime('%Y-%m-%d')
        to_date = end_date.strftime('%Y-%m-%d')
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“: {from_date} ã€œ {to_date}")
        logger.info(f"ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        logger.info(f"ğŸš€ æ—¥çµŒ225å…¨225éŠ˜æŸ„å®Œå…¨ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—é–‹å§‹!")
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        all_stock_data = []
        company_items = list(self.company_mapping.items())
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å…¨éŠ˜æŸ„ã®ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            futures = {
                executor.submit(self.fetch_stock_data_worker, company_info, from_date, to_date): company_info[0]
                for company_info in company_items
            }
            
            # çµæœã‚’åé›†
            for future in as_completed(futures):
                code = futures[future]
                try:
                    result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    if result is not None and not result.empty:
                        all_stock_data.append(result)
                except Exception as e:
                    logger.error(f"éŠ˜æŸ„ {code} ã®ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        if all_stock_data:
            logger.info("ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†é–‹å§‹...")
            combined_df = pd.concat(all_stock_data, ignore_index=True)
            
            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—
            logger.info("ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—é–‹å§‹...")
            final_df = self.calculate_technical_indicators(combined_df)
            
            # ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/processed/nikkei225_complete_225stocks_{timestamp}.parquet"
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs("data/processed", exist_ok=True)
            final_df.to_parquet(output_file, index=False)
            
            # ãƒãƒƒãƒãƒ³ã‚°çµ±è¨ˆ
            match_stats = final_df.groupby('MatchMethod').size().to_dict()
            
            logger.info(f"ğŸ‰ æ—¥çµŒ225å…¨225éŠ˜æŸ„å®Œå…¨ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†!")
            logger.info(f"æˆåŠŸ: {len(all_stock_data)}/225éŠ˜æŸ„")
            logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(final_df):,}ä»¶")
            logger.info(f"ãƒãƒƒãƒãƒ³ã‚°æ‰‹æ³•åˆ¥çµ±è¨ˆ:")
            for method, count in match_stats.items():
                logger.info(f"  - {method}: {count:,}ä»¶")
            logger.info(f"ä¿å­˜å…ˆ: {output_file}")
            
            return final_df
        else:
            logger.error("ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return pd.DataFrame()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        fetcher = Nikkei225CompleteParallelFetcher()
        
        # æ—¥çµŒ225å…¨225éŠ˜æŸ„å®Œå…¨ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—ï¼ˆ10å¹´é–“ã€10ä¸¦åˆ—ï¼‰
        df = fetcher.fetch_all_nikkei225_complete_data(years=10, max_workers=10)
        
        if not df.empty:
            print(f"\nğŸ‰ æ—¥çµŒ225å…¨225éŠ˜æŸ„å®Œå…¨ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†!")
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
            print(f"  - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}")
            print(f"  - éŠ˜æŸ„æ•°: {df['Code'].nunique()}")
            print(f"  - æœŸé–“: {df['Date'].min()} ã€œ {df['Date'].max()}")
            print(f"  - å¹³å‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°/éŠ˜æŸ„: {len(df)/df['Code'].nunique():.0f}")
            
            # ãƒãƒƒãƒãƒ³ã‚°æ‰‹æ³•åˆ¥çµ±è¨ˆè¡¨ç¤º
            match_stats = df.groupby('MatchMethod').agg({
                'Code': 'nunique',
                'Date': 'count'
            }).rename(columns={'Code': 'éŠ˜æŸ„æ•°', 'Date': 'ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°'})
            print(f"\nğŸ“ˆ ãƒãƒƒãƒãƒ³ã‚°æ‰‹æ³•åˆ¥çµ±è¨ˆ:")
            print(match_stats)
            
        else:
            print("\nâŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()