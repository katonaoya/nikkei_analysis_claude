#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ + ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ« + ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
60%ç²¾åº¦é”æˆã‚’ç›®æŒ‡ã™
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

from jquants_auth import JQuantsAuth
from jquants_fundamental import JQuantsFundamental
from yahoo_market_data import YahooMarketData

class EnhancedDataIntegration:
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_data_file = "data/processed/integrated_with_external.parquet"
        self.output_file = "data/processed/enhanced_integrated_data.parquet"
        
    def load_base_data(self) -> pd.DataFrame:
        """æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.base_data_file).exists():
                df = pd.read_parquet(self.base_data_file)
                logger.success(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}ä»¶")
                
                # ã‚«ãƒ©ãƒ çµ±ä¸€
                if 'date' in df.columns and 'Date' not in df.columns:
                    df['Date'] = pd.to_datetime(df['date'])
                if 'code' in df.columns and 'Stock' not in df.columns:
                    df['Stock'] = df['code'].astype(str)
                
                return df
            else:
                logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.base_data_file}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def get_stock_list(self, base_df: pd.DataFrame, limit: int = 200) -> list:
        """å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆä¸»è¦éŠ˜æŸ„ã«é™å®šï¼‰"""
        if base_df.empty:
            return []
        
        # éŠ˜æŸ„åˆ¥ã®ãƒ‡ãƒ¼ã‚¿é‡ã‚’ç¢ºèª
        stock_counts = base_df['Stock'].value_counts()
        
        # ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã«ã‚ã‚‹éŠ˜æŸ„ã‚’é¸æŠï¼ˆæœ€ä½100æ—¥ä»¥ä¸Šï¼‰
        valid_stocks = stock_counts[stock_counts >= 100].head(limit).index.tolist()
        
        logger.info(f"å¯¾è±¡éŠ˜æŸ„é¸æŠ: {len(valid_stocks)}éŠ˜æŸ„ï¼ˆãƒ‡ãƒ¼ã‚¿ååˆ†ãªéŠ˜æŸ„ã‹ã‚‰é¸æŠï¼‰")
        return valid_stocks
    
    def integrate_fundamental_data(self, base_df: pd.DataFrame, stock_list: list) -> pd.DataFrame:
        """ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        logger.info("ğŸ”„ ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        # J-Quantsèªè¨¼
        auth = JQuantsAuth()
        
        if not auth.test_auth():
            logger.warning("J-Quantsèªè¨¼å¤±æ•—ã€ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ãªã—ã§ç¶™ç¶š")
            return base_df
        
        # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
        fundamental = JQuantsFundamental(auth)
        
        # æœŸé–“è¨­å®šï¼ˆéå»2å¹´ï¼‰
        to_date = datetime.now().strftime('%Y-%m-%d')
        from_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')
        
        try:
            # è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆåˆ¶é™ä»˜ãï¼‰
            limited_stocks = stock_list[:50]  # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦50éŠ˜æŸ„ã«åˆ¶é™
            logger.info(f"ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—å¯¾è±¡: {len(limited_stocks)}éŠ˜æŸ„")
            
            financial_df = fundamental.get_bulk_financial_data(limited_stocks, from_date, to_date)
            
            if financial_df.empty:
                logger.warning("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return base_df
            
            # ç‰¹å¾´é‡ç”Ÿæˆ
            features_df = fundamental.process_fundamental_features(financial_df)
            
            if features_df.empty:
                logger.warning("ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´é‡ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return base_df
            
            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸
            logger.info("ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆä¸­...")
            
            # æ—¥ä»˜ç¯„å›²ã‚’èª¿æ•´ï¼ˆãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯å››åŠæœŸå˜ä½ãªã®ã§å‰æ–¹è£œå®Œï¼‰
            enhanced_df = base_df.copy()
            
            for _, fund_row in features_df.iterrows():
                stock = fund_row['Stock']
                fund_date = fund_row['Date']
                
                # è©²å½“éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                stock_mask = (enhanced_df['Stock'] == stock) & (enhanced_df['Date'] >= fund_date)
                
                if stock_mask.sum() > 0:
                    # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´é‡ã‚’è¿½åŠ 
                    for col in features_df.columns:
                        if col not in ['Stock', 'Date']:
                            enhanced_df.loc[stock_mask, col] = fund_row[col]
            
            # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´é‡ã®å‰æ–¹è£œå®Œ
            fund_cols = [col for col in features_df.columns if col not in ['Stock', 'Date']]
            
            for stock in enhanced_df['Stock'].unique():
                stock_mask = enhanced_df['Stock'] == stock
                enhanced_df.loc[stock_mask, fund_cols] = enhanced_df.loc[stock_mask, fund_cols].fillna(method='ffill')
            
            # æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
            for col in fund_cols:
                if col in enhanced_df.columns:
                    enhanced_df[col] = enhanced_df[col].fillna(enhanced_df[col].median())
            
            logger.success(f"âœ… ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(fund_cols)}ç‰¹å¾´é‡è¿½åŠ ")
            return enhanced_df
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆå¤±æ•—: {e}")
            return base_df
    
    def integrate_market_data(self, base_df: pd.DataFrame) -> pd.DataFrame:
        """ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        logger.info("ğŸ”„ ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        try:
            # Yahoo Financeã‹ã‚‰ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—
            market_data = YahooMarketData()
            data_dict = market_data.get_all_market_data(period="2y")
            
            if not data_dict:
                logger.warning("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return base_df
            
            # ãƒãƒ¼ã‚±ãƒƒãƒˆç‰¹å¾´é‡ç”Ÿæˆ
            market_features = market_data.calculate_market_features(data_dict)
            
            if market_features.empty:
                logger.warning("ãƒãƒ¼ã‚±ãƒƒãƒˆç‰¹å¾´é‡ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return base_df
            
            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸
            logger.info("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆä¸­...")
            
            # æ—¥ä»˜ã§ãƒãƒ¼ã‚¸
            enhanced_df = base_df.merge(market_features, on='Date', how='left')
            
            # å‰æ–¹è£œå®Œã§æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹
            market_cols = [col for col in market_features.columns if col != 'Date']
            enhanced_df[market_cols] = enhanced_df[market_cols].fillna(method='ffill')
            
            # æ®‹ã‚Šã®æ¬ æå€¤ã‚’å¾Œæ–¹è£œå®Œ
            enhanced_df[market_cols] = enhanced_df[market_cols].fillna(method='bfill')
            
            # ãã‚Œã§ã‚‚æ®‹ã‚‹æ¬ æå€¤ã‚’0ã§è£œå®Œ
            enhanced_df[market_cols] = enhanced_df[market_cols].fillna(0)
            
            logger.success(f"âœ… ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(market_cols)}ç‰¹å¾´é‡è¿½åŠ ")
            return enhanced_df
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆå¤±æ•—: {e}")
            return base_df
    
    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        logger.info("ğŸ”§ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        
        enhanced_df = df.copy()
        
        try:
            # 1. ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ« Ã— ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«
            if 'PER' in df.columns and 'RSI' in df.columns:
                enhanced_df['PER_RSI_interaction'] = df['PER'] * (100 - df['RSI']) / 100  # å‰²å®‰ Ã— å£²ã‚‰ã‚Œéã
            
            if 'ROE' in df.columns and 'Price_vs_MA20' in df.columns:
                enhanced_df['ROE_Momentum_interaction'] = df['ROE'] * df['Price_vs_MA20']  # åç›Šæ€§ Ã— ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
            
            # 2. ãƒãƒ¼ã‚±ãƒƒãƒˆ Ã— å€‹åˆ¥æ ª
            if 'nikkei225_return_1d' in df.columns and 'Volume_Ratio' in df.columns:
                enhanced_df['Market_Volume_interaction'] = df['nikkei225_return_1d'] * df['Volume_Ratio']  # å¸‚å ´å‹•å‘ Ã— å‡ºæ¥é«˜
            
            if 'vix_close' in df.columns and 'Volatility' in df.columns:
                enhanced_df['VIX_Stock_Vol_ratio'] = df['vix_close'] / (df['Volatility'] * 100 + 1)  # å¸‚å ´ææ€– / å€‹åˆ¥ãƒœãƒ©
            
            # 3. ã‚»ã‚¯ã‚¿ãƒ¼ç›¸å¯¾å¼·åº¦ï¼ˆä»®æƒ³ï¼‰
            if 'PBR' in df.columns and 'topix_return_1d' in df.columns:
                enhanced_df['Value_Market_sync'] = (1 / (df['PBR'] + 0.1)) * df['topix_return_1d']  # ãƒãƒªãƒ¥ãƒ¼ Ã— å¸‚å ´
            
            # 4. ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚©ãƒ­ãƒ¼å¼·åº¦
            if 'Momentum_5' in df.columns and 'nikkei225_ma20_slope' in df.columns:
                enhanced_df['Trend_Alignment'] = np.sign(df['Momentum_5']) * np.sign(df['nikkei225_ma20_slope'])  # ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è‡´
            
            # 5. ãƒªã‚¹ã‚¯èª¿æ•´ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬
            if 'ROE' in df.columns and 'vix_close' in df.columns:
                enhanced_df['Risk_Adjusted_Quality'] = df['ROE'] * np.exp(-df['vix_close'] / 100)  # å“è³ª Ã— ãƒªã‚¹ã‚¯èª¿æ•´
            
            interaction_cols = len([col for col in enhanced_df.columns if 'interaction' in col.lower() or 'sync' in col.lower() or 'alignment' in col.lower() or 'adjusted' in col.lower()])
            
            logger.success(f"âœ… ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {interaction_cols}ç‰¹å¾´é‡è¿½åŠ ")
            
        except Exception as e:
            logger.warning(f"ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        return enhanced_df
    
    def finalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€çµ‚çš„ãªç‰¹å¾´é‡èª¿æ•´"""
        logger.info("ğŸ¯ æœ€çµ‚ç‰¹å¾´é‡èª¿æ•´ä¸­...")
        
        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        required_base_cols = ['Date', 'Stock', 'close', 'high', 'low', 'open', 'volume']
        missing_cols = [col for col in required_base_cols if col not in df.columns]
        
        if missing_cols:
            logger.error(f"å¿…è¦ãªåŸºæœ¬åˆ—ãŒä¸è¶³: {missing_cols}")
            return df
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ç”Ÿæˆ
        df = df.sort_values(['Stock', 'Date'])
        
        # ç¿Œæ—¥ã®é«˜å€¤ãŒå½“æ—¥çµ‚å€¤ã‚ˆã‚Š1%ä»¥ä¸Šé«˜ã„å ´åˆã‚’äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        df['next_high'] = df.groupby('Stock')['high'].shift(-1)
        df['Target'] = (df['next_high'] > df['close'] * 1.01).astype(int)
        
        # ç•°å¸¸å€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if col not in ['Target', 'Date']:
                # 99%ã¨1%ã§ç•°å¸¸å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—
                q99 = df[col].quantile(0.99)
                q01 = df[col].quantile(0.01)
                df[col] = df[col].clip(q01, q99)
        
        # ç„¡é™å¤§å€¤ã‚’NaNã«å¤‰æ›
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # æœ€çµ‚çš„ãªæ¬ æå€¤å‡¦ç†
        df = df.fillna(method='ffill').fillna(0)
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        total_features = len([col for col in df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']]) 
        target_count = df['Target'].sum()
        target_rate = target_count / len(df) * 100
        
        logger.success(f"âœ… æœ€çµ‚ç‰¹å¾´é‡èª¿æ•´å®Œäº†:")
        logger.info(f"  ç·ç‰¹å¾´é‡æ•°: {total_features}")
        logger.info(f"  ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}")
        logger.info(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé™½æ€§ç‡: {target_rate:.2f}%")
        
        return df
    
    def run_integration(self) -> pd.DataFrame:
        """çµ±åˆå‡¦ç†å®Ÿè¡Œ"""
        logger.info("ğŸš€ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
        
        # 1. ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        base_df = self.load_base_data()
        if base_df.empty:
            logger.error("ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
            return pd.DataFrame()
        
        # 2. å¯¾è±¡éŠ˜æŸ„é¸æŠ
        stock_list = self.get_stock_list(base_df)
        if not stock_list:
            logger.error("å¯¾è±¡éŠ˜æŸ„ãŒé¸æŠã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¯¾è±¡éŠ˜æŸ„ã«é™å®š
        base_df = base_df[base_df['Stock'].isin(stock_list)].copy()
        
        # 3. ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        enhanced_df = self.integrate_fundamental_data(base_df, stock_list)
        
        # 4. ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆ
        enhanced_df = self.integrate_market_data(enhanced_df)
        
        # 5. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆ
        enhanced_df = self.create_interaction_features(enhanced_df)
        
        # 6. æœ€çµ‚èª¿æ•´
        final_df = self.finalize_features(enhanced_df)
        
        if not final_df.empty:
            # ä¿å­˜
            try:
                final_df.to_parquet(self.output_file)
                logger.success(f"âœ… æ‹¡å¼µçµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {self.output_file}")
                
                # çµ±è¨ˆã‚µãƒãƒªãƒ¼
                feature_cols = [col for col in final_df.columns if col not in ['Date', 'Stock', 'Target']]
                logger.info("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
                logger.info(f"  æœŸé–“: {final_df['Date'].min()} ï½ {final_df['Date'].max()}")
                logger.info(f"  éŠ˜æŸ„æ•°: {final_df['Stock'].nunique()}")
                logger.info(f"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
                logger.info(f"  ãƒ‡ãƒ¼ã‚¿å“è³ª: {(1 - final_df.isnull().sum().sum() / (len(final_df) * len(final_df.columns))) * 100:.1f}%")
                
            except Exception as e:
                logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: {e}")
        
        return final_df

# å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    integrator = EnhancedDataIntegration()
    enhanced_data = integrator.run_integration()
    
    if not enhanced_data.empty:
        logger.success("ğŸ‰ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†ï¼60%ç²¾åº¦å‘ä¸Šã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ")
    else:
        logger.error("âš ï¸ çµ±åˆå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")