#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Yahoo Finance 10å¹´åˆ†æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆUSD/JPY, VIX, TOPIX, æ—¥çµŒ225ç­‰ï¼‰ã‚’10å¹´é–“åˆ†å–å¾—
"""

import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import os
from pathlib import Path
import time

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class YahooFinanceExtendedFetcher:
    """Yahoo Finance 10å¹´åˆ†æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, start_date: str = None, end_date: str = None):
        """åˆæœŸåŒ–"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“è¨­å®šï¼ˆ10å¹´é–“ï¼‰
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=10*365)).strftime('%Y-%m-%d')
        
        self.start_date = start_date
        self.end_date = end_date
        
        # å¤–éƒ¨æŒ‡æ¨™ã‚·ãƒ³ãƒœãƒ«å®šç¾©
        self.symbols = {
            'usdjpy': 'USDJPY=X',      # USD/JPY
            'vix': '^VIX',             # VIXææ€–æŒ‡æ•°
            'nikkei225': '^N225',      # æ—¥çµŒ225æŒ‡æ•°
            'topix': '^TOPX',          # TOPIXæŒ‡æ•°
            'sp500': '^GSPC',          # S&P500
            'nasdaq': '^IXIC',         # NASDAQ
            'dxy': 'DX-Y.NYB',         # ãƒ‰ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            'gold': 'GC=F',            # é‡‘å…ˆç‰©
            'crude_oil': 'CL=F',       # åŸæ²¹å…ˆç‰©
            'jgb_10y': '^TNX'          # 10å¹´å‚µåˆ©å›ã‚Šï¼ˆä»£æ›¿ï¼‰
        }
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("data/external_extended")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“: {self.start_date} ã€œ {self.end_date}")
        logger.info(f"å¯¾è±¡æŒ‡æ¨™æ•°: {len(self.symbols)}å€‹")
    
    def fetch_symbol_data(self, symbol: str, name: str) -> pd.DataFrame:
        """å€‹åˆ¥ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        logger.info(f"å–å¾—é–‹å§‹: {name} ({symbol})")
        
        try:
            # Yahoo Financeã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            ticker = yf.Ticker(symbol)
            data = ticker.history(start=self.start_date, end=self.end_date, auto_adjust=True)
            
            if data.empty:
                logger.warning(f"{name}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return pd.DataFrame()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’Dateã‚«ãƒ©ãƒ ã«å¤‰æ›
            data = data.reset_index()
            data['Symbol'] = symbol
            data['Name'] = name
            
            # åŸºæœ¬çš„ãªæŠ€è¡“æŒ‡æ¨™è¨ˆç®—
            data['Daily_Return'] = data['Close'].pct_change()
            data['Log_Return'] = np.log(data['Close'] / data['Close'].shift(1))
            
            # ç§»å‹•å¹³å‡
            data['MA_5'] = data['Close'].rolling(5).mean()
            data['MA_20'] = data['Close'].rolling(20).mean()
            data['MA_60'] = data['Close'].rolling(60).mean()
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            data['Volatility_5'] = data['Daily_Return'].rolling(5).std()
            data['Volatility_20'] = data['Daily_Return'].rolling(20).std()
            
            # ç§»å‹•å¹³å‡ã‹ã‚‰ã®ä¹–é›¢ç‡
            data['MA20_Deviation'] = (data['Close'] - data['MA_20']) / data['MA_20']
            
            # VIXç‰¹æœ‰ã®æŒ‡æ¨™
            if 'VIX' in symbol:
                data['VIX_Spike'] = (data['Close'] > data['MA_20'] * 1.5).astype(int)
                data['VIX_High'] = (data['Close'] > 30).astype(int)
            
            # USD/JPYç‰¹æœ‰ã®æŒ‡æ¨™
            if 'USDJPY' in symbol:
                data['USDJPY_Trend'] = np.where(data['Close'] > data['MA_20'], 1, 
                                               np.where(data['Close'] < data['MA_20'], -1, 0))
            
            logger.info(f"{name}: {len(data)}ä»¶å–å¾—å®Œäº†")
            return data
            
        except Exception as e:
            logger.error(f"{name} ({symbol}) å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def fetch_all_external_data(self) -> dict:
        """å…¨å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        logger.info("ğŸš€ Yahoo Finance 10å¹´åˆ†å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹!")
        
        all_data = {}
        
        for name, symbol in self.symbols.items():
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(1)
            
            data = self.fetch_symbol_data(symbol, name)
            if not data.empty:
                all_data[name] = data
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = self.output_dir / f"{name}_10years_{timestamp}.parquet"
                data.to_parquet(output_file, index=False)
                logger.info(f"ä¿å­˜å®Œäº†: {output_file}")
        
        return all_data
    
    def create_integrated_dataset(self, all_data: dict) -> pd.DataFrame:
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
        logger.info("çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹...")
        
        # åŸºæº–æ—¥ä»˜ã‚’æ—¥çµŒ225ã‹ã‚‰å–å¾—
        if 'nikkei225' in all_data and not all_data['nikkei225'].empty:
            base_dates = all_data['nikkei225'][['Date']].copy()
            base_dates = base_dates.sort_values('Date')
        else:
            logger.error("æ—¥çµŒ225ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚çµ±åˆã§ãã¾ã›ã‚“")
            return pd.DataFrame()
        
        logger.info(f"åŸºæº–æ—¥æ•°: {len(base_dates)}æ—¥é–“")
        
        # å„æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        integrated_df = base_dates.copy()
        
        for name, data in all_data.items():
            if data.empty:
                continue
            
            # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãƒªãƒãƒ¼ãƒ 
            merge_cols = ['Date', 'Close', 'Daily_Return', 'Volatility_20', 'MA_20', 'MA20_Deviation']
            
            # ç‰¹æ®ŠæŒ‡æ¨™ã®è¿½åŠ 
            if 'VIX' in name:
                merge_cols.extend(['VIX_Spike', 'VIX_High'])
            elif 'usdjpy' in name:
                merge_cols.extend(['USDJPY_Trend'])
            
            # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿é¸æŠ
            available_cols = [col for col in merge_cols if col in data.columns]
            merge_data = data[available_cols].copy()
            
            # ã‚«ãƒ©ãƒ åã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ ï¼ˆDateã¯é™¤ãï¼‰
            rename_dict = {col: f"{name}_{col}" for col in available_cols if col != 'Date'}
            merge_data = merge_data.rename(columns=rename_dict)
            
            # ãƒãƒ¼ã‚¸
            integrated_df = pd.merge(integrated_df, merge_data, on='Date', how='left')
            logger.info(f"{name}: ãƒãƒ¼ã‚¸å®Œäº† ({len(merge_data)}æ—¥é–“)")
        
        # å‰æ–¹è£œå®Œã§æ¬ æå€¤å‡¦ç†
        integrated_df = integrated_df.fillna(method='ffill')
        integrated_df = integrated_df.fillna(method='bfill')
        
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(integrated_df)}ä»¶, {len(integrated_df.columns)}ã‚«ãƒ©ãƒ ")
        
        return integrated_df
    
    def save_integrated_data(self, integrated_df: pd.DataFrame):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Parquetå½¢å¼ã§ä¿å­˜
        parquet_file = self.output_dir / f"external_integrated_10years_{timestamp}.parquet"
        integrated_df.to_parquet(parquet_file, index=False)
        
        # CSVå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆç¢ºèªç”¨ï¼‰
        csv_file = self.output_dir / f"external_integrated_10years_{timestamp}.csv"
        integrated_df.to_csv(csv_file, index=False)
        
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  Parquet: {parquet_file}")
        logger.info(f"  CSV: {csv_file}")
        
        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆæƒ…å ±
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        logger.info(f"  æœŸé–“: {integrated_df['Date'].min()} ã€œ {integrated_df['Date'].max()}")
        logger.info(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(integrated_df):,}ä»¶")
        logger.info(f"  ã‚«ãƒ©ãƒ æ•°: {len(integrated_df.columns)}å€‹")
        logger.info(f"  æ¬ æå€¤: {integrated_df.isnull().sum().sum()}å€‹")
        
        return parquet_file, csv_file
    
    def run_extended_fetch(self):
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—å®Ÿè¡Œ"""
        logger.info("ğŸ“Š Yahoo Finance 10å¹´åˆ†æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—å®Ÿè¡Œé–‹å§‹!")
        
        try:
            # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
            all_data = self.fetch_all_external_data()
            
            if not all_data:
                logger.error("ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            integrated_df = self.create_integrated_dataset(all_data)
            
            if integrated_df.empty:
                logger.error("çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            # ä¿å­˜
            parquet_file, csv_file = self.save_integrated_data(integrated_df)
            
            # æˆåŠŸçµ±è¨ˆ
            success_count = len([k for k, v in all_data.items() if not v.empty])
            total_count = len(self.symbols)
            
            logger.info(f"ğŸ‰ Yahoo Finance 10å¹´åˆ†æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†!")
            logger.info(f"æˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
            logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(integrated_df):,}ä»¶")
            logger.info(f"ä¿å­˜å…ˆ: {parquet_file}")
            
            return {
                'all_data': all_data,
                'integrated_data': integrated_df,
                'files': {'parquet': parquet_file, 'csv': csv_file},
                'success_rate': success_count / total_count,
                'summary': {
                    'total_records': len(integrated_df),
                    'date_range': f"{integrated_df['Date'].min()} - {integrated_df['Date'].max()}",
                    'columns': len(integrated_df.columns),
                    'symbols_success': success_count,
                    'symbols_total': total_count
                }
            }
            
        except Exception as e:
            logger.error(f"æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # 10å¹´é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    fetcher = YahooFinanceExtendedFetcher()
    
    results = fetcher.run_extended_fetch()
    
    if results:
        print(f"\nâœ… Yahoo Finance 10å¹´åˆ†ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†!")
        print(f"ğŸ“Š å–å¾—çµ±è¨ˆ:")
        print(f"  - æˆåŠŸç‡: {results['success_rate']:.1%}")
        print(f"  - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {results['summary']['total_records']:,}ä»¶")
        print(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {results['summary']['date_range']}")
        print(f"  - ã‚«ãƒ©ãƒ æ•°: {results['summary']['columns']}å€‹")
        print(f"  - æˆåŠŸã‚·ãƒ³ãƒœãƒ«: {results['summary']['symbols_success']}/{results['summary']['symbols_total']}")
        print(f"\nğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - Parquet: {results['files']['parquet']}")
        print(f"  - CSV: {results['files']['csv']}")
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()