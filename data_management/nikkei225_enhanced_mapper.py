#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥çµŒ225æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
J-Quants APIã‹ã‚‰å–å¾—ã—ãŸå…¨ä¸Šå ´éŠ˜æŸ„ã¨æ—¥çµŒ225éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’é«˜ç²¾åº¦ã§ãƒãƒƒãƒ”ãƒ³ã‚°
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from pathlib import Path
import logging
from typing import List, Optional, Dict, Tuple
from dotenv import load_dotenv
import re
import threading
from difflib import SequenceMatcher

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

JQUANTS_BASE_URL = "https://api.jquants.com/v1"

class EnhancedNikkei225Mapper:
    """æ—¥çµŒ225æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        self.token_lock = threading.Lock()
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        # æ—¥çµŒ225éŠ˜æŸ„ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
        self.target_companies = self._load_target_companies()
        logger.info(f"æ—¥çµŒ225ã‚¿ãƒ¼ã‚²ãƒƒãƒˆéŠ˜æŸ„æ•°: {len(self.target_companies)}éŠ˜æŸ„")
    
    def _load_target_companies(self) -> pd.DataFrame:
        """æ—¥çµŒ225ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­èª­ã¿è¾¼ã¿"""
        try:
            df = pd.read_csv('/Users/naoya/Desktop/AIé–¢ä¿‚/è‡ªå‹•å£²è²·ãƒ„ãƒ¼ãƒ«/claude_code_develop/docment/ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±/nikkei225_4digit_list.csv')
            logger.info(f"æ—¥çµŒ225ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}éŠ˜æŸ„")
            return df
        except Exception as e:
            logger.error(f"æ—¥çµŒ225ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        with self.token_lock:
            if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
                return self.id_token
            
            logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
            time.sleep(1)
            
            try:
                # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
                auth_payload = {
                    "mailaddress": self.mail_address,
                    "password": self.password
                }
                
                resp = requests.post(
                    f"{JQUANTS_BASE_URL}/token/auth_user",
                    data=json.dumps(auth_payload),
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
                
                if resp.status_code == 429:
                    logger.warning("èªè¨¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    return self._get_id_token()
                    
                resp.raise_for_status()
                refresh_token = resp.json().get("refreshToken")
                
                if not refresh_token:
                    raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                time.sleep(1)
                resp = requests.post(
                    f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                    timeout=30
                )
                
                if resp.status_code == 429:
                    logger.warning("èªè¨¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    return self._get_id_token()
                    
                resp.raise_for_status()
                self.id_token = resp.json().get("idToken")
                
                if not self.id_token:
                    raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                self.token_expires_at = datetime.now() + timedelta(hours=1)
                
                logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
                return self.id_token
                
            except Exception as e:
                logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
                raise
    
    def get_all_listed_companies(self) -> pd.DataFrame:
        """APIä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‚’å…¨ã¦å–å¾—"""
        token = self._get_id_token()
        headers = {"Authorization": f"Bearer {token}"}
        url = f"{JQUANTS_BASE_URL}/listed/info"
        
        all_companies = []
        pagination_key = None
        
        logger.info("å…¨ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‚’å–å¾—ä¸­...")
        
        while True:
            params = {}
            if pagination_key:
                params["pagination_key"] = pagination_key
            
            try:
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                response = requests.get(url, headers=headers, params=params, timeout=30)
                
                if response.status_code == 429:
                    logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    continue
                
                response.raise_for_status()
                data = response.json()
                
                if "info" in data:
                    for company in data["info"]:
                        all_companies.append({
                            "api_code": company.get("Code", ""),
                            "company_name": company.get("CompanyName", ""),
                            "sector": company.get("Sector33Code", ""),
                            "sector_name": company.get("Sector33Name", "")
                        })
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                pagination_key = data.get("pagination_key")
                if not pagination_key:
                    break
                    
            except Exception as e:
                logger.error(f"ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(5)
                continue
        
        df = pd.DataFrame(all_companies)
        logger.info(f"APIä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—å®Œäº†: {len(df)}éŠ˜æŸ„")
        return df
    
    def normalize_company_name(self, name: str) -> str:
        """ä¼æ¥­åæ­£è¦åŒ–ï¼ˆæ ªå¼è¡¨è¨˜é™¤å»ãªã©ï¼‰"""
        if pd.isna(name) or name == "":
            return ""
        
        # æ ªå¼ä¼šç¤¾è¡¨è¨˜ã®é™¤å»
        patterns = [
            r'ï¼ˆæ ªï¼‰', r'\(æ ª\)', r'æ ªå¼ä¼šç¤¾', r'\(æ ªå¼ä¼šç¤¾\)', r'ï¼ˆæ ªå¼ä¼šç¤¾ï¼‰',
            r'ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹', r'ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°', r'HD', r'Hd',
            r'\s+', r'ã€€+',  # ç©ºç™½ã®æ­£è¦åŒ–
        ]
        
        normalized = name
        for pattern in patterns:
            normalized = re.sub(pattern, '', normalized)
        
        return normalized.strip()
    
    def similarity_score(self, text1: str, text2: str) -> float:
        """æ–‡å­—åˆ—é¡ä¼¼åº¦è¨ˆç®—"""
        if not text1 or not text2:
            return 0.0
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def enhanced_mapping(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        logger.info("æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†é–‹å§‹...")
        
        # APIä¸Šå ´éŠ˜æŸ„å–å¾—
        api_companies = self.get_all_listed_companies()
        
        matched_companies = []
        match_details = []
        
        # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­ã«å¯¾ã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        for idx, target_row in self.target_companies.iterrows():
            target_code = str(target_row['code'])
            target_name = str(target_row['company'])
            target_name_norm = self.normalize_company_name(target_name)
            
            matched = False
            best_match = None
            best_score = 0.0
            match_method = ""
            
            # ãƒ•ã‚§ãƒ¼ã‚º1: ç›´æ¥ã‚³ãƒ¼ãƒ‰ä¸€è‡´ï¼ˆ5æ¡ï¼‰
            direct_match = api_companies[api_companies['api_code'] == target_code]
            if not direct_match.empty:
                best_match = direct_match.iloc[0]
                match_method = "ç›´æ¥ã‚³ãƒ¼ãƒ‰ä¸€è‡´(5æ¡)"
                matched = True
                logger.info(f"âœ… ç›´æ¥ä¸€è‡´: {target_code} -> {best_match['company_name']}")
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: 4æ¡ã‚³ãƒ¼ãƒ‰ä¸€è‡´ï¼ˆæœ«å°¾0è¿½åŠ ï¼‰
            if not matched:
                code_with_zero = target_code + "0"
                zero_match = api_companies[api_companies['api_code'] == code_with_zero]
                if not zero_match.empty:
                    best_match = zero_match.iloc[0]
                    match_method = "4æ¡+0ãƒ‘ã‚¿ãƒ¼ãƒ³"
                    matched = True
                    logger.info(f"âœ… 4æ¡+0ä¸€è‡´: {target_code} -> {code_with_zero} -> {best_match['company_name']}")
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ï¼ˆå‰æ–¹ä¸€è‡´ãƒ»å¾Œæ–¹ä¸€è‡´ï¼‰
            if not matched:
                # å‰æ–¹ä¸€è‡´
                prefix_matches = api_companies[api_companies['api_code'].str.startswith(target_code)]
                if not prefix_matches.empty:
                    best_match = prefix_matches.iloc[0]
                    match_method = f"å‰æ–¹ä¸€è‡´({target_code}*)"
                    matched = True
                    logger.info(f"âœ… å‰æ–¹ä¸€è‡´: {target_code} -> {best_match['api_code']} -> {best_match['company_name']}")
                
                # å¾Œæ–¹ä¸€è‡´
                if not matched:
                    suffix_matches = api_companies[api_companies['api_code'].str.endswith(target_code)]
                    if not suffix_matches.empty:
                        best_match = suffix_matches.iloc[0]
                        match_method = f"å¾Œæ–¹ä¸€è‡´(*{target_code})"
                        matched = True
                        logger.info(f"âœ… å¾Œæ–¹ä¸€è‡´: {target_code} -> {best_match['api_code']} -> {best_match['company_name']}")
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: ä¼æ¥­åã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°
            if not matched:
                max_similarity = 0.0
                best_name_match = None
                
                for _, api_row in api_companies.iterrows():
                    api_name_norm = self.normalize_company_name(api_row['company_name'])
                    
                    # å®Œå…¨ä¸€è‡´
                    if target_name_norm == api_name_norm and target_name_norm != "":
                        best_name_match = api_row
                        max_similarity = 1.0
                        break
                    
                    # éƒ¨åˆ†ä¸€è‡´ï¼ˆå«æœ‰é–¢ä¿‚ï¼‰
                    if target_name_norm in api_name_norm or api_name_norm in target_name_norm:
                        if target_name_norm != "" and api_name_norm != "":
                            similarity = self.similarity_score(target_name_norm, api_name_norm)
                            if similarity > max_similarity:
                                max_similarity = similarity
                                best_name_match = api_row
                    
                    # é¡ä¼¼åº¦è¨ˆç®—
                    similarity = self.similarity_score(target_name_norm, api_name_norm)
                    if similarity > max_similarity and similarity > 0.7:
                        max_similarity = similarity
                        best_name_match = api_row
                
                if best_name_match is not None and max_similarity > 0.7:
                    best_match = best_name_match
                    match_method = f"ä¼æ¥­åä¸€è‡´(é¡ä¼¼åº¦:{max_similarity:.2f})"
                    matched = True
                    logger.info(f"âœ… ä¼æ¥­åä¸€è‡´: {target_name_norm} -> {self.normalize_company_name(best_match['company_name'])} (é¡ä¼¼åº¦:{max_similarity:.2f})")
            
            # çµæœè¨˜éŒ²
            if matched and best_match is not None:
                matched_companies.append({
                    "target_code": target_code,
                    "target_name": target_name,
                    "api_code": best_match['api_code'],
                    "api_name": best_match['company_name'],
                    "sector": best_match.get('sector_name', ''),
                    "match_method": match_method,
                    "similarity_score": best_score
                })
            
            # ãƒãƒƒãƒãƒ³ã‚°è©³ç´°è¨˜éŒ²
            match_details.append({
                "target_code": target_code,
                "target_name": target_name,
                "matched": matched,
                "match_method": match_method if matched else "æœªãƒãƒƒãƒ",
                "api_code": best_match['api_code'] if matched else "",
                "api_name": best_match['company_name'] if matched else ""
            })
        
        # çµæœã‚’DataFrameã«å¤‰æ›
        matched_df = pd.DataFrame(matched_companies)
        details_df = pd.DataFrame(match_details)
        
        # å–å¾—ä¸å¯ä¼æ¥­ãƒªã‚¹ãƒˆ
        unmatched_df = details_df[details_df['matched'] == False].copy()
        
        logger.info(f"ğŸ‰ æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†!")
        logger.info(f"ãƒãƒƒãƒã—ãŸéŠ˜æŸ„æ•°: {len(matched_df)}/{len(self.target_companies)} ({len(matched_df)/len(self.target_companies)*100:.1f}%)")
        
        return matched_df, unmatched_df, api_companies
    
    def save_mapping_results(self, matched_df: pd.DataFrame, unmatched_df: pd.DataFrame, api_companies: pd.DataFrame):
        """ãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # docment/ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("docment/ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒãƒƒãƒã—ãŸä¼æ¥­ä¸€è¦§
        matched_file = output_dir / f"nikkei225_matched_companies_{timestamp}.csv"
        matched_df.to_csv(matched_file, index=False, encoding='utf-8-sig')
        logger.info(f"ãƒãƒƒãƒã—ãŸä¼æ¥­ä¸€è¦§ä¿å­˜: {matched_file}")
        
        # å–å¾—ã§ããªã„ä¼æ¥­ä¸€è¦§
        unmatched_file = output_dir / f"nikkei225_unmatched_companies_{timestamp}.csv"
        unmatched_df.to_csv(unmatched_file, index=False, encoding='utf-8-sig')
        logger.info(f"å–å¾—ã§ããªã„ä¼æ¥­ä¸€è¦§ä¿å­˜: {unmatched_file}")
        
        # APIå…¨ä¼æ¥­ä¸€è¦§
        api_file = output_dir / f"jquants_all_companies_{timestamp}.csv"
        api_companies.to_csv(api_file, index=False, encoding='utf-8-sig')
        logger.info(f"APIå…¨ä¼æ¥­ä¸€è¦§ä¿å­˜: {api_file}")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°çµ±è¨ˆ
        stats = {
            "ç·ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­æ•°": len(self.target_companies),
            "ãƒãƒƒãƒã—ãŸä¼æ¥­æ•°": len(matched_df),
            "å–å¾—ã§ããªã„ä¼æ¥­æ•°": len(unmatched_df),
            "ãƒãƒƒãƒç‡": f"{len(matched_df)/len(self.target_companies)*100:.1f}%",
            "APIç·ä¼æ¥­æ•°": len(api_companies)
        }
        
        stats_file = output_dir / f"mapping_statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ãƒãƒƒãƒ”ãƒ³ã‚°çµ±è¨ˆä¿å­˜: {stats_file}")
        
        return matched_file, unmatched_file, api_file, stats

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        mapper = EnhancedNikkei225Mapper()
        
        # æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        matched_df, unmatched_df, api_companies = mapper.enhanced_mapping()
        
        # çµæœä¿å­˜
        matched_file, unmatched_file, api_file, stats = mapper.save_mapping_results(
            matched_df, unmatched_df, api_companies
        )
        
        print(f"\nâœ… æ‹¡å¼µãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†å®Œäº†")
        print(f"ğŸ“Š ãƒãƒƒãƒ”ãƒ³ã‚°çµæœ:")
        print(f"  - ç·ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­æ•°: {stats['ç·ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­æ•°']}")
        print(f"  - ãƒãƒƒãƒã—ãŸä¼æ¥­æ•°: {stats['ãƒãƒƒãƒã—ãŸä¼æ¥­æ•°']}")
        print(f"  - å–å¾—ã§ããªã„ä¼æ¥­æ•°: {stats['å–å¾—ã§ããªã„ä¼æ¥­æ•°']}")
        print(f"  - ãƒãƒƒãƒç‡: {stats['ãƒãƒƒãƒç‡']}")
        print(f"  - APIç·ä¼æ¥­æ•°: {stats['APIç·ä¼æ¥­æ•°']}")
        print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - ãƒãƒƒãƒä¼æ¥­: {matched_file}")
        print(f"  - æœªãƒãƒƒãƒä¼æ¥­: {unmatched_file}")
        print(f"  - APIå…¨ä¼æ¥­: {api_file}")
        
    except Exception as e:
        logger.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()