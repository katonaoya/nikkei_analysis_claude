#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥çµŒ225å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé«˜ç²¾åº¦å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ542,143ä»¶ï¼‰ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ãƒ»æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import os
import joblib
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# MLé–¢é€£
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from lightgbm import LGBMClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Nikkei225CompletePrecisionTrainer:
    """æ—¥çµŒ225å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé«˜ç²¾åº¦å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, data_file: str = None):
        """åˆæœŸåŒ–"""
        # æœ€æ–°ã®æ—¥çµŒ225å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
        if data_file is None:
            data_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
        
        self.data_file = data_file
        self.df = None
        self.model = None
        self.scaler = None
        self.selector = None
        self.feature_cols = None
        
        # 95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.model_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 300,           # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨
            'max_depth': 8,                # è¤‡é›‘ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            'min_child_samples': 30,       # éå­¦ç¿’é˜²æ­¢
            'subsample': 0.8,              # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            'colsample_bytree': 0.8,       # ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            'learning_rate': 0.03,         # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨
            'reg_alpha': 0.1,              # L1æ­£å‰‡åŒ–
            'reg_lambda': 0.1,             # L2æ­£å‰‡åŒ–
            'random_state': 42,            # å†ç¾æ€§ç¢ºä¿
            'verbose': -1                  # ãƒ­ã‚°æŠ‘åˆ¶
        }
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file}")
    
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        logger.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        try:
            self.df = pd.read_parquet(self.data_file)
            logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df):,}ä»¶, {self.df['Code'].nunique()}éŠ˜æŸ„")
            
            # æ—¥ä»˜å‹å¤‰æ›
            self.df['Date'] = pd.to_datetime(self.df['Date'])
            
            # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ç¢ºèª
            logger.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {self.df['Date'].min()} ã€œ {self.df['Date'].max()}")
            
            # åŸºæœ¬çµ±è¨ˆ
            logger.info(f"éŠ˜æŸ„æ•°: {self.df['Code'].nunique()}")
            logger.info(f"å¹³å‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°/éŠ˜æŸ„: {len(self.df)/self.df['Code'].nunique():.0f}")
            
            return True
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_enhanced_features(self):
        """95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã®æ‹¡å¼µç‰¹å¾´é‡ä½œæˆ"""
        logger.info("æ‹¡å¼µç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        enhanced_df = self.df.copy()
        
        # éŠ˜æŸ„åˆ¥ã«ç‰¹å¾´é‡è¨ˆç®—
        result_dfs = []
        
        for code in enhanced_df['Code'].unique():
            code_df = enhanced_df[enhanced_df['Code'] == code].copy()
            code_df = code_df.sort_values('Date')
            
            # åŸºæœ¬ãƒªã‚¿ãƒ¼ãƒ³ã¨ãƒœãƒªãƒ¥ãƒ¼ãƒ 
            code_df['Returns'] = code_df['Close'].pct_change(fill_method=None)
            code_df['Volume_MA_20'] = code_df['Volume'].rolling(20).mean()
            code_df['Price_Volume_Trend'] = code_df['Returns'] * code_df['Volume']
            
            # ç§»å‹•å¹³å‡ï¼ˆ4ç¨®é¡ï¼‰
            for window in [5, 10, 20, 50]:
                code_df[f'MA_{window}'] = code_df['Close'].rolling(window).mean()
                code_df[f'MA_{window}_ratio'] = code_df['Close'] / code_df[f'MA_{window}']
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ3ç¨®é¡ï¼‰
            for window in [5, 10, 20]:
                code_df[f'Volatility_{window}'] = code_df['Returns'].rolling(window).std()
            
            # RSIï¼ˆ3ç¨®é¡ï¼‰
            for window in [7, 14, 21]:
                delta = code_df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_df[f'RSI_{window}'] = 100 - (100 / (1 + rs))
            
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆ20æ—¥ï¼‰
            for window in [20]:
                rolling_mean = code_df['Close'].rolling(window).mean()
                rolling_std = code_df['Close'].rolling(window).std()
                code_df[f'BB_upper_{window}'] = rolling_mean + (rolling_std * 2)
                code_df[f'BB_lower_{window}'] = rolling_mean - (rolling_std * 2)
                code_df[f'BB_ratio_{window}'] = (code_df['Close'] - code_df[f'BB_lower_{window}']) / (code_df[f'BB_upper_{window}'] - code_df[f'BB_lower_{window}'])
            
            # MACDï¼ˆ3æŒ‡æ¨™ï¼‰
            exp1 = code_df['Close'].ewm(span=12, adjust=False).mean()
            exp2 = code_df['Close'].ewm(span=26, adjust=False).mean()
            code_df['MACD'] = exp1 - exp2
            code_df['MACD_signal'] = code_df['MACD'].ewm(span=9, adjust=False).mean()
            code_df['MACD_histogram'] = code_df['MACD'] - code_df['MACD_signal']
            
            # ã‚ªãƒ³ãƒãƒ©ãƒ³ã‚¹ãƒœãƒªãƒ¥ãƒ¼ãƒ ï¼ˆOBVï¼‰
            code_df['OBV'] = (code_df['Volume'] * np.where(code_df['Close'] > code_df['Close'].shift(1), 1, 
                             np.where(code_df['Close'] < code_df['Close'].shift(1), -1, 0))).cumsum()
            
            # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ï¼ˆ14æ—¥ï¼‰
            for window in [14]:
                low_min = code_df['Low'].rolling(window).min()
                high_max = code_df['High'].rolling(window).max()
                code_df[f'Stoch_K_{window}'] = 100 * (code_df['Close'] - low_min) / (high_max - low_min)
                code_df[f'Stoch_D_{window}'] = code_df[f'Stoch_K_{window}'].rolling(3).mean()
            
            result_dfs.append(code_df)
        
        # çµåˆ
        enhanced_df = pd.concat(result_dfs, ignore_index=True)
        
        # ç›®çš„å¤‰æ•°ä½œæˆï¼ˆ95.45%ç²¾åº¦ã®æ ¸å¿ƒï¼‰
        logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
        enhanced_df['Target'] = 0
        
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            # ç¿Œæ—¥ã®é«˜å€¤ãŒå‰æ—¥çµ‚å€¤ã‹ã‚‰1%ä»¥ä¸Šä¸Šæ˜‡
            next_high = code_data['High'].shift(-1)    # ç¿Œæ—¥é«˜å€¤
            prev_close = code_data['Close'].shift(1)   # å‰æ—¥çµ‚å€¤
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # æ¬ æå€¤é™¤å»
        enhanced_df = enhanced_df.dropna()
        
        self.df = enhanced_df
        logger.info(f"æ‹¡å¼µç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(self.df):,}ä»¶")
        
        # æ­£ä¾‹ç‡ç¢ºèª
        positive_rate = self.df['Target'].mean()
        logger.info(f"æ­£ä¾‹ç‡: {positive_rate:.3f} ({positive_rate:.1%})")
        
        return enhanced_df
    
    def prepare_features_and_target(self):
        """ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™"""
        logger.info("ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™...")
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ é¸æŠï¼ˆéæ•°å€¤åˆ—é™¤å¤–ï¼‰
        exclude_cols = ['Date', 'Code', 'CompanyName', 'MatchMethod', 'ApiCode', 'Target']
        self.feature_cols = [col for col in self.df.columns if col not in exclude_cols]
        
        # æ•°å€¤å‹ã®ã¿é¸æŠ
        numeric_cols = self.df[self.feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        self.feature_cols = numeric_cols
        
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(self.feature_cols)}")
        
        X = self.df[self.feature_cols]
        y = self.df['Target']
        
        # ç„¡é™å€¤ã‚„NaNé™¤å»
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        return X, y
    
    def train_and_validate_model(self):
        """95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã®å­¦ç¿’ãƒ»æ¤œè¨¼"""
        logger.info("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æ¤œè¨¼é–‹å§‹...")
        
        # ç‰¹å¾´é‡æº–å‚™
        X, y = self.prepare_features_and_target()
        
        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆ95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ï¼‰
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start_date = latest_date - pd.Timedelta(days=30)  # 30æ—¥é–“ãƒ†ã‚¹ãƒˆ
        
        logger.info(f"è¨“ç·´æœŸé–“: ã€œ {test_start_date}")
        logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {test_start_date} ã€œ {latest_date}")
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_mask = df_sorted['Date'] < test_start_date
        test_mask = df_sorted['Date'] >= test_start_date
        
        X_train = X[train_mask]
        y_train = y[train_mask]
        X_test = X[test_mask]
        y_test = y[test_mask]
        
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶")
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test):,}ä»¶")
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆä¸Šä½30ç‰¹å¾´é‡ï¼‰
        logger.info("ç‰¹å¾´é‡é¸æŠ...")
        self.selector = SelectKBest(score_func=f_classif, k=30)
        X_train_selected = self.selector.fit_transform(X_train, y_train)
        X_test_selected = self.selector.transform(X_test)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        logger.info("ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°...")
        self.scaler = RobustScaler()
        X_train_scaled = self.scaler.fit_transform(X_train_selected)
        X_test_scaled = self.scaler.transform(X_test_selected)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        logger.info("LightGBMãƒ¢ãƒ‡ãƒ«å­¦ç¿’...")
        self.model = LGBMClassifier(**self.model_params)
        self.model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        # æ—¥åˆ¥ç²¾åº¦æ¤œè¨¼ï¼ˆ95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ï¼‰
        return self.evaluate_daily_precision(df_sorted[test_mask], y_pred_proba)
    
    def evaluate_daily_precision(self, test_df, pred_proba):
        """æ—¥åˆ¥ç²¾åº¦è©•ä¾¡ï¼ˆ95.45%ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ï¼‰"""
        logger.info("æ—¥åˆ¥ç²¾åº¦è©•ä¾¡é–‹å§‹...")
        
        test_df_copy = test_df.copy()
        test_df_copy['PredProba'] = pred_proba
        
        # å–¶æ¥­æ—¥åˆ¥ã«è©•ä¾¡
        unique_dates = sorted(test_df_copy['Date'].unique())
        daily_results = []
        total_predictions = 0
        total_correct = 0
        
        logger.info(f"æ¤œè¨¼æœŸé–“: {unique_dates[0]} ã€œ {unique_dates[-1]} ({len(unique_dates)}å–¶æ¥­æ—¥)")
        
        for test_date in unique_dates:
            daily_data = test_df_copy[test_df_copy['Date'] == test_date]
            
            if len(daily_data) < 3:
                continue
            
            # ä¸Šä½3éŠ˜æŸ„é¸æŠï¼ˆè³ªé‡è¦–æˆ¦ç•¥ï¼‰
            top3_indices = daily_data['PredProba'].nlargest(3).index
            selected_predictions = daily_data.loc[top3_indices]
            
            # å®Ÿéš›ã®çµæœ
            actual_results = selected_predictions['Target'].values
            n_correct = np.sum(actual_results)
            n_total = len(actual_results)
            daily_precision = n_correct / n_total if n_total > 0 else 0
            
            daily_results.append({
                'date': test_date,
                'n_correct': n_correct,
                'n_total': n_total,
                'precision': daily_precision,
                'selected_codes': selected_predictions['Code'].tolist()
            })
            
            total_correct += n_correct
            total_predictions += n_total
            
            logger.info(f"{test_date.strftime('%Y-%m-%d')}: {n_correct}/{n_total} = {daily_precision:.1%} "
                       f"[{', '.join(selected_predictions['Code'].astype(str).tolist())}]")
        
        # ç·åˆç²¾åº¦è¨ˆç®—
        overall_precision = total_correct / total_predictions if total_predictions > 0 else 0
        
        logger.info(f"\nğŸ‰ æ¤œè¨¼çµæœ:")
        logger.info(f"æ¤œè¨¼å–¶æ¥­æ—¥æ•°: {len(daily_results)}æ—¥é–“")
        logger.info(f"ç·äºˆæ¸¬æ•°: {total_predictions}ä»¶")
        logger.info(f"çš„ä¸­æ•°: {total_correct}ä»¶")
        logger.info(f"ç·åˆç²¾åº¦: {overall_precision:.4f} ({overall_precision:.2%})")
        
        return {
            'overall_precision': overall_precision,
            'daily_results': daily_results,
            'total_correct': total_correct,
            'total_predictions': total_predictions,
            'n_days': len(daily_results)
        }
    
    def save_model(self, results):
        """ãƒ¢ãƒ‡ãƒ«ã¨çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        precision_str = f"{results['overall_precision']:.4f}".replace('.', '')
        
        # modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs("models", exist_ok=True)
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        model_file = f"models/nikkei225_complete_model_{len(self.df)}records_{precision_str}precision_{timestamp}.joblib"
        
        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'selector': self.selector,
            'feature_cols': self.feature_cols,
            'precision': results['overall_precision'],
            'results': results,
            'data_info': {
                'total_records': len(self.df),
                'n_companies': self.df['Code'].nunique(),
                'data_period': f"{self.df['Date'].min()} - {self.df['Date'].max()}",
                'model_params': self.model_params
            }
        }
        
        # ä¿å­˜
        joblib.dump(model_data, model_file)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_file
    
    def run_complete_training(self):
        """å®Œå…¨ãªå­¦ç¿’ãƒ»æ¤œè¨¼å®Ÿè¡Œ"""
        logger.info("ğŸš€ æ—¥çµŒ225å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé«˜ç²¾åº¦å­¦ç¿’ãƒ»æ¤œè¨¼é–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            if not self.load_and_prepare_data():
                return None
            
            # ç‰¹å¾´é‡ä½œæˆ
            self.create_enhanced_features()
            
            # å­¦ç¿’ãƒ»æ¤œè¨¼
            results = self.train_and_validate_model()
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_file = self.save_model(results)
            
            # çµæœã‚µãƒãƒªãƒ¼
            logger.info(f"\nğŸ¯ æœ€çµ‚çµæœ:")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(self.df):,}ä»¶ ({self.df['Code'].nunique()}éŠ˜æŸ„)")
            logger.info(f"æ¤œè¨¼ç²¾åº¦: {results['overall_precision']:.4f} ({results['overall_precision']:.2%})")
            logger.info(f"æ¤œè¨¼æœŸé–“: {results['n_days']}å–¶æ¥­æ—¥")
            logger.info(f"äºˆæ¸¬æˆåŠŸ: {results['total_correct']}/{results['total_predictions']}ä»¶")
            logger.info(f"ä¿å­˜å…ˆ: {model_file}")
            
            return results
            
        except Exception as e:
            logger.error(f"å­¦ç¿’ãƒ»æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    trainer = Nikkei225CompletePrecisionTrainer()
    results = trainer.run_complete_training()
    
    if results:
        print(f"\nâœ… å­¦ç¿’ãƒ»æ¤œè¨¼å®Œäº†!")
        print(f"ğŸ“Š é”æˆç²¾åº¦: {results['overall_precision']:.2%}")
        print(f"ğŸ“ˆ æ¤œè¨¼å®Ÿç¸¾: {results['total_correct']}/{results['total_predictions']}ä»¶")
        print(f"ğŸ“… æ¤œè¨¼æœŸé–“: {results['n_days']}å–¶æ¥­æ—¥é–“")
    else:
        print("\nâŒ å­¦ç¿’ãƒ»æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()