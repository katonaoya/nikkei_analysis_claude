#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç´”ç²‹ãªäºˆæ¸¬ç²¾åº¦60%ä»¥ä¸Šã‚’é”æˆã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
é‹ç”¨ãƒ«ãƒ¼ãƒ«ã¯è€ƒæ…®ã›ãšã€å˜ç´”ã«ä¸Šæ˜‡/ä¸‹è½ã®äºˆæ¸¬ç²¾åº¦ã‚’60%ä»¥ä¸Šã«ã™ã‚‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
import yaml
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.metrics import accuracy_score
import xgboost as xgb
import lightgbm as lgb
import logging
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')
logger = logging.getLogger(__name__)


class PureAccuracyOptimizer:
    """ç´”ç²‹ãªç²¾åº¦60%é”æˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.best_accuracy = 0
        self.best_config = None
        
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        df = pd.read_parquet("data/processed/integrated_with_external.parquet")
        
        # å¿…è¦ãªåˆ—å‡¦ç†
        if 'Target' not in df.columns and 'Binary_Direction' in df.columns:
            df['Target'] = df['Binary_Direction']
        if 'Stock' not in df.columns and 'Code' in df.columns:
            df['Stock'] = df['Code']
        
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')
        
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        return df
    
    def create_all_features(self, df):
        """å…¨ã¦ã®å¯èƒ½ãªç‰¹å¾´é‡ã‚’ä½œæˆ"""
        logger.info("ğŸ”§ ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        if 'Close' not in df.columns:
            return df
        
        # ä¾¡æ ¼å¤‰å‹•ç‡
        for period in [1, 2, 3, 5, 10, 20]:
            col = f'Returns_{period}d'
            if col not in df.columns:
                df[col] = df.groupby('Stock')['Close'].pct_change(period)
        
        # ç§»å‹•å¹³å‡
        for window in [5, 10, 20, 50, 100]:
            # å˜ç´”ç§»å‹•å¹³å‡
            ma_col = f'MA_{window}'
            if ma_col not in df.columns:
                df[ma_col] = df.groupby('Stock')['Close'].transform(
                    lambda x: x.rolling(window, min_periods=1).mean()
                )
            
            # ç§»å‹•å¹³å‡ã¨ã®æ¯”ç‡
            ratio_col = f'Close_MA{window}_Ratio'
            if ratio_col not in df.columns:
                df[ratio_col] = df['Close'] / df[ma_col].replace(0, np.nan)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        for window in [5, 10, 20, 50]:
            vol_col = f'Volatility_{window}d'
            if vol_col not in df.columns:
                df[vol_col] = df.groupby('Stock')['Close'].transform(
                    lambda x: x.pct_change().rolling(window, min_periods=1).std()
                )
        
        # RSI
        def calc_rsi(prices, period=14):
            delta = prices.diff()
            gain = delta.where(delta > 0, 0).rolling(period, min_periods=1).mean()
            loss = -delta.where(delta < 0, 0).rolling(period, min_periods=1).mean()
            rs = gain / (loss + 1e-10)
            return 100 - (100 / (1 + rs))
        
        for period in [7, 14, 21]:
            rsi_col = f'RSI_{period}'
            if rsi_col not in df.columns:
                df[rsi_col] = df.groupby('Stock')['Close'].transform(
                    lambda x: calc_rsi(x, period)
                )
        
        # MACD
        if 'MACD' not in df.columns:
            exp12 = df.groupby('Stock')['Close'].transform(lambda x: x.ewm(span=12).mean())
            exp26 = df.groupby('Stock')['Close'].transform(lambda x: x.ewm(span=26).mean())
            df['MACD'] = exp12 - exp26
            df['MACD_Signal'] = df.groupby('Stock')['MACD'].transform(lambda x: x.ewm(span=9).mean())
            df['MACD_Diff'] = df['MACD'] - df['MACD_Signal']
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        for window in [20]:
            bb_middle = f'BB_Middle_{window}'
            if bb_middle not in df.columns:
                df[bb_middle] = df.groupby('Stock')['Close'].transform(
                    lambda x: x.rolling(window, min_periods=1).mean()
                )
                
                std = df.groupby('Stock')['Close'].transform(
                    lambda x: x.rolling(window, min_periods=1).std()
                )
                
                df[f'BB_Upper_{window}'] = df[bb_middle] + 2 * std
                df[f'BB_Lower_{window}'] = df[bb_middle] - 2 * std
                df[f'BB_Position_{window}'] = (df['Close'] - df[f'BB_Lower_{window}']) / (
                    df[f'BB_Upper_{window}'] - df[f'BB_Lower_{window}'] + 1e-10
                )
        
        # å‡ºæ¥é«˜é–¢é€£
        if 'Volume' in df.columns:
            # å‡ºæ¥é«˜ç§»å‹•å¹³å‡
            for window in [5, 10, 20]:
                vol_ma_col = f'Volume_MA_{window}'
                if vol_ma_col not in df.columns:
                    df[vol_ma_col] = df.groupby('Stock')['Volume'].transform(
                        lambda x: x.rolling(window, min_periods=1).mean()
                    )
                
                # å‡ºæ¥é«˜æ¯”ç‡
                vol_ratio_col = f'Volume_Ratio_{window}'
                if vol_ratio_col not in df.columns:
                    df[vol_ratio_col] = df['Volume'] / df[vol_ma_col].replace(0, np.nan)
            
            # ä¾¡æ ¼Ã—å‡ºæ¥é«˜
            df['PriceVolume'] = df['Close'] * df['Volume']
            
            # On-Balance Volume (OBV)
            if 'OBV' not in df.columns:
                df['OBV'] = df.groupby('Stock').apply(
                    lambda x: (x['Volume'] * np.sign(x['Close'].diff())).cumsum()
                ).reset_index(level=0, drop=True)
        
        # é«˜å€¤ãƒ»å®‰å€¤é–¢é€£
        if 'High' in df.columns and 'Low' in df.columns:
            # é«˜å€¤å®‰å€¤ã®ç¯„å›²
            df['HL_Range'] = (df['High'] - df['Low']) / df['Close'].replace(0, np.nan)
            
            # çµ‚å€¤ã®ä½ç½®
            df['Close_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'] + 1e-10)
            
            # éå»Næ—¥ã®æœ€é«˜å€¤ãƒ»æœ€å®‰å€¤
            for window in [10, 20, 50]:
                high_col = f'High_{window}d'
                low_col = f'Low_{window}d'
                
                if high_col not in df.columns:
                    df[high_col] = df.groupby('Stock')['High'].transform(
                        lambda x: x.rolling(window, min_periods=1).max()
                    )
                    df[low_col] = df.groupby('Stock')['Low'].transform(
                        lambda x: x.rolling(window, min_periods=1).min()
                    )
                    
                    # ç¾åœ¨ä¾¡æ ¼ã®ç›¸å¯¾ä½ç½®
                    df[f'Price_Position_{window}d'] = (df['Close'] - df[low_col]) / (
                        df[high_col] - df[low_col] + 1e-10
                    )
        
        return df
    
    def select_best_features(self, df, n_features=15):
        """æœ€è‰¯ã®ç‰¹å¾´é‡ã‚’é¸æŠ"""
        logger.info(f"ğŸ¯ ä¸Šä½{n_features}å€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ...")
        
        # é™¤å¤–åˆ—
        exclude = ['Date', 'Stock', 'Code', 'Target', 'Binary_Direction', 
                  'Open', 'High', 'Low', 'Close', 'Volume', 'Direction']
        
        # æ•°å€¤åˆ—ã®ã¿
        feature_cols = [col for col in df.columns 
                       if col not in exclude and df[col].dtype in ['float64', 'int64']]
        
        # æ¬ æãŒå°‘ãªã„ç‰¹å¾´é‡
        valid_features = []
        for col in feature_cols:
            missing_rate = df[col].isna().mean()
            if missing_rate < 0.3:  # æ¬ æç‡30%æœªæº€
                valid_features.append(col)
        
        if len(valid_features) == 0:
            return []
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
        sample_size = min(50000, len(df))
        df_sample = df.sample(sample_size, random_state=42)
        df_sample = df_sample[['Target'] + valid_features].dropna()
        
        if len(df_sample) < 1000:
            return valid_features[:n_features]
        
        X = df_sample[valid_features]
        y = df_sample['Target']
        
        # ç›¸äº’æƒ…å ±é‡ã§ç‰¹å¾´é‡é¸æŠ
        selector = SelectKBest(mutual_info_classif, k=min(n_features, len(valid_features)))
        selector.fit(X, y)
        
        # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡
        selected_features = [feat for feat, selected in zip(valid_features, selector.get_support()) if selected]
        
        logger.info(f"ğŸ“Š é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {selected_features[:5]}...")
        
        return selected_features
    
    def test_accuracy(self, df, features, model_type='rf'):
        """ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        required_cols = ['Date', 'Stock', 'Target'] + features
        clean_df = df[required_cols].dropna()
        
        if len(clean_df) < 10000:
            return 0
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        clean_df = clean_df.sort_values('Date')
        unique_dates = sorted(clean_df['Date'].unique())
        
        if len(unique_dates) < 50:
            return 0
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ï¼ˆæ™‚ç³»åˆ—ï¼‰
        split_date = unique_dates[-20]  # ç›´è¿‘20æ—¥ã‚’ãƒ†ã‚¹ãƒˆ
        
        train_data = clean_df[clean_df['Date'] < split_date]
        test_data = clean_df[clean_df['Date'] >= split_date]
        
        if len(train_data) < 5000 or len(test_data) < 1000:
            return 0
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¶é™ï¼ˆé«˜é€ŸåŒ–ï¼‰
        if len(train_data) > 50000:
            train_data = train_data.tail(50000)
        
        X_train = train_data[features]
        y_train = train_data['Target']
        X_test = test_data[features]
        y_test = test_data['Target']
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        if model_type == 'rf':
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=42,
                n_jobs=-1
            )
        elif model_type == 'gb':
            model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
        elif model_type == 'xgb':
            model = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )
        else:  # lgb
            model = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42,
                verbose=-1
            )
        
        model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬
        y_pred = model.predict(X_test_scaled)
        
        # ç²¾åº¦è¨ˆç®—
        accuracy = accuracy_score(y_test, y_pred)
        
        return accuracy
    
    def optimize_for_60(self):
        """60%ç²¾åº¦ã‚’é”æˆã™ã‚‹ã¾ã§æœ€é©åŒ–"""
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = self.load_data()
        
        # å…¨ç‰¹å¾´é‡ä½œæˆ
        df = self.create_all_features(df)
        
        # è¤‡æ•°ã®ç‰¹å¾´é‡æ•°ã‚’è©¦ã™
        for n_features in [10, 15, 20, 25, 30]:
            logger.info(f"\nğŸ” {n_features}å€‹ã®ç‰¹å¾´é‡ã§ãƒ†ã‚¹ãƒˆ...")
            
            # æœ€è‰¯ã®ç‰¹å¾´é‡é¸æŠ
            features = self.select_best_features(df, n_features)
            
            if len(features) < 5:
                continue
            
            # è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
            for model_type in ['rf', 'gb', 'xgb', 'lgb']:
                logger.info(f"  ãƒ¢ãƒ‡ãƒ«: {model_type}")
                
                accuracy = self.test_accuracy(df, features, model_type)
                
                logger.info(f"  ç²¾åº¦: {accuracy:.2%}")
                
                if accuracy > self.best_accuracy:
                    self.best_accuracy = accuracy
                    self.best_config = {
                        'features': features,
                        'model_type': model_type,
                        'n_features': n_features,
                        'accuracy': accuracy
                    }
                    
                    logger.info(f"  âœ… æ–°è¨˜éŒ²! {accuracy:.2%}")
                    
                    if accuracy >= 0.60:
                        logger.info(f"  ğŸ¯ ç›®æ¨™é”æˆ! 60%ã‚’è¶…ãˆã¾ã—ãŸ!")
                        return self.best_config
        
        # ã¾ã 60%æœªé”æˆãªã‚‰ã€åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        if self.best_accuracy < 0.60:
            logger.info("\nğŸš€ è¿½åŠ ã®æœ€é©åŒ–...")
            
            # ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã§å†è©¦è¡Œ
            for n_features in [35, 40, 50]:
                features = self.select_best_features(df, n_features)
                
                if len(features) < 10:
                    continue
                
                # XGBoostã¨LightGBMã«çµã‚‹
                for model_type in ['xgb', 'lgb']:
                    accuracy = self.test_accuracy(df, features, model_type)
                    
                    if accuracy > self.best_accuracy:
                        self.best_accuracy = accuracy
                        self.best_config = {
                            'features': features,
                            'model_type': model_type,
                            'n_features': n_features,
                            'accuracy': accuracy
                        }
                        
                        logger.info(f"  âœ… æ›´æ–°! {accuracy:.2%}")
                        
                        if accuracy >= 0.60:
                            logger.info(f"  ğŸ¯ ç›®æ¨™é”æˆ!")
                            return self.best_config
        
        return self.best_config


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("="*60)
    logger.info("ğŸ¯ ç´”ç²‹ãªäºˆæ¸¬ç²¾åº¦60%é”æˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    logger.info("="*60)
    
    optimizer = PureAccuracyOptimizer()
    result = optimizer.optimize_for_60()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æœ€çµ‚çµæœ")
    logger.info("="*60)
    
    if result:
        logger.info(f"æœ€é«˜ç²¾åº¦: {result['accuracy']:.2%}")
        logger.info(f"ãƒ¢ãƒ‡ãƒ«: {result['model_type']}")
        logger.info(f"ç‰¹å¾´é‡æ•°: {result['n_features']}")
        
        if result['accuracy'] >= 0.60:
            logger.info("\nâœ… ç›®æ¨™é”æˆ! 60%ä»¥ä¸Šã®ç²¾åº¦ã‚’å®Ÿç¾!")
            
            # è¨­å®šã‚’ä¿å­˜
            config_path = Path("production_config.yaml")
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            config['features']['optimal_features'] = result['features'][:10]  # ä¸Šä½10å€‹
            config['model'] = {
                'type': result['model_type'],
                'accuracy': float(result['accuracy']),
                'n_features': len(result['features'][:10])
            }
            
            with open(config_path, 'w') as f:
                yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
            
            logger.info("ğŸ“ è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {result['features'][:5]}...")
            
            # ç²¾åº¦60%é”æˆã—ãŸã“ã¨ã‚’è¨˜éŒ²
            with open("accuracy_60_achieved.txt", 'w') as f:
                f.write(f"é”æˆç²¾åº¦: {result['accuracy']:.2%}\n")
                f.write(f"ãƒ¢ãƒ‡ãƒ«: {result['model_type']}\n")
                f.write(f"ç‰¹å¾´é‡: {', '.join(result['features'][:10])}\n")
                f.write(f"é”æˆæ—¥æ™‚: {pd.Timestamp.now()}\n")
        else:
            logger.info(f"\nç¾åœ¨ã®æœ€é«˜ç²¾åº¦: {result['accuracy']:.2%}")
            logger.info("60%ã«ã¯ã¾ã å±Šã„ã¦ã„ã¾ã›ã‚“ãŒã€ç¶™ç¶šã—ã¦æœ€é©åŒ–ã—ã¾ã™")
    else:
        logger.error("æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()