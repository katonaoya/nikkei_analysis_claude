#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
530,744ä»¶ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§LightGBMãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’æœ€çµ‚æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime, timedelta
import joblib
import warnings
from typing import Tuple, Optional

# æ©Ÿæ¢°å­¦ç¿’é–¢é€£
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import precision_score, classification_report
import lightgbm as lgb

warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Nikkei225FullPrecisionTest:
    """æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦æ¤œè¨¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # æœ€æ–°ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
        self.data_dir = Path("data/nikkei225_full")
        self.model_dir = Path("models")
        self.model_dir.mkdir(exist_ok=True)
        
        # æ‹¡å¼µLightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ã«æœ€é©åŒ–ï¼‰
        self.model_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 300,
            'max_depth': 8,
            'min_child_samples': 30,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'learning_rate': 0.03,  # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ã«å°ã•ã
            'reg_alpha': 0.1,       # L1æ­£å‰‡åŒ–è¿½åŠ 
            'reg_lambda': 0.1,      # L2æ­£å‰‡åŒ–è¿½åŠ 
            'random_state': 42,
            'verbose': -1
        }
        
        logger.info("æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ç²¾åº¦æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def load_latest_full_data(self) -> pd.DataFrame:
        """æœ€æ–°ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        if not self.data_dir.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_dir}")
        
        # æœ€æ–°ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        parquet_files = list(self.data_dir.glob("nikkei225_full_*.parquet"))
        if not parquet_files:
            raise FileNotFoundError(f"å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_dir}")
        
        latest_file = max(parquet_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ğŸ“ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {latest_file.name}")
        
        df = pd.read_parquet(latest_file)
        
        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤º
        logger.info(f"ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†:")
        logger.info(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}ä»¶")
        logger.info(f"  éŠ˜æŸ„æ•°: {df['Code'].nunique()}éŠ˜æŸ„")
        logger.info(f"  æœŸé–“: {df['Date'].min()} ï½ {df['Date'].max()}")
        
        return df
    
    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‹¡å¼µæŠ€è¡“æŒ‡æ¨™ã®ç”Ÿæˆï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰"""
        logger.info("ğŸ”§ æ‹¡å¼µæŠ€è¡“æŒ‡æ¨™ç”Ÿæˆé–‹å§‹...")
        
        df = df.copy()
        df = df.sort_values(['Code', 'Date'])
        
        enhanced_df_list = []
        
        for code in df['Code'].unique():
            code_df = df[df['Code'] == code].copy()
            
            # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            code_df['Returns'] = code_df['Close'].pct_change()
            code_df['Volume_MA_20'] = code_df['Volume'].rolling(20).mean()
            code_df['Price_Volume_Trend'] = code_df['Returns'] * code_df['Volume']
            
            # ç§»å‹•å¹³å‡ï¼ˆå¤šæœŸé–“ï¼‰
            for window in [5, 10, 20, 50]:
                code_df[f'MA_{window}'] = code_df['Close'].rolling(window).mean()
                code_df[f'MA_{window}_ratio'] = code_df['Close'] / code_df[f'MA_{window}']
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå¤šæœŸé–“ï¼‰
            for window in [5, 10, 20]:
                code_df[f'Volatility_{window}'] = code_df['Returns'].rolling(window).std()
            
            # RSIï¼ˆå¤šæœŸé–“ï¼‰
            for window in [7, 14, 21]:
                delta = code_df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_df[f'RSI_{window}'] = 100 - (100 / (1 + rs))
            
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
            for window in [20]:
                rolling_mean = code_df['Close'].rolling(window).mean()
                rolling_std = code_df['Close'].rolling(window).std()
                code_df[f'BB_upper_{window}'] = rolling_mean + (rolling_std * 2)
                code_df[f'BB_lower_{window}'] = rolling_mean - (rolling_std * 2)
                code_df[f'BB_ratio_{window}'] = (code_df['Close'] - code_df[f'BB_lower_{window}']) / (code_df[f'BB_upper_{window}'] - code_df[f'BB_lower_{window}'])
            
            # MACD
            exp1 = code_df['Close'].ewm(span=12, adjust=False).mean()
            exp2 = code_df['Close'].ewm(span=26, adjust=False).mean()
            code_df['MACD'] = exp1 - exp2
            code_df['MACD_signal'] = code_df['MACD'].ewm(span=9, adjust=False).mean()
            code_df['MACD_histogram'] = code_df['MACD'] - code_df['MACD_signal']
            
            # ã‚ªãƒ³ãƒãƒ©ãƒ³ã‚¹ãƒœãƒªãƒ¥ãƒ¼ãƒ 
            code_df['OBV'] = (code_df['Volume'] * np.where(code_df['Close'] > code_df['Close'].shift(1), 1, 
                             np.where(code_df['Close'] < code_df['Close'].shift(1), -1, 0))).cumsum()
            
            # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
            for window in [14]:
                low_min = code_df['Low'].rolling(window).min()
                high_max = code_df['High'].rolling(window).max()
                code_df[f'Stoch_K_{window}'] = 100 * (code_df['Close'] - low_min) / (high_max - low_min)
                code_df[f'Stoch_D_{window}'] = code_df[f'Stoch_K_{window}'].rolling(3).mean()
            
            enhanced_df_list.append(code_df)
        
        enhanced_df = pd.concat(enhanced_df_list, ignore_index=True)
        
        # ç›®çš„å¤‰æ•°ã®ä½œæˆ
        enhanced_df['Target'] = 0
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            # ç¿Œæ—¥ã®é«˜å€¤ãŒå‰æ—¥çµ‚å€¤ã‹ã‚‰1%ä»¥ä¸Šä¸Šæ˜‡
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # NaNã‚’é™¤å»
        enhanced_df = enhanced_df.dropna()
        
        logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†:")
        logger.info(f"  å‡¦ç†å¾Œãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(enhanced_df):,}ä»¶")
        logger.info(f"  ç‰¹å¾´é‡æ•°: {len([col for col in enhanced_df.columns if col not in ['Code', 'Date', 'CompanyName', 'Target']])}å€‹")
        logger.info(f"  æ­£ä¾‹ç‡: {enhanced_df['Target'].mean():.3f}")
        
        return enhanced_df
    
    def advanced_time_series_validation(self, df: pd.DataFrame) -> Tuple[float, dict]:
        """æ‹¡å¼µæ™‚ç³»åˆ—ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆ30æ—¥é–“ï¼‰"""
        logger.info("ğŸ¯ æ‹¡å¼µæ™‚ç³»åˆ—ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ30æ—¥é–“ï¼‰...")
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é›¢
        feature_cols = [col for col in df.columns if col not in ['Code', 'Date', 'CompanyName', 'Target']]
        
        # æ—¥ä»˜åˆ—ã‚’datetimeå‹ã«å¤‰æ›
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df.sort_values('Date')
        
        # ãƒ†ã‚¹ãƒˆæœŸé–“è¨­å®šï¼ˆç›´è¿‘30æ—¥é–“ï¼‰
        latest_date = df_sorted['Date'].max()
        test_start_date = latest_date - pd.Timedelta(days=30)
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_df = df_sorted[df_sorted['Date'] < test_start_date].copy()
        test_df = df_sorted[df_sorted['Date'] >= test_start_date].copy()
        
        logger.info(f"ğŸ“… è¨“ç·´æœŸé–“: {train_df['Date'].min()} ï½ {train_df['Date'].max()}")
        logger.info(f"ğŸ“… ãƒ†ã‚¹ãƒˆæœŸé–“: {test_df['Date'].min()} ï½ {test_df['Date'].max()}")
        logger.info(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,}ä»¶")
        logger.info(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}ä»¶")
        
        if len(test_df) == 0:
            logger.error("âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return 0.0, {}
        
        # ç‰¹å¾´é‡æº–å‚™
        X_train = train_df[feature_cols]
        y_train = train_df['Target']
        X_test = test_df[feature_cols]
        y_test = test_df['Target']
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆä¸Šä½30å€‹ï¼‰
        logger.info("ğŸ” ç‰¹å¾´é‡é¸æŠä¸­...")
        selector = SelectKBest(score_func=f_classif, k=min(30, len(feature_cols)))
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]
        logger.info(f"âœ… é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {len(selected_features)}å€‹")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # LightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´
        logger.info("ğŸ¤– æ‹¡å¼µLightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        model = lgb.LGBMClassifier(**self.model_params)
        model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        # æ—¥åˆ¥ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        test_df_copy = test_df.copy()
        test_df_copy['PredProba'] = pred_proba
        
        daily_results = []
        unique_dates = sorted(test_df_copy['Date'].unique())
        
        for test_date in unique_dates:
            daily_data = test_df_copy[test_df_copy['Date'] == test_date]
            
            if len(daily_data) < 3:
                continue
            
            # ä¸Šä½3éŠ˜æŸ„é¸æŠ
            top3_indices = daily_data['PredProba'].nlargest(3).index
            selected_predictions = daily_data.loc[top3_indices]
            
            # å®Ÿéš›ã®çµæœ
            actual_results = selected_predictions['Target'].values
            precision = np.mean(actual_results)
            
            daily_results.append({
                'date': test_date,
                'precision': precision,
                'n_correct': np.sum(actual_results),
                'n_total': len(actual_results),
                'selected_codes': selected_predictions['Code'].tolist(),
                'probabilities': selected_predictions['PredProba'].tolist()
            })
        
        if not daily_results:
            logger.error("âŒ æœ‰åŠ¹ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 0.0, {}
        
        # ç·åˆç²¾åº¦è¨ˆç®—
        total_correct = sum(result['n_correct'] for result in daily_results)
        total_predictions = sum(result['n_total'] for result in daily_results)
        overall_precision = total_correct / total_predictions if total_predictions > 0 else 0.0
        
        # è©³ç´°çµ±è¨ˆ
        daily_precisions = [result['precision'] for result in daily_results]
        stats = {
            'overall_precision': overall_precision,
            'total_correct': total_correct,
            'total_predictions': total_predictions,
            'test_days': len(daily_results),
            'mean_daily_precision': np.mean(daily_precisions),
            'std_daily_precision': np.std(daily_precisions),
            'min_daily_precision': np.min(daily_precisions),
            'max_daily_precision': np.max(daily_precisions),
            'selected_features': selected_features,
            'daily_results': daily_results[-5:]  # æœ€æ–°5æ—¥åˆ†
        }
        
        logger.info("="*60)
        logger.info("ğŸ“Š æ‹¡å¼µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœï¼ˆæ—¥çµŒ225å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")
        logger.info("="*60)
        logger.info(f"ğŸ¯ ç·åˆç²¾åº¦: {overall_precision:.4f} ({overall_precision*100:.2f}%)")
        logger.info(f"âœ… çš„ä¸­æ•°: {total_correct}/{total_predictions}")
        logger.info(f"ğŸ“… ãƒ†ã‚¹ãƒˆæœŸé–“: {len(daily_results)}æ—¥é–“")
        logger.info(f"ğŸ“ˆ æ—¥æ¬¡ç²¾åº¦: {np.mean(daily_precisions):.4f}Â±{np.std(daily_precisions):.4f}")
        
        return overall_precision, stats
    
    def save_final_model_and_results(self, df: pd.DataFrame, precision: float, stats: dict) -> str:
        """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´
        feature_cols = [col for col in df.columns if col not in ['Code', 'Date', 'CompanyName', 'Target']]
        X = df[feature_cols]
        y = df['Target']
        
        selector = SelectKBest(score_func=f_classif, k=min(30, len(feature_cols)))
        X_selected = selector.fit_transform(X, y)
        
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X_selected)
        
        final_model = lgb.LGBMClassifier(**self.model_params)
        final_model.fit(X_scaled, y)
        
        # ä¿å­˜
        model_filename = f"nikkei225_full_model_{len(df)}records_{precision:.4f}precision_{timestamp}.joblib"
        model_path = self.model_dir / model_filename
        
        joblib.dump({
            'model': final_model,
            'scaler': scaler,
            'selector': selector,
            'feature_cols': feature_cols,
            'precision': precision,
            'stats': stats,
            'data_info': {
                'total_records': len(df),
                'stocks': df['Code'].nunique(),
                'period': f"{df['Date'].min()} - {df['Date'].max()}"
            }
        }, model_path)
        
        logger.info(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_filename}")
        return str(model_path)
    
    def run_complete_validation(self) -> dict:
        """å®Œå…¨ãªç²¾åº¦æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        logger.info("ğŸš€ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ç²¾åº¦æ¤œè¨¼é–‹å§‹")
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.load_latest_full_data()
            
            # 2. ç‰¹å¾´é‡ç”Ÿæˆ
            enhanced_df = self.create_advanced_features(df)
            
            # 3. ç²¾åº¦æ¤œè¨¼
            precision, stats = self.advanced_time_series_validation(enhanced_df)
            
            # 4. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_path = self.save_final_model_and_results(enhanced_df, precision, stats)
            
            # 5. æœ€çµ‚çµæœ
            final_results = {
                'precision': precision,
                'precision_percent': precision * 100,
                'data_records': len(enhanced_df),
                'data_stocks': enhanced_df['Code'].nunique(),
                'model_path': model_path,
                'stats': stats
            }
            
            logger.info("="*60)
            logger.info("ğŸ‰ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†")
            logger.info("="*60)
            logger.info(f"ğŸ“Š æœ€çµ‚ç²¾åº¦: {precision:.4f} ({precision*100:.2f}%)")
            logger.info(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿è¦æ¨¡: {len(enhanced_df):,}ä»¶ ({enhanced_df['Code'].nunique()}éŠ˜æŸ„)")
            logger.info(f"ğŸ¯ 60%ç›®æ¨™: {'âœ… é”æˆ' if precision >= 0.60 else 'âŒ æœªé”æˆ'}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ç²¾åº¦æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    try:
        validator = Nikkei225FullPrecisionTest()
        results = validator.run_complete_validation()
        
        logger.info("="*60)
        logger.info("ğŸ“Š æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*60)
        logger.info(f"ğŸ¯ é”æˆç²¾åº¦: {results['precision_percent']:.2f}%")
        logger.info(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {results['data_records']:,}ä»¶ ({results['data_stocks']}éŠ˜æŸ„)")
        logger.info(f"ğŸ’¾ ä¿å­˜ãƒ¢ãƒ‡ãƒ«: {Path(results['model_path']).name}")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
        baseline_precision = 0.5758  # æ—¢å­˜ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        improvement = (results['precision'] - baseline_precision) / baseline_precision * 100
        
        logger.info(f"ğŸ“ˆ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ: {baseline_precision:.4f} â†’ {results['precision']:.4f}")
        logger.info(f"ğŸ“Š æ”¹å–„ç‡: {improvement:+.1f}%")
        
        if results['precision'] >= 0.60:
            logger.info("ğŸ‰ 60%ç›®æ¨™é”æˆï¼å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦ã‚’å®Ÿç¾")
        else:
            logger.info(f"âš ï¸  60%ç›®æ¨™æœªé”æˆï¼ˆç¾åœ¨{results['precision_percent']:.2f}%ï¼‰")
            
    except Exception as e:
        logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise


if __name__ == "__main__":
    main()