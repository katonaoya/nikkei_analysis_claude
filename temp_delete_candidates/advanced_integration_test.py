#!/usr/bin/env python3
"""
é«˜åº¦çµ±åˆãƒ†ã‚¹ãƒˆ
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ + ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ´»ç”¨ã§90%ç²¾åº¦ã‚’ç›®æŒ‡ã™
J-Quantsèªè¨¼ä¸è¦ç‰ˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

from yahoo_market_data import YahooMarketData
from loguru import logger

class AdvancedIntegrationTest:
    """é«˜åº¦çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.base_data_file = "data/processed/integrated_with_external.parquet"
    
    def load_and_prepare_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        logger.info("ğŸ”„ é«˜åº¦ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        try:
            df = pd.read_parquet(self.base_data_file)
            
            # ã‚«ãƒ©ãƒ çµ±ä¸€
            if 'date' in df.columns:
                df['Date'] = pd.to_datetime(df['date'])
            if 'code' in df.columns:
                df['Stock'] = df['code'].astype(str)
            
            logger.success(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}ä»¶")
        except Exception as e:
            logger.error(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return pd.DataFrame()
        
        # ä¸»è¦éŠ˜æŸ„é¸æŠï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªé‡è¦–ï¼‰
        stock_counts = df['Stock'].value_counts()
        quality_stocks = stock_counts[stock_counts >= 300].head(150).index.tolist()
        df = df[df['Stock'].isin(quality_stocks)].copy()
        
        logger.info(f"é«˜å“è³ªãƒ‡ãƒ¼ã‚¿éŠ˜æŸ„: {len(quality_stocks)}éŠ˜æŸ„")
        
        # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆ
        market_data = YahooMarketData()
        data_dict = market_data.get_all_market_data(period="2y")
        
        if data_dict:
            market_features = market_data.calculate_market_features(data_dict)
            if not market_features.empty:
                # æ—¥ä»˜ã®å‹ã‚’çµ±ä¸€
                df['Date'] = pd.to_datetime(df['Date']).dt.date
                market_features['Date'] = pd.to_datetime(market_features['Date']).dt.date
                
                # ãƒãƒ¼ã‚¸
                df = df.merge(market_features, on='Date', how='left')
                
                # æ¬ æå€¤è£œå®Œ
                market_cols = [col for col in market_features.columns if col != 'Date']
                df[market_cols] = df[market_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)
                
                logger.success(f"âœ… ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(market_cols)}ç‰¹å¾´é‡")
        
        return df
    
    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        logger.info("ğŸ”§ é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­...")
        
        enhanced_df = df.copy()
        enhanced_df = enhanced_df.sort_values(['Stock', 'Date'])
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ
        enhanced_df['next_high'] = enhanced_df.groupby('Stock')['high'].shift(-1)
        enhanced_df['Target'] = (enhanced_df['next_high'] > enhanced_df['close'] * 1.01).astype(int)
        
        # æ—¢å­˜ç‰¹å¾´é‡ã®æ”¹è‰¯ã¨æ–°è¦ç‰¹å¾´é‡
        for stock, stock_df in enhanced_df.groupby('Stock'):
            stock_mask = enhanced_df['Stock'] == stock
            stock_data = enhanced_df[stock_mask].sort_values('Date')
            
            if len(stock_data) < 50:
                continue
            
            # 1. é«˜åº¦ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
            # MACD
            ema12 = stock_data['close'].ewm(span=12).mean()
            ema26 = stock_data['close'].ewm(span=26).mean()
            macd = ema12 - ema26
            signal = macd.ewm(span=9).mean()
            enhanced_df.loc[stock_mask, 'MACD'] = macd
            enhanced_df.loc[stock_mask, 'MACD_Signal'] = signal
            enhanced_df.loc[stock_mask, 'MACD_Histogram'] = macd - signal
            
            # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
            low_min = stock_data['low'].rolling(14).min()
            high_max = stock_data['high'].rolling(14).max()
            k_percent = 100 * (stock_data['close'] - low_min) / (high_max - low_min)
            enhanced_df.loc[stock_mask, 'Stochastic_K'] = k_percent
            enhanced_df.loc[stock_mask, 'Stochastic_D'] = k_percent.rolling(3).mean()
            
            # ã‚¦ã‚£ãƒªã‚¢ãƒ ã‚º%R
            enhanced_df.loc[stock_mask, 'Williams_R'] = -100 * (high_max - stock_data['close']) / (high_max - low_min)
            
            # 2. ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
            # å‰æ—¥æ¯”å¤‰åŒ–ç‡
            returns = stock_data['close'].pct_change()
            enhanced_df.loc[stock_mask, 'Return_1d'] = returns
            enhanced_df.loc[stock_mask, 'Return_2d'] = stock_data['close'].pct_change(2)
            enhanced_df.loc[stock_mask, 'Return_3d'] = stock_data['close'].pct_change(3)
            
            # ãƒªã‚¿ãƒ¼ãƒ³ã®åŠ é€Ÿåº¦ï¼ˆå¤‰åŒ–ç‡ã®å¤‰åŒ–ç‡ï¼‰
            enhanced_df.loc[stock_mask, 'Return_Acceleration'] = returns.diff()
            
            # ä¾¡æ ¼ãƒ¬ãƒ³ã‚¸
            enhanced_df.loc[stock_mask, 'Daily_Range'] = (stock_data['high'] - stock_data['low']) / stock_data['close']
            enhanced_df.loc[stock_mask, 'Body_Size'] = abs(stock_data['close'] - stock_data['open']) / stock_data['close']
            
            # 3. ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆ†æ
            volume_sma = stock_data['volume'].rolling(20).mean()
            enhanced_df.loc[stock_mask, 'Volume_SMA_Ratio'] = stock_data['volume'] / volume_sma
            enhanced_df.loc[stock_mask, 'Price_Volume_Trend'] = ((stock_data['close'] - stock_data['close'].shift(1)) / stock_data['close'].shift(1)) * stock_data['volume']
            
            # 4. ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            # è¤‡æ•°æœŸé–“ç§»å‹•å¹³å‡
            for period in [5, 10, 25, 50]:
                ma = stock_data['close'].rolling(period).mean()
                enhanced_df.loc[stock_mask, f'MA_{period}'] = ma
                enhanced_df.loc[stock_mask, f'Price_MA_{period}_Ratio'] = stock_data['close'] / ma - 1
                enhanced_df.loc[stock_mask, f'MA_{period}_Slope'] = ma.pct_change(3)
            
            # ç§»å‹•å¹³å‡ã®ä½ç½®é–¢ä¿‚
            if len(stock_data) > 50:
                ma5 = enhanced_df.loc[stock_mask, 'MA_5']
                ma10 = enhanced_df.loc[stock_mask, 'MA_10']
                ma25 = enhanced_df.loc[stock_mask, 'MA_25']
                ma50 = enhanced_df.loc[stock_mask, 'MA_50']
                
                enhanced_df.loc[stock_mask, 'MA_Alignment'] = ((ma5 > ma10) & (ma10 > ma25) & (ma25 > ma50)).astype(int)
                enhanced_df.loc[stock_mask, 'Golden_Cross'] = ((ma5 > ma25) & (ma5.shift(1) <= ma25.shift(1))).astype(int)
            
            # 5. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æ
            for period in [5, 10, 20]:
                vol = returns.rolling(period).std()
                enhanced_df.loc[stock_mask, f'Volatility_{period}'] = vol
                enhanced_df.loc[stock_mask, f'Volatility_{period}_Norm'] = vol / vol.rolling(60).mean()
        
        # 6. ãƒãƒ¼ã‚±ãƒƒãƒˆç›¸å¯¾ç‰¹å¾´é‡
        if 'nikkei225_close' in enhanced_df.columns:
            nikkei_return = enhanced_df['nikkei225_return_1d']
            stock_return = enhanced_df['Return_1d']
            
            # ãƒ™ãƒ¼ã‚¿ï¼ˆå¸‚å ´æ„Ÿå¿œåº¦ï¼‰
            enhanced_df['Beta_20d'] = stock_return.rolling(20).corr(nikkei_return)
            enhanced_df['Alpha_20d'] = stock_return - enhanced_df['Beta_20d'] * nikkei_return
            
            # ç›¸å¯¾å¼·åº¦
            enhanced_df['Relative_Strength'] = stock_return.rolling(20).mean() - nikkei_return.rolling(20).mean()
            
        # 7. è¤‡åˆæŒ‡æ¨™
        if 'vix_close' in enhanced_df.columns:
            # ãƒªã‚¹ã‚¯èª¿æ•´æŒ‡æ¨™
            enhanced_df['Risk_Adjusted_Return'] = enhanced_df['Return_1d'] / (enhanced_df['vix_close'] / 100 + 0.01)
            enhanced_df['VIX_Stock_Divergence'] = enhanced_df['Volatility_20'] - (enhanced_df['vix_close'] / 100)
        
        # æ¬ æå€¤å‡¦ç†
        enhanced_df = enhanced_df.fillna(method='ffill').fillna(0)
        
        # ç•°å¸¸å€¤å‡¦ç†
        numeric_cols = enhanced_df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in ['Target', 'Date']:
                q99 = enhanced_df[col].quantile(0.99)
                q01 = enhanced_df[col].quantile(0.01)
                enhanced_df[col] = enhanced_df[col].clip(q01, q99)
        
        # ç„¡é™å¤§å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        feature_count = len([col for col in enhanced_df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']])
        logger.success(f"âœ… é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {feature_count}ç‰¹å¾´é‡")
        
        return enhanced_df
    
    def advanced_feature_selection(self, X_train, y_train, max_features: int = 40) -> list:
        """é«˜åº¦ç‰¹å¾´é‡é¸æŠï¼ˆè¤‡æ•°æ‰‹æ³•çµ„ã¿åˆã‚ã›ï¼‰"""
        logger.info("ğŸ¯ é«˜åº¦ç‰¹å¾´é‡é¸æŠä¸­...")
        
        # 1. çµ±è¨ˆçš„é‡è¦åº¦
        selector1 = SelectKBest(score_func=f_classif, k=min(60, X_train.shape[1]))
        X_selected1 = selector1.fit_transform(X_train, y_train)
        features1 = X_train.columns[selector1.get_support()].tolist()
        
        # 2. RandomForestã«ã‚ˆã‚‹é‡è¦åº¦
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        features2 = feature_importance.head(50)['feature'].tolist()
        
        # 3. ç›¸é–¢åˆ†æã«ã‚ˆã‚‹å†—é•·æ€§é™¤å»
        selected_features = list(set(features1 + features2))
        correlation_matrix = X_train[selected_features].corr().abs()
        
        # é«˜ç›¸é–¢ãƒšã‚¢ã‚’é™¤å»
        upper_tri = correlation_matrix.where(
            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
        )
        
        to_remove = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]
        final_features = [f for f in selected_features if f not in to_remove]
        
        # æœ€å¤§ç‰¹å¾´é‡æ•°ã«åˆ¶é™
        if len(final_features) > max_features:
            # é‡è¦åº¦é †ã«åˆ¶é™
            importance_order = feature_importance[feature_importance['feature'].isin(final_features)]
            final_features = importance_order.head(max_features)['feature'].tolist()
        
        logger.info(f"âœ… ç‰¹å¾´é‡é¸æŠå®Œäº†: {len(final_features)}å€‹é¸æŠ")
        return final_features
    
    def run_advanced_strategies(self, df: pd.DataFrame) -> list:
        """é«˜åº¦æˆ¦ç•¥ç¾¤å®Ÿè¡Œ"""
        logger.info("ğŸš€ é«˜åº¦æˆ¦ç•¥ç¾¤ã«ã‚ˆã‚‹90%ç²¾åº¦ãƒãƒ£ãƒ¬ãƒ³ã‚¸é–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df_sorted = df.sort_values(['Stock', 'Date'])
        unique_dates = sorted(df_sorted['Date'].unique())
        test_dates = unique_dates[-25:]  # æœ€æ–°25æ—¥
        
        feature_cols = [col for col in df.columns 
                       if col not in ['Date', 'Stock', 'Target', 'next_high'] 
                       and df[col].dtype in ['int64', 'float64']]
        
        logger.info(f"ä½¿ç”¨å¯èƒ½ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
        
        strategies_results = []
        
        # === æˆ¦ç•¥1: è¶…é«˜åº¦LightGBM + å‹•çš„ç‰¹å¾´é‡é¸æŠ ===
        logger.info("\\nğŸ¯ æˆ¦ç•¥1: è¶…é«˜åº¦LightGBM + å‹•çš„ç‰¹å¾´é‡é¸æŠ")
        
        strategy1_preds = []
        strategy1_actuals = []
        
        for i, test_date in enumerate(test_dates[-12:]):  # æœ€æ–°12æ—¥
            train_data = df_sorted[df_sorted['Date'] < test_date]
            test_data = df_sorted[df_sorted['Date'] == test_date]
            
            train_clean = train_data.dropna(subset=['Target'] + feature_cols)
            test_clean = test_data.dropna(subset=['Target'] + feature_cols)
            
            if len(train_clean) < 1000 or len(test_clean) < 3:
                continue
            
            X_train_full = train_clean[feature_cols]
            y_train = train_clean['Target']
            X_test_full = test_clean[feature_cols]
            y_test = test_clean['Target']
            
            # å‹•çš„ç‰¹å¾´é‡é¸æŠ
            selected_features = self.advanced_feature_selection(X_train_full, y_train, max_features=35)
            
            X_train = X_train_full[selected_features]
            X_test = X_test_full[selected_features]
            
            # é«˜åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # è¶…é«˜åº¦LightGBMãƒ¢ãƒ‡ãƒ«
            model = lgb.LGBMClassifier(
                n_estimators=300,
                max_depth=5,
                min_child_samples=10,
                subsample=0.9,
                colsample_bytree=0.8,
                learning_rate=0.05,
                reg_alpha=0.1,
                reg_lambda=0.1,
                num_leaves=31,
                random_state=42,
                verbose=-1
            )
            
            model.fit(X_train_scaled, y_train)
            probs = model.predict_proba(X_test_scaled)[:, 1]
            
            # ä¸Šä½2éŠ˜æŸ„é¸æŠï¼ˆã‚ˆã‚Šå³é¸ï¼‰
            n_select = min(2, len(probs))
            top_indices = np.argsort(probs)[-n_select:]
            
            selected_actuals = y_test.iloc[top_indices].values
            strategy1_preds.extend([1] * len(selected_actuals))
            strategy1_actuals.extend(selected_actuals)
            
            if i % 4 == 0:
                logger.info(f"  é€²æ—: {i+1}/12")
        
        if strategy1_preds:
            precision1 = sum(strategy1_actuals) / len(strategy1_actuals)
            strategies_results.append(('è¶…é«˜åº¦LightGBM', precision1, len(strategy1_preds)))
            logger.info(f"  çµæœ: {precision1:.2%}")
        
        # === æˆ¦ç•¥2: æœ€å¼·ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ===
        logger.info("\\nğŸ”¥ æˆ¦ç•¥2: æœ€å¼·ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«")
        
        models = [
            lgb.LGBMClassifier(n_estimators=200, max_depth=4, learning_rate=0.08, random_state=42, verbose=-1),
            RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=10, random_state=43),
            GradientBoostingClassifier(n_estimators=150, max_depth=4, learning_rate=0.1, random_state=44),
            ExtraTreesClassifier(n_estimators=200, max_depth=5, random_state=45)
        ]
        
        strategy2_preds = []
        strategy2_actuals = []
        
        for test_date in test_dates[-12:]:
            train_data = df_sorted[df_sorted['Date'] < test_date]
            test_data = df_sorted[df_sorted['Date'] == test_date]
            
            train_clean = train_data.dropna(subset=['Target'] + feature_cols)
            test_clean = test_data.dropna(subset=['Target'] + feature_cols)
            
            if len(train_clean) < 1000 or len(test_clean) < 2:
                continue
            
            X_train_full = train_clean[feature_cols]
            y_train = train_clean['Target']
            X_test_full = test_clean[feature_cols]
            y_test = test_clean['Target']
            
            # ç‰¹å¾´é‡é¸æŠ
            selected_features = self.advanced_feature_selection(X_train_full, y_train, max_features=30)
            X_train = X_train_full[selected_features]
            X_test = X_test_full[selected_features]
            
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            ensemble_probs = []
            weights = [0.35, 0.25, 0.25, 0.15]  # LightGBMã‚’é‡è¦–
            
            for model, weight in zip(models, weights):
                model.fit(X_train_scaled, y_train)
                probs = model.predict_proba(X_test_scaled)[:, 1]
                ensemble_probs.append(probs * weight)
            
            final_probs = np.sum(ensemble_probs, axis=0)
            
            # ä¸Šä½1éŠ˜æŸ„é¸æŠï¼ˆè¶…å³é¸ï¼‰
            best_idx = np.argmax(final_probs)
            if final_probs[best_idx] >= 0.8:  # 80%ä»¥ä¸Šã®å ´åˆã®ã¿
                selected_actuals = [y_test.iloc[best_idx]]
                strategy2_preds.extend([1])
                strategy2_actuals.extend(selected_actuals)
        
        if strategy2_preds:
            precision2 = sum(strategy2_actuals) / len(strategy2_actuals)
            strategies_results.append(('æœ€å¼·ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«80%é–¾å€¤', precision2, len(strategy2_preds)))
            logger.info(f"  çµæœ: {precision2:.2%}")
        
        # === æˆ¦ç•¥3: PCA + è¶…ä¿å®ˆé¸æŠ ===
        logger.info("\\nğŸ’ æˆ¦ç•¥3: PCAæ¬¡å…ƒå‰Šæ¸› + è¶…ä¿å®ˆé¸æŠ")
        
        strategy3_preds = []
        strategy3_actuals = []
        
        for test_date in test_dates[-10:]:
            train_data = df_sorted[df_sorted['Date'] < test_date]
            test_data = df_sorted[df_sorted['Date'] == test_date]
            
            train_clean = train_data.dropna(subset=['Target'] + feature_cols)
            test_clean = test_data.dropna(subset=['Target'] + feature_cols)
            
            if len(train_clean) < 1000 or len(test_clean) < 1:
                continue
            
            X_train_full = train_clean[feature_cols]
            y_train = train_clean['Target']
            X_test_full = test_clean[feature_cols]
            y_test = test_clean['Target']
            
            # ç‰¹å¾´é‡é¸æŠ
            selected_features = self.advanced_feature_selection(X_train_full, y_train, max_features=50)
            X_train = X_train_full[selected_features]
            X_test = X_test_full[selected_features]
            
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # PCAæ¬¡å…ƒå‰Šæ¸›
            pca = PCA(n_components=0.95)  # 95%ã®æƒ…å ±ã‚’ä¿æŒ
            X_train_pca = pca.fit_transform(X_train_scaled)
            X_test_pca = pca.transform(X_test_scaled)
            
            # é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«
            model = lgb.LGBMClassifier(
                n_estimators=500,
                max_depth=6,
                min_child_samples=5,
                subsample=0.95,
                colsample_bytree=0.85,
                learning_rate=0.03,
                reg_alpha=0.2,
                reg_lambda=0.2,
                random_state=42,
                verbose=-1
            )
            
            model.fit(X_train_pca, y_train)
            probs = model.predict_proba(X_test_pca)[:, 1]
            
            # 85%ä»¥ä¸Šã®ç¢ºç‡ã®å ´åˆã®ã¿é¸æŠ
            high_conf_mask = probs >= 0.85
            if sum(high_conf_mask) > 0:
                selected_actuals = y_test[high_conf_mask].values
                strategy3_preds.extend([1] * len(selected_actuals))
                strategy3_actuals.extend(selected_actuals)
        
        if strategy3_preds:
            precision3 = sum(strategy3_actuals) / len(strategy3_actuals)
            strategies_results.append(('PCA+è¶…ä¿å®ˆ85%é–¾å€¤', precision3, len(strategy3_preds)))
            logger.info(f"  çµæœ: {precision3:.2%}")
        
        return strategies_results
    
    def run_test(self) -> bool:
        """é«˜åº¦çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸ¯ é«˜åº¦çµ±åˆã«ã‚ˆã‚‹90%ç²¾åº¦é”æˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df = self.load_and_prepare_data()
        if df.empty:
            logger.error("ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ
        enhanced_df = self.create_advanced_features(df)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
        target_rate = enhanced_df['Target'].mean()
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å“è³ª: é™½æ€§ç‡{target_rate:.2%}")
        
        # æˆ¦ç•¥å®Ÿè¡Œ
        results = self.run_advanced_strategies(enhanced_df)
        
        # çµæœè¡¨ç¤º
        print("\\n" + "="*80)
        print("ğŸ¯ é«˜åº¦çµ±åˆã«ã‚ˆã‚‹90%ç²¾åº¦é”æˆãƒ†ã‚¹ãƒˆçµæœ")
        print("="*80)
        
        print(f"{'æˆ¦ç•¥å':<30} {'ç²¾åº¦':<12} {'é¸æŠæ•°':<8} {'ç›®æ¨™é”æˆ':<10}")
        print("-"*70)
        
        best_precision = 0
        best_strategy = None
        success_90 = False
        success_85 = False
        
        for name, precision, count in sorted(results, key=lambda x: x[1], reverse=True):
            if precision >= 0.90:
                status = "ğŸ† 90%+"
                success_90 = True
            elif precision >= 0.85:
                status = "ğŸ¥‡ 85%+"
                success_85 = True
            elif precision >= 0.80:
                status = "ğŸ¥ˆ 80%+"
            elif precision >= 0.70:
                status = "ğŸ¥‰ 70%+"
            else:
                status = "âŒ <70%"
            
            print(f"{name:<30} {precision:<12.2%} {count:<8d} {status:<10}")
            
            if precision > best_precision:
                best_precision = precision
                best_strategy = (name, precision, count)
        
        # æˆæœåˆ¤å®š
        if success_90:
            print(f"\\nğŸ† ã€90%ç²¾åº¦é”æˆæˆåŠŸï¼ã€‘")
            print(f"é©šç•°çš„ãªç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸï¼")
        elif success_85:
            print(f"\\nğŸ¥‡ ã€85%ç²¾åº¦é”æˆæˆåŠŸï¼ã€‘")
            print(f"éå¸¸ã«é«˜ã„ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸï¼")
        elif best_precision >= 0.80:
            print(f"\\nğŸ¥ˆ ã€80%ç²¾åº¦é”æˆï¼ã€‘")
            print(f"å„ªç§€ãªç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸï¼")
        else:
            print(f"\\nğŸ“Š ã€çµæœåˆ†æã€‘")
            print(f"æœ€é«˜ç²¾åº¦: {best_precision:.2%}")
        
        if best_strategy:
            print(f"\\nğŸ“Š æœ€å„ªç§€æˆ¦ç•¥: {best_strategy[0]}")
            print(f"é”æˆç²¾åº¦: {best_strategy[1]:.2%}")
            print(f"é¸æŠéŠ˜æŸ„æ•°: {best_strategy[2]}")
            
            # æˆåŠŸè¨˜éŒ²
            success_file = 'advanced_integration_results.txt'
            with open(success_file, 'w') as f:
                f.write(f"é«˜åº¦çµ±åˆãƒ†ã‚¹ãƒˆçµæœ\\n")
                f.write(f"æœ€é«˜ç²¾åº¦: {best_strategy[1]:.2%}\\n")
                f.write(f"æˆ¦ç•¥: {best_strategy[0]}\\n")
                f.write(f"é¸æŠæ•°: {best_strategy[2]}\\n")
                f.write(f"é”æˆæ™‚åˆ»: {datetime.now()}\\n")
                f.write(f"ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ + Yahoo Finance + é«˜åº¦ç‰¹å¾´é‡\\n")
            
            print(f"\\nğŸ’¾ çµæœè¨˜éŒ²ä¿å­˜: {success_file}")
        
        return best_precision >= 0.85

# å®Ÿè¡Œ
if __name__ == "__main__":
    test = AdvancedIntegrationTest()
    success = test.run_test()
    
    if success:
        print("\\nğŸ‰ é«˜åº¦çµ±åˆã«ã‚ˆã‚Š85%ä»¥ä¸Šã®ç²¾åº¦é”æˆæˆåŠŸï¼")
    else:
        print("\\nğŸ“ˆ æ—¢å­˜ã®83.33%ã‚‚å«ã‚ã¦å„ªç§€ãªçµæœã§ã™")