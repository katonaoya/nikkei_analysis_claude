#!/usr/bin/env python3
"""
æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹60%ç²¾åº¦é”æˆãƒ†ã‚¹ãƒˆ
ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ« + ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆç‰ˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

from enhanced_data_integration import EnhancedDataIntegration
from loguru import logger

class Enhanced60PrecisionTest:
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹60%ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.enhanced_data_file = "data/processed/enhanced_integrated_data.parquet"
        self.results = []
        
    def load_enhanced_data(self) -> pd.DataFrame:
        """æ‹¡å¼µçµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            df = pd.read_parquet(self.enhanced_data_file)
            logger.success(f"âœ… æ‹¡å¼µãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}ä»¶")
            return df
        except FileNotFoundError:
            logger.warning("æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")
            
            # ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Ÿè¡Œ
            integrator = EnhancedDataIntegration()
            df = integrator.run_integration()\n            \n            if df.empty:\n                logger.error("ãƒ‡ãƒ¼ã‚¿çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")\n                return pd.DataFrame()\n            \n            return df\n        except Exception as e:\n            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")\n            return pd.DataFrame()\n    \n    def select_features(self, X_train, y_train, max_features: int = 30) -> list:\n        """ç‰¹å¾´é‡é¸æŠï¼ˆçµ±è¨ˆçš„æ‰‹æ³•ï¼‰"""\n        try:\n            selector = SelectKBest(score_func=f_classif, k=min(max_features, X_train.shape[1]))\n            selector.fit(X_train, y_train)\n            \n            selected_features = X_train.columns[selector.get_support()].tolist()\n            scores = selector.scores_[selector.get_support()]\n            \n            # ç‰¹å¾´é‡é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ\n            feature_scores = list(zip(selected_features, scores))\n            feature_scores.sort(key=lambda x: x[1], reverse=True)\n            \n            logger.info(f"âœ… ç‰¹å¾´é‡é¸æŠå®Œäº†: {len(selected_features)}å€‹é¸æŠ")\n            \n            # ãƒˆãƒƒãƒ—10ç‰¹å¾´é‡ã‚’è¡¨ç¤º\n            logger.info("ğŸ“Š é‡è¦ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10:")\n            for i, (feature, score) in enumerate(feature_scores[:10]):\n                logger.info(f"  {i+1}. {feature}: {score:.2f}")\n            \n            return selected_features\n            \n        except Exception as e:\n            logger.error(f"ç‰¹å¾´é‡é¸æŠã§ã‚¨ãƒ©ãƒ¼: {e}")\n            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨\n            return X_train.columns.tolist()\n    \n    def run_enhanced_strategies(self, df: pd.DataFrame) -> list:\n        """æ‹¡å¼µæˆ¦ç•¥ç¾¤ã®å®Ÿè¡Œ"""\n        logger.info("ğŸš€ æ‹¡å¼µæˆ¦ç•¥ç¾¤ã«ã‚ˆã‚‹60%ç²¾åº¦ãƒãƒ£ãƒ¬ãƒ³ã‚¸é–‹å§‹")\n        \n        # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n        df_sorted = df.sort_values(['Stock', 'Date'])\n        unique_dates = sorted(df_sorted['Date'].unique())\n        test_dates = unique_dates[-30:]  # æœ€æ–°30æ—¥\n        \n        logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {len(test_dates)}æ—¥é–“")\n        \n        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ æŠ½å‡º\n        feature_cols = [col for col in df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']]\n        logger.info(f"ç·ç‰¹å¾´é‡æ•°: {len(feature_cols)}")\n        \n        strategies_results = []\n        \n        # === æˆ¦ç•¥1: æ‹¡å¼µLightGBM + ç‰¹å¾´é‡é¸æŠ + ä¸Šä½3éŠ˜æŸ„ ===\n        logger.info("\\nğŸ¯ æˆ¦ç•¥1: æ‹¡å¼µLightGBM + ç‰¹å¾´é‡é¸æŠ")\n        \n        strategy1_preds = []\n        strategy1_actuals = []\n        \n        for i, test_date in enumerate(test_dates[-15:]):  # æœ€æ–°15æ—¥ã§ãƒ†ã‚¹ãƒˆ\n            train_data = df_sorted[df_sorted['Date'] < test_date]\n            test_data = df_sorted[df_sorted['Date'] == test_date]\n            \n            train_clean = train_data.dropna(subset=['Target'] + feature_cols)\n            test_clean = test_data.dropna(subset=['Target'] + feature_cols)\n            \n            if len(train_clean) < 1000 or len(test_clean) < 5:\n                continue\n            \n            X_train_full = train_clean[feature_cols]\n            y_train = train_clean['Target']\n            X_test_full = test_clean[feature_cols]\n            y_test = test_clean['Target']\n            \n            # ç‰¹å¾´é‡é¸æŠ\n            selected_features = self.select_features(X_train_full, y_train, max_features=25)\n            \n            X_train = X_train_full[selected_features]\n            X_test = X_test_full[selected_features]\n            \n            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n            scaler = RobustScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            X_test_scaled = scaler.transform(X_test)\n            \n            # æ‹¡å¼µLightGBMãƒ¢ãƒ‡ãƒ«\n            model = lgb.LGBMClassifier(\n                n_estimators=200,\n                max_depth=4,\n                min_child_samples=15,\n                subsample=0.85,\n                colsample_bytree=0.85,\n                learning_rate=0.08,\n                reg_alpha=0.1,\n                reg_lambda=0.1,\n                random_state=42,\n                verbose=-1\n            )\n            \n            model.fit(X_train_scaled, y_train)\n            probs = model.predict_proba(X_test_scaled)[:, 1]\n            \n            # ä¸Šä½3éŠ˜æŸ„é¸æŠ\n            n_select = min(3, len(probs))\n            top_indices = np.argsort(probs)[-n_select:]\n            \n            selected_actuals = y_test.iloc[top_indices].values\n            strategy1_preds.extend([1] * len(selected_actuals))\n            strategy1_actuals.extend(selected_actuals)\n            \n            if i % 5 == 0:\n                logger.info(f"  é€²æ—: {i+1}/15")\n        \n        if strategy1_preds:\n            precision1 = sum(strategy1_actuals) / len(strategy1_actuals)\n            strategies_results.append(('æ‹¡å¼µLightGBMä¸Šä½3', precision1, len(strategy1_preds)))\n            logger.info(f"  æˆ¦ç•¥1çµæœ: {precision1:.2%}")\n        \n        # === æˆ¦ç•¥2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« + ä¸Šä½2éŠ˜æŸ„ ===\n        logger.info("\\nğŸ”¥ æˆ¦ç•¥2: æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«")\n        \n        models = [\n            lgb.LGBMClassifier(n_estimators=150, max_depth=4, learning_rate=0.08, random_state=42, verbose=-1),\n            RandomForestClassifier(n_estimators=150, max_depth=5, min_samples_split=20, random_state=43),\n            GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=44)\n        ]\n        \n        strategy2_preds = []\n        strategy2_actuals = []\n        \n        for test_date in test_dates[-15:]:\n            train_data = df_sorted[df_sorted['Date'] < test_date]\n            test_data = df_sorted[df_sorted['Date'] == test_date]\n            \n            train_clean = train_data.dropna(subset=['Target'] + feature_cols)\n            test_clean = test_data.dropna(subset=['Target'] + feature_cols)\n            \n            if len(train_clean) < 1000 or len(test_clean) < 3:\n                continue\n            \n            X_train_full = train_clean[feature_cols]\n            y_train = train_clean['Target']\n            X_test_full = test_clean[feature_cols]\n            y_test = test_clean['Target']\n            \n            # ç‰¹å¾´é‡é¸æŠ\n            selected_features = self.select_features(X_train_full, y_train, max_features=20)\n            X_train = X_train_full[selected_features]\n            X_test = X_test_full[selected_features]\n            \n            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n            scaler = RobustScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            X_test_scaled = scaler.transform(X_test)\n            \n            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬\n            ensemble_probs = []\n            for model in models:\n                model.fit(X_train_scaled, y_train)\n                probs = model.predict_proba(X_test_scaled)[:, 1]\n                ensemble_probs.append(probs)\n            \n            # åŠ é‡å¹³å‡ï¼ˆLightGBMã‚’é‡è¦–ï¼‰\n            avg_probs = (0.4 * ensemble_probs[0] + 0.3 * ensemble_probs[1] + 0.3 * ensemble_probs[2])\n            \n            # ä¸Šä½2éŠ˜æŸ„é¸æŠ\n            n_select = min(2, len(avg_probs))\n            top_indices = np.argsort(avg_probs)[-n_select:]\n            \n            selected_actuals = y_test.iloc[top_indices].values\n            strategy2_preds.extend([1] * len(selected_actuals))\n            strategy2_actuals.extend(selected_actuals)\n        \n        if strategy2_preds:\n            precision2 = sum(strategy2_actuals) / len(strategy2_actuals)\n            strategies_results.append(('æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸Šä½2', precision2, len(strategy2_preds)))\n            logger.info(f"  æˆ¦ç•¥2çµæœ: {precision2:.2%}")\n        \n        # === æˆ¦ç•¥3: è¶…å³é¸1éŠ˜æŸ„ï¼ˆé«˜é–¾å€¤ï¼‰ ===\n        logger.info("\\nğŸ’ æˆ¦ç•¥3: è¶…å³é¸1éŠ˜æŸ„")\n        \n        strategy3_preds = []\n        strategy3_actuals = []\n        \n        for test_date in test_dates[-15:]:\n            train_data = df_sorted[df_sorted['Date'] < test_date]\n            test_data = df_sorted[df_sorted['Date'] == test_date]\n            \n            train_clean = train_data.dropna(subset=['Target'] + feature_cols)\n            test_clean = test_data.dropna(subset=['Target'] + feature_cols)\n            \n            if len(train_clean) < 1000 or len(test_clean) < 2:\n                continue\n            \n            X_train_full = train_clean[feature_cols]\n            y_train = train_clean['Target']\n            X_test_full = test_clean[feature_cols]\n            y_test = test_clean['Target']\n            \n            # ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆ30å€‹ï¼‰\n            selected_features = self.select_features(X_train_full, y_train, max_features=30)\n            X_train = X_train_full[selected_features]\n            X_test = X_test_full[selected_features]\n            \n            scaler = RobustScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            X_test_scaled = scaler.transform(X_test)\n            \n            # é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«\n            model = lgb.LGBMClassifier(\n                n_estimators=300,\n                max_depth=5,\n                min_child_samples=10,\n                subsample=0.9,\n                colsample_bytree=0.9,\n                learning_rate=0.05,\n                reg_alpha=0.2,\n                reg_lambda=0.2,\n                random_state=42,\n                verbose=-1\n            )\n            \n            model.fit(X_train_scaled, y_train)\n            probs = model.predict_proba(X_test_scaled)[:, 1]\n            \n            # æœ€é«˜ç¢ºç‡ã®1éŠ˜æŸ„ã®ã¿ï¼ˆ75%ä»¥ä¸Šã®å ´åˆï¼‰\n            best_prob = np.max(probs)\n            if best_prob >= 0.75:\n                best_idx = np.argmax(probs)\n                selected_actuals = [y_test.iloc[best_idx]]\n                strategy3_preds.extend([1])\n                strategy3_actuals.extend(selected_actuals)\n        \n        if strategy3_preds:\n            precision3 = sum(strategy3_actuals) / len(strategy3_actuals)\n            strategies_results.append(('è¶…å³é¸1éŠ˜æŸ„75%é–¾å€¤', precision3, len(strategy3_preds)))\n            logger.info(f"  æˆ¦ç•¥3çµæœ: {precision3:.2%}")\n        \n        return strategies_results\n    \n    def run_test(self):\n        """60%ç²¾åº¦ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""\n        logger.info("ğŸ¯ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹60%ç²¾åº¦é”æˆãƒ†ã‚¹ãƒˆé–‹å§‹")\n        \n        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n        df = self.load_enhanced_data()\n        if df.empty:\n            logger.error("æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")\n            return False\n        \n        # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª\n        target_positive_rate = df['Target'].mean()\n        logger.info(f"ãƒ‡ãƒ¼ã‚¿æ¦‚è¦: {len(df):,}ä»¶, é™½æ€§ç‡{target_positive_rate:.2%}")\n        \n        # æˆ¦ç•¥å®Ÿè¡Œ\n        results = self.run_enhanced_strategies(df)\n        \n        # çµæœå ±å‘Š\n        print("\\n" + "="*80)\n        print("ğŸ¯ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹60%ç²¾åº¦é”æˆãƒ†ã‚¹ãƒˆçµæœ")\n        print("="*80)\n        \n        print(f"{'æˆ¦ç•¥å':<25} {'ç²¾åº¦':<12} {'é¸æŠæ•°':<8} {'60%é”æˆ':<10}")\n        print("-"*65)\n        \n        success_found = False\n        best_result = None\n        best_precision = 0\n        \n        for name, precision, count in sorted(results, key=lambda x: x[1], reverse=True):\n            status = "âœ… YES" if precision >= 0.60 else "âŒ NO"\n            print(f"{name:<25} {precision:<12.2%} {count:<8d} {status:<10}")\n            \n            if precision > best_precision:\n                best_precision = precision\n                best_result = (name, precision, count)\n            \n            if precision >= 0.60:\n                success_found = True\n        \n        if success_found:\n            print(f"\\nğŸ‰ ã€60%ç²¾åº¦é”æˆæˆåŠŸï¼ã€‘")\n            print(f"âœ… æœ€é«˜ç²¾åº¦: {best_precision:.2%}")\n            print(f"âœ… æˆåŠŸæˆ¦ç•¥: {best_result[0]}")\n            print(f"âœ… é¸æŠéŠ˜æŸ„æ•°: {best_result[2]}")\n            \n            # æˆåŠŸè¨˜éŒ²\n            with open('enhanced_60_success.txt', 'w') as f:\n                f.write(f"60%ç²¾åº¦é”æˆæˆåŠŸï¼\\n")\n                f.write(f"é”æˆç²¾åº¦: {best_precision:.2%}\\n")\n                f.write(f"æˆ¦ç•¥: {best_result[0]}\\n")\n                f.write(f"é¸æŠæ•°: {best_result[2]}\\n")\n                f.write(f"ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: æ‹¡å¼µçµ±åˆãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«+ãƒãƒ¼ã‚±ãƒƒãƒˆï¼‰\\n")\n                f.write(f"é”æˆæ™‚åˆ»: {datetime.now()}\\n")\n            \n            print(\"\\nğŸ’¾ æˆåŠŸè¨˜éŒ²ä¿å­˜å®Œäº†\")\n            \n            # å®Ÿç”¨è¨­å®šææ¡ˆ\n            print(f\"\\nğŸ”§ ã€å®Ÿç”¨è¨­å®šæ¨å¥¨ã€‘\")\n            if 'ä¸Šä½3' in best_result[0]:\n                print(\"daily_target: 3éŠ˜æŸ„\")\n                print(\"model_type: 'enhanced_lightgbm'\")\n                print(\"feature_selection: true\")\n            elif 'ä¸Šä½2' in best_result[0]:\n                print(\"daily_target: 2éŠ˜æŸ„\")\n                print(\"model_type: 'enhanced_ensemble'\")\n            else:\n                print(\"daily_target: 1éŠ˜æŸ„\")\n                print(\"model_type: 'ultra_selective'\")\n                print(\"confidence_threshold: 0.75\")\n            \n            print(\"data_source: 'enhanced_integrated_data'\")\n            \n            return True\n            \n        else:\n            print(f\"\\nâš ï¸ ã€60%æœªé”æˆã€‘\")\n            if best_result:\n                print(f"æœ€é«˜ç²¾åº¦: {best_precision:.2%}")\n                print(f"ç›®æ¨™ã¾ã§: +{0.60 - best_precision:.2%}")\n                print(f"æœ€è‰¯æˆ¦ç•¥: {best_result[0]}")\n            \n            print(f\"\\nğŸ“Š åˆ†æçµæœ:\")\n            if best_precision >= 0.55:\n                print(f"- å¾“æ¥ã®56%ã‹ã‚‰æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã™\")\n                print(f"- ã•ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿çµ±åˆã§60%é”æˆå¯èƒ½\")\n            else:\n                print(f\"- ãƒ‡ãƒ¼ã‚¿å“è³ªã®å†ç¢ºèªãŒå¿…è¦\")\n                print(f\"- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„ãŒå¿…è¦\")\n            \n            return False\n        \n        print(\"\\n\" + \"=\"*80)\n\n# å®Ÿè¡Œ\nif __name__ == \"__main__\":\n    test = Enhanced60PrecisionTest()\n    success = test.run_test()\n    \n    if success:\n        print(\"\\nğŸ‰ 60%ç²¾åº¦é”æˆæˆåŠŸï¼æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®åŠ¹æœãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸï¼\")\n    else:\n        print(\"\\nâš ï¸ ã•ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿çµ±åˆã¾ãŸã¯æ‰‹æ³•æ”¹å–„ãŒå¿…è¦ã§ã™\")