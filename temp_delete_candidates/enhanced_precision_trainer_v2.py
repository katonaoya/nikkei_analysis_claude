#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼·åŒ–ç‰ˆé«˜ç²¾åº¦å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  v2.0
æ”¹å–„ææ¡ˆ1-3ã‚’çµ±åˆå®Ÿè£…ï¼š
1. Yahoo Finance 10å¹´åˆ†å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿çµ±åˆ
2. å­£ç¯€æ€§è€ƒæ…®ã—ãŸæ¤œè¨¼æœŸé–“é¸æŠ
3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å°å…¥ã«ã‚ˆã‚‹ç²¾åº¦å®‰å®šåŒ–
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import os
import joblib
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# MLé–¢é€£
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
from sklearn.ensemble import VotingClassifier

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedPrecisionTrainerV2:
    """å¼·åŒ–ç‰ˆé«˜ç²¾åº¦å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  v2.0"""
    
    def __init__(self, stock_data_file: str = None, external_data_file: str = None):
        """åˆæœŸåŒ–"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        if stock_data_file is None:
            stock_data_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
        if external_data_file is None:
            external_data_file = "data/external_extended/external_integrated_10years_20250909_231815.parquet"
        
        self.stock_data_file = stock_data_file
        self.external_data_file = external_data_file
        self.stock_df = None
        self.external_df = None
        self.integrated_df = None
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
        self.models = {}
        self.scalers = {}
        self.selectors = {}
        self.feature_cols = None
        
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {stock_data_file}")
        logger.info(f"å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {external_data_file}")
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        try:
            # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.stock_df = pd.read_parquet(self.stock_data_file)
            logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.stock_df):,}ä»¶, {self.stock_df['Code'].nunique()}éŠ˜æŸ„")
            
            # å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.external_df = pd.read_parquet(self.external_data_file)
            logger.info(f"å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.external_df):,}ä»¶, {len(self.external_df.columns)}ã‚«ãƒ©ãƒ ")
            
            # æ—¥ä»˜å‹å¤‰æ›ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ï¼‰
            self.stock_df['Date'] = pd.to_datetime(self.stock_df['Date']).dt.tz_localize(None)
            self.external_df['Date'] = pd.to_datetime(self.external_df['Date']).dt.tz_localize(None)
            
            return True
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def integrate_external_data(self):
        """å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿çµ±åˆ"""
        logger.info("å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        integrated_df = pd.merge(
            self.stock_df, 
            self.external_df, 
            on='Date', 
            how='left'
        )
        
        # å¤–éƒ¨æŒ‡æ¨™ã®æ¬ æå€¤ã‚’å‰æ–¹è£œå®Œ
        external_cols = [col for col in self.external_df.columns if col != 'Date']
        for col in external_cols:
            if col in integrated_df.columns:
                integrated_df[col] = integrated_df[col].ffill().bfill()
        
        logger.info(f"å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_df):,}ä»¶, {len(integrated_df.columns)}ã‚«ãƒ©ãƒ ")
        
        # çµ±åˆå‰å¾Œã®æ¯”è¼ƒ
        logger.info(f"çµ±åˆå‰ã‚«ãƒ©ãƒ æ•°: {len(self.stock_df.columns)}")
        logger.info(f"çµ±åˆå¾Œã‚«ãƒ©ãƒ æ•°: {len(integrated_df.columns)}")
        logger.info(f"è¿½åŠ ã•ã‚ŒãŸã‚«ãƒ©ãƒ æ•°: {len(integrated_df.columns) - len(self.stock_df.columns)}")
        
        self.integrated_df = integrated_df
        return integrated_df
    
    def create_enhanced_features(self):
        """æ‹¡å¼µç‰¹å¾´é‡ä½œæˆï¼ˆå¤–éƒ¨æŒ‡æ¨™å«ã‚€ï¼‰"""
        logger.info("æ‹¡å¼µç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        enhanced_df = self.integrated_df.copy()
        
        # éŠ˜æŸ„åˆ¥ã«ç‰¹å¾´é‡è¨ˆç®—
        result_dfs = []
        
        for code in enhanced_df['Code'].unique():
            code_df = enhanced_df[enhanced_df['Code'] == code].copy()
            code_df = code_df.sort_values('Date')
            
            # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆæ—¢å­˜ï¼‰
            code_df['Returns'] = code_df['Close'].pct_change(fill_method=None)
            code_df['Volume_MA_20'] = code_df['Volume'].rolling(20).mean()
            code_df['Price_Volume_Trend'] = code_df['Returns'] * code_df['Volume']
            
            # ç§»å‹•å¹³å‡ï¼ˆ4ç¨®é¡ï¼‰
            for window in [5, 10, 20, 50]:
                code_df[f'MA_{window}'] = code_df['Close'].rolling(window).mean()
                code_df[f'MA_{window}_ratio'] = code_df['Close'] / code_df[f'MA_{window}']
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ3ç¨®é¡ï¼‰
            for window in [5, 10, 20]:
                code_df[f'Volatility_{window}'] = code_df['Returns'].rolling(window).std()
            
            # RSIï¼ˆ3ç¨®é¡ï¼‰
            for window in [7, 14, 21]:
                delta = code_df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_df[f'RSI_{window}'] = 100 - (100 / (1 + rs))
            
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
            rolling_mean = code_df['Close'].rolling(20).mean()
            rolling_std = code_df['Close'].rolling(20).std()
            code_df['BB_upper_20'] = rolling_mean + (rolling_std * 2)
            code_df['BB_lower_20'] = rolling_mean - (rolling_std * 2)
            code_df['BB_ratio_20'] = (code_df['Close'] - code_df['BB_lower_20']) / (code_df['BB_upper_20'] - code_df['BB_lower_20'])
            
            # MACD
            exp1 = code_df['Close'].ewm(span=12, adjust=False).mean()
            exp2 = code_df['Close'].ewm(span=26, adjust=False).mean()
            code_df['MACD'] = exp1 - exp2
            code_df['MACD_signal'] = code_df['MACD'].ewm(span=9, adjust=False).mean()
            code_df['MACD_histogram'] = code_df['MACD'] - code_df['MACD_signal']
            
            # OBV
            code_df['OBV'] = (code_df['Volume'] * np.where(code_df['Close'] > code_df['Close'].shift(1), 1, 
                             np.where(code_df['Close'] < code_df['Close'].shift(1), -1, 0))).cumsum()
            
            # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
            low_min = code_df['Low'].rolling(14).min()
            high_max = code_df['High'].rolling(14).max()
            code_df['Stoch_K_14'] = 100 * (code_df['Close'] - low_min) / (high_max - low_min)
            code_df['Stoch_D_14'] = code_df['Stoch_K_14'].rolling(3).mean()
            
            # ğŸ†• å¤–éƒ¨æŒ‡æ¨™ã¨ã®ç›¸é–¢ç‰¹å¾´é‡
            if 'usdjpy_Close' in code_df.columns:
                code_df['Stock_USDJPY_Corr'] = code_df['Returns'].rolling(20).corr(code_df['usdjpy_Daily_Return'])
                code_df['Stock_USDJPY_Ratio'] = code_df['Close'] / code_df['usdjpy_Close']
            
            if 'vix_Close' in code_df.columns:
                code_df['Stock_VIX_Corr'] = code_df['Returns'].rolling(20).corr(code_df['vix_Daily_Return'])
                code_df['VIX_Fear_Factor'] = code_df['vix_Close'] / code_df['vix_MA_20']
            
            if 'nikkei225_Close' in code_df.columns:
                code_df['Stock_Market_Beta'] = code_df['Returns'].rolling(60).cov(code_df['nikkei225_Daily_Return']) / code_df['nikkei225_Daily_Return'].rolling(60).var()
                code_df['Market_Relative_Strength'] = code_df['MA_20'] / code_df['nikkei225_MA_20']
            
            result_dfs.append(code_df)
        
        # çµåˆ
        enhanced_df = pd.concat(result_dfs, ignore_index=True)
        
        # ç›®çš„å¤‰æ•°ä½œæˆ
        logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
        enhanced_df['Target'] = 0
        
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # æ¬ æå€¤é™¤å»ï¼ˆæ®µéšçš„ã«å®Ÿè¡Œï¼‰
        logger.info(f"æ¬ æå€¤é™¤å»å‰: {len(enhanced_df):,}ä»¶")
        
        # é‡è¦ã‚«ãƒ©ãƒ ã®æ¬ æå€¤ç¢ºèª
        important_cols = ['Close', 'Target', 'Returns']
        for col in important_cols:
            if col in enhanced_df.columns:
                null_count = enhanced_df[col].isnull().sum()
                logger.info(f"{col}ã®æ¬ æå€¤: {null_count:,}ä»¶ ({null_count/len(enhanced_df)*100:.2f}%)")
        
        # ç„¡é™å€¤ã‚’å…ˆã«å‡¦ç†
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        
        # æ®µéšçš„æ¬ æå€¤å‡¦ç†
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code'])  # å¿…é ˆã‚«ãƒ©ãƒ 
        if 'Target' in enhanced_df.columns:
            enhanced_df = enhanced_df.dropna(subset=['Target'])
        
        # æ®‹ã‚Šã®æ¬ æå€¤ã‚’å‰æ–¹è£œå®Œ
        enhanced_df = enhanced_df.fillna(method='ffill').fillna(method='bfill')
        enhanced_df = enhanced_df.dropna()  # æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        self.integrated_df = enhanced_df
        logger.info(f"æ‹¡å¼µç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶")
        
        # æ­£ä¾‹ç‡ç¢ºèª
        positive_rate = enhanced_df['Target'].mean()
        logger.info(f"æ­£ä¾‹ç‡: {positive_rate:.3f} ({positive_rate:.1%})")
        
        return enhanced_df
    
    def select_optimal_validation_period(self):
        """å­£ç¯€æ€§è€ƒæ…®ã—ãŸæœ€é©æ¤œè¨¼æœŸé–“é¸æŠ"""
        logger.info("å­£ç¯€æ€§è€ƒæ…®ã—ãŸæœ€é©æ¤œè¨¼æœŸé–“é¸æŠ...")
        
        df_sorted = self.integrated_df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        
        # å€™è£œæœŸé–“å®šç¾©ï¼ˆå­£ç¯€æ€§è€ƒæ…®ï¼‰
        validation_periods = [
            # å®‰å®šæœŸé–“ï¼ˆé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ™‚æœŸå›é¿ï¼‰
            {
                'name': '7æœˆå®‰å®šæœŸ',
                'start_days': 60,  # 7æœˆ1æ—¥é ƒã‹ã‚‰
                'end_days': 30,    # 8æœˆ10æ—¥é ƒã¾ã§
                'description': 'å¤æ¯ã‚Œå‰ã®å®‰å®šæœŸé–“'
            },
            {
                'name': '3æœˆæœŸæœ«å‰',
                'start_days': 190,  # 3æœˆ1æ—¥é ƒã‹ã‚‰
                'end_days': 160,   # 3æœˆ31æ—¥é ƒã¾ã§
                'description': 'æœŸæœ«å‰ã®æ´»ç™ºãªå–å¼•æœŸé–“'
            },
            {
                'name': '10æœˆå®‰å®šæœŸ', 
                'start_days': 120, # 10æœˆ1æ—¥é ƒã‹ã‚‰
                'end_days': 90,    # 10æœˆ31æ—¥é ƒã¾ã§
                'description': 'ç§‹ã®å®‰å®šã—ãŸå–å¼•æœŸé–“'
            }
        ]
        
        best_period = None
        best_score = 0
        
        for period in validation_periods:
            test_start = latest_date - timedelta(days=period['start_days'])
            test_end = latest_date - timedelta(days=period['end_days'])
            
            # æœŸé–“å†…ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            period_data = df_sorted[
                (df_sorted['Date'] >= test_start) & 
                (df_sorted['Date'] <= test_end)
            ]
            
            if len(period_data) < 1000:  # æœ€å°ãƒ‡ãƒ¼ã‚¿é‡ãƒã‚§ãƒƒã‚¯
                continue
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—ï¼ˆå®‰å®šæ€§æŒ‡æ¨™ï¼‰
            period_volatility = period_data.groupby('Date')['Returns'].std().mean()
            
            # æ­£ä¾‹ç‡ã®ãƒãƒ©ãƒ³ã‚¹ç¢ºèª
            positive_rate = period_data['Target'].mean()
            balance_score = 1 - abs(positive_rate - 0.5)  # 0.5ã«è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
            
            # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ Ã— ãƒãƒ©ãƒ³ã‚¹ Ã— ãƒ‡ãƒ¼ã‚¿é‡ï¼‰
            stability_score = (1 / (period_volatility + 0.001))
            data_volume_score = min(len(period_data) / 2000, 1.0)
            total_score = stability_score * balance_score * data_volume_score
            
            logger.info(f"{period['name']}: {test_start.date()} - {test_end.date()}")
            logger.info(f"  ãƒ‡ãƒ¼ã‚¿é‡: {len(period_data):,}ä»¶")
            logger.info(f"  ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {period_volatility:.4f}")
            logger.info(f"  æ­£ä¾‹ç‡: {positive_rate:.3f}")
            logger.info(f"  ç·åˆã‚¹ã‚³ã‚¢: {total_score:.4f}")
            
            if total_score > best_score:
                best_score = total_score
                best_period = {
                    **period,
                    'start_date': test_start,
                    'end_date': test_end,
                    'score': total_score,
                    'data_count': len(period_data),
                    'volatility': period_volatility,
                    'positive_rate': positive_rate
                }
        
        if best_period:
            logger.info(f"ğŸ¯ æœ€é©æ¤œè¨¼æœŸé–“é¸æŠ: {best_period['name']}")
            logger.info(f"æœŸé–“: {best_period['start_date'].date()} - {best_period['end_date'].date()}")
            logger.info(f"ã‚¹ã‚³ã‚¢: {best_period['score']:.4f}")
        
        return best_period
    
    def prepare_features_and_target(self):
        """ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™"""
        logger.info("ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™...")
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ é¸æŠ
        exclude_cols = ['Date', 'Code', 'CompanyName', 'MatchMethod', 'ApiCode', 'Target']
        self.feature_cols = [col for col in self.integrated_df.columns if col not in exclude_cols]
        
        # æ•°å€¤å‹ã®ã¿é¸æŠ
        numeric_cols = self.integrated_df[self.feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        self.feature_cols = numeric_cols
        
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(self.feature_cols)}")
        
        X = self.integrated_df[self.feature_cols]
        y = self.integrated_df['Target']
        
        # ç„¡é™å€¤ã‚„NaNé™¤å»
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        return X, y
    
    def create_ensemble_models(self):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ...")
        
        # ğŸ†• å¼·åŒ–ç‰ˆLightGBM
        lgbm_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 500,      # å¢—å¼·
            'max_depth': 10,          # æ·±ã•å¢—åŠ 
            'min_child_samples': 20,   # èª¿æ•´
            'subsample': 0.85,
            'colsample_bytree': 0.85,
            'learning_rate': 0.02,    # ä½ä¸‹
            'reg_alpha': 0.15,
            'reg_lambda': 0.15,
            'random_state': 42,
            'verbose': -1
        }
        
        # ğŸ†• Random Forest
        rf_params = {
            'n_estimators': 300,
            'max_depth': 12,
            'min_samples_split': 10,
            'min_samples_leaf': 5,
            'max_features': 0.7,
            'random_state': 42,
            'n_jobs': -1
        }
        
        # ğŸ†• XGBoost
        xgb_params = {
            'objective': 'binary:logistic',
            'n_estimators': 400,
            'max_depth': 8,
            'learning_rate': 0.03,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'random_state': 42,
            'eval_metric': 'logloss'
        }
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        models = {
            'lightgbm': LGBMClassifier(**lgbm_params),
            'random_forest': RandomForestClassifier(**rf_params),
            'xgboost': xgb.XGBClassifier(**xgb_params)
        }
        
        return models
    
    def train_ensemble_models(self, validation_period):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")
        
        # ç‰¹å¾´é‡æº–å‚™
        X, y = self.prepare_features_and_target()
        
        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœ€é©æœŸé–“ä½¿ç”¨ï¼‰
        df_sorted = self.integrated_df.sort_values('Date')
        
        if validation_period:
            test_start = validation_period['start_date']
            test_end = validation_period['end_date']
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®30æ—¥åˆ†å‰²
            test_end = df_sorted['Date'].max()
            test_start = test_end - timedelta(days=30)
        
        logger.info(f"è¨“ç·´æœŸé–“: ã€œ {test_start.date()}")
        logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {test_start.date()} ã€œ {test_end.date()}")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_mask = df_sorted['Date'] < test_start
        test_mask = (df_sorted['Date'] >= test_start) & (df_sorted['Date'] <= test_end)
        
        X_train = X[train_mask]
        y_train = y[train_mask]
        X_test = X[test_mask]
        y_test = y[test_mask]
        
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶")
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test):,}ä»¶")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        base_models = self.create_ensemble_models()
        
        # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«å­¦ç¿’
        trained_models = {}
        
        for name, model in base_models.items():
            logger.info(f"{name}ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")
            
            # ç‰¹å¾´é‡é¸æŠï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ã«æœ€é©åŒ–ï¼‰
            k_features = {
                'lightgbm': 40,
                'random_forest': 35, 
                'xgboost': 30
            }
            
            selector = SelectKBest(score_func=f_classif, k=k_features[name])
            X_train_selected = selector.fit_transform(X_train, y_train)
            X_test_selected = selector.transform(X_test)
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
            if name in ['random_forest']:
                scaler = StandardScaler()
            else:
                scaler = RobustScaler()
            
            X_train_scaled = scaler.fit_transform(X_train_selected)
            X_test_scaled = scaler.transform(X_test_selected)
            
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model.fit(X_train_scaled, y_train)
            
            # ä¿å­˜
            trained_models[name] = {
                'model': model,
                'scaler': scaler,
                'selector': selector
            }
            
            logger.info(f"{name}ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
        
        self.models = trained_models
        
        # ğŸ†• ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        return self.evaluate_ensemble_performance(df_sorted[test_mask], X_test, y_test)
    
    def evaluate_ensemble_performance(self, test_df, X_test, y_test):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ€§èƒ½è©•ä¾¡"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ€§èƒ½è©•ä¾¡é–‹å§‹...")
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
        model_predictions = {}
        
        for name, model_data in self.models.items():
            model = model_data['model']
            scaler = model_data['scaler']
            selector = model_data['selector']
            
            X_selected = selector.transform(X_test)
            X_scaled = scaler.transform(X_selected)
            pred_proba = model.predict_proba(X_scaled)[:, 1]
            
            model_predictions[name] = pred_proba
        
        # ğŸ†• ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆåŠ é‡å¹³å‡ï¼‰
        weights = {'lightgbm': 0.5, 'random_forest': 0.3, 'xgboost': 0.2}
        ensemble_proba = np.zeros(len(X_test))
        
        for name, proba in model_predictions.items():
            ensemble_proba += weights[name] * proba
        
        # æ—¥åˆ¥ç²¾åº¦è©•ä¾¡
        return self.evaluate_daily_precision_ensemble(test_df, ensemble_proba, model_predictions)
    
    def evaluate_daily_precision_ensemble(self, test_df, ensemble_proba, model_predictions):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ—¥åˆ¥ç²¾åº¦è©•ä¾¡"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ—¥åˆ¥ç²¾åº¦è©•ä¾¡...")
        
        test_df_copy = test_df.copy()
        test_df_copy['EnsemblePredProba'] = ensemble_proba
        
        # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚‚è¿½åŠ 
        for name, proba in model_predictions.items():
            test_df_copy[f'{name}_PredProba'] = proba
        
        # å–¶æ¥­æ—¥åˆ¥è©•ä¾¡
        unique_dates = sorted(test_df_copy['Date'].unique())
        daily_results = []
        ensemble_results = {'total_correct': 0, 'total_predictions': 0}
        individual_results = {name: {'total_correct': 0, 'total_predictions': 0} for name in model_predictions.keys()}
        
        logger.info(f"æ¤œè¨¼æœŸé–“: {unique_dates[0].date()} ã€œ {unique_dates[-1].date()} ({len(unique_dates)}å–¶æ¥­æ—¥)")
        
        for test_date in unique_dates:
            daily_data = test_df_copy[test_df_copy['Date'] == test_date]
            
            if len(daily_data) < 3:
                continue
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆä¸Šä½3éŠ˜æŸ„ï¼‰
            top3_ensemble = daily_data['EnsemblePredProba'].nlargest(3).index
            ensemble_results_daily = daily_data.loc[top3_ensemble]['Target'].values
            ensemble_correct = np.sum(ensemble_results_daily)
            ensemble_total = len(ensemble_results_daily)
            
            ensemble_results['total_correct'] += ensemble_correct
            ensemble_results['total_predictions'] += ensemble_total
            
            # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
            individual_daily = {}
            for name in model_predictions.keys():
                top3_individual = daily_data[f'{name}_PredProba'].nlargest(3).index
                individual_actual = daily_data.loc[top3_individual]['Target'].values
                individual_correct = np.sum(individual_actual)
                individual_total = len(individual_actual)
                
                individual_results[name]['total_correct'] += individual_correct
                individual_results[name]['total_predictions'] += individual_total
                individual_daily[name] = individual_correct / individual_total if individual_total > 0 else 0
            
            ensemble_precision = ensemble_correct / ensemble_total if ensemble_total > 0 else 0
            
            daily_results.append({
                'date': test_date,
                'ensemble_correct': ensemble_correct,
                'ensemble_total': ensemble_total,
                'ensemble_precision': ensemble_precision,
                'individual_precision': individual_daily,
                'selected_codes': daily_data.loc[top3_ensemble]['Code'].tolist()
            })
            
            logger.info(f"{test_date.strftime('%Y-%m-%d')}: Ensemble {ensemble_correct}/{ensemble_total}={ensemble_precision:.1%} "
                       f"[{', '.join(daily_data.loc[top3_ensemble]['Code'].astype(str).tolist())}]")
        
        # ç·åˆç²¾åº¦è¨ˆç®—
        ensemble_overall = ensemble_results['total_correct'] / ensemble_results['total_predictions']
        individual_overall = {
            name: results['total_correct'] / results['total_predictions'] 
            for name, results in individual_results.items()
        }
        
        logger.info(f"\nğŸ‰ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼çµæœ:")
        logger.info(f"æ¤œè¨¼å–¶æ¥­æ—¥æ•°: {len(daily_results)}æ—¥é–“")
        logger.info(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {ensemble_overall:.4f} ({ensemble_overall:.2%})")
        logger.info(f"å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ç²¾åº¦:")
        for name, precision in individual_overall.items():
            logger.info(f"  {name}: {precision:.4f} ({precision:.2%})")
        
        return {
            'ensemble_precision': ensemble_overall,
            'individual_precision': individual_overall,
            'daily_results': daily_results,
            'ensemble_stats': ensemble_results,
            'individual_stats': individual_results,
            'n_days': len(daily_results)
        }
    
    def save_enhanced_models(self, results, validation_period):
        """å¼·åŒ–ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        ensemble_precision = f"{results['ensemble_precision']:.4f}".replace('.', '')
        
        os.makedirs("models/enhanced_v2", exist_ok=True)
        
        model_file = f"models/enhanced_v2/enhanced_ensemble_model_{len(self.integrated_df)}records_{ensemble_precision}precision_{timestamp}.joblib"
        
        model_data = {
            'models': self.models,
            'feature_cols': self.feature_cols,
            'ensemble_precision': results['ensemble_precision'],
            'individual_precision': results['individual_precision'],
            'results': results,
            'validation_period': validation_period,
            'data_info': {
                'total_records': len(self.integrated_df),
                'n_companies': self.integrated_df['Code'].nunique(),
                'data_period': f"{self.integrated_df['Date'].min()} - {self.integrated_df['Date'].max()}",
                'external_indicators': True,
                'seasonal_optimization': True,
                'ensemble_method': 'weighted_voting'
            }
        }
        
        joblib.dump(model_data, model_file)
        logger.info(f"å¼·åŒ–ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_file
    
    def run_enhanced_training(self):
        """å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ»æ¤œè¨¼å®Ÿè¡Œ"""
        logger.info("ğŸš€ å¼·åŒ–ç‰ˆé«˜ç²¾åº¦å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  v2.0 é–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            if not self.load_data():
                return None
            
            # å¤–éƒ¨æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            self.integrate_external_data()
            
            # ç‰¹å¾´é‡ä½œæˆ
            self.create_enhanced_features()
            
            # æœ€é©æ¤œè¨¼æœŸé–“é¸æŠ
            validation_period = self.select_optimal_validation_period()
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ»æ¤œè¨¼
            results = self.train_ensemble_models(validation_period)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_file = self.save_enhanced_models(results, validation_period)
            
            # çµæœã‚µãƒãƒªãƒ¼
            logger.info(f"\nğŸ¯ å¼·åŒ–ç‰ˆæœ€çµ‚çµæœ:")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(self.integrated_df):,}ä»¶ ({self.integrated_df['Code'].nunique()}éŠ˜æŸ„)")
            logger.info(f"å¤–éƒ¨æŒ‡æ¨™: {len(self.external_df.columns)-1}å€‹çµ±åˆ")
            logger.info(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {results['ensemble_precision']:.4f} ({results['ensemble_precision']:.2%})")
            logger.info(f"æ¤œè¨¼æœŸé–“: {results['n_days']}å–¶æ¥­æ—¥")
            logger.info(f"æœ€é©æœŸé–“: {validation_period['name'] if validation_period else 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}")
            logger.info(f"ä¿å­˜å…ˆ: {model_file}")
            
            return results
            
        except Exception as e:
            logger.error(f"å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ»æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    trainer = EnhancedPrecisionTrainerV2()
    results = trainer.run_enhanced_training()
    
    if results:
        print(f"\nâœ… å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ»æ¤œè¨¼å®Œäº†!")
        print(f"ğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {results['ensemble_precision']:.2%}")
        print(f"ğŸ“ˆ å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ç²¾åº¦:")
        for name, precision in results['individual_precision'].items():
            print(f"  - {name}: {precision:.2%}")
        print(f"ğŸ“… æ¤œè¨¼æœŸé–“: {results['n_days']}å–¶æ¥­æ—¥é–“")
    else:
        print("\nâŒ å¼·åŒ–ç‰ˆå­¦ç¿’ãƒ»æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()