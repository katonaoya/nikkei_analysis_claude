#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥çµŒ225å…¨225éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å®Œå…¨å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
æ­£ç¢ºãªãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦J-Quants APIã‹ã‚‰æ—¥çµŒ225å…¨225éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ã§é«˜é€Ÿå–å¾—
"""
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from pathlib import Path
import logging
from typing import List, Optional, Dict, Tuple
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

JQUANTS_BASE_URL = "https://api.jquants.com/v1"

class Nikkei225CompleteFetcher:
    """æ—¥çµŒ225å…¨225éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å®Œå…¨å–å¾—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†
        self.token_lock = threading.Lock()
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        # æ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ï¼ˆæ­£ç¢ºãªãƒªã‚¹ãƒˆä½¿ç”¨ï¼‰
        self.nikkei225_codes = self._load_complete_nikkei225_codes()
        logger.info(f"æ—¥çµŒ225éŠ˜æŸ„æ•°: {len(self.nikkei225_codes)}éŠ˜æŸ„")
        
        # ä¸¦åˆ—å®Ÿè¡Œç”¨ã®å…±æœ‰ãƒ‡ãƒ¼ã‚¿
        self.results_queue = Queue()
        self.progress_count = 0
        self.progress_lock = threading.Lock()
    
    def _load_complete_nikkei225_codes(self) -> List[str]:
        """æ­£ç¢ºãªæ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ãƒªã‚¹ãƒˆä½¿ç”¨ã€4æ¡å½¢å¼ã«å¤‰æ›ï¼‰"""
        try:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®æ­£ç¢ºãªãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv('/Users/naoya/Desktop/AIé–¢ä¿‚/è‡ªå‹•å£²è²·ãƒ„ãƒ¼ãƒ«/claude_code_develop/docment/ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±/nikkei225_4digit_list.csv')
            
            # 5æ¡ã‚³ãƒ¼ãƒ‰ã‚’4æ¡ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ï¼ˆæœ€å¾Œã®0ã‚’é™¤åŽ»ï¼‰
            codes = []
            for code_str in df['code'].astype(str):
                if len(code_str) == 5 and code_str.endswith('0'):
                    # 5æ¡ã‚³ãƒ¼ãƒ‰ã®å ´åˆã€æœ€å¾Œã®0ã‚’é™¤åŽ»ã—ã¦4æ¡ã«å¤‰æ›
                    code_4digit = code_str[:-1]
                else:
                    # ãã®ä»–ã®å ´åˆã¯4æ¡ã§ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    code_4digit = code_str.zfill(4)
                codes.append(code_4digit)
            
            logger.info(f"æ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: {len(codes)}éŠ˜æŸ„")
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰: {codes[:5]}")
            return codes
        except Exception as e:
            logger.error(f"æ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã€ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³æ–¹å¼ï¼‰"""
        with self.token_lock:
            if self.id_token:
                return self.id_token
            
            logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
            time.sleep(1)  # èªè¨¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“éš”èª¿æ•´
            
            try:
                # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
                auth_payload = {
                    "mailaddress": self.mail_address,
                    "password": self.password
                }
                
                resp = requests.post(
                    f"{JQUANTS_BASE_URL}/token/auth_user",
                    data=json.dumps(auth_payload),
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
                
                if resp.status_code == 429:
                    logger.warning("èªè¨¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    return self._get_id_token()
                    
                resp.raise_for_status()
                refresh_token = resp.json().get("refreshToken")
                
                if not refresh_token:
                    raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                time.sleep(1)
                resp = requests.post(
                    f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                    timeout=30
                )
                
                if resp.status_code == 429:
                    logger.warning("èªè¨¼ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    return self._get_id_token()
                    
                resp.raise_for_status()
                self.id_token = resp.json().get("idToken")
                
                if not self.id_token:
                    raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
                return self.id_token
                
            except Exception as e:
                logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
                raise
    
    def _get_listed_companies(self) -> Dict[str, Dict[str, str]]:
        """ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‚’å–å¾—ã—ã¦æ­£ç¢ºãªéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®š"""
        token = self._get_id_token()
        
        headers = {"Authorization": f"Bearer {token}"}
        url = f"{JQUANTS_BASE_URL}/listed/info"
        
        all_companies = {}
        pagination_key = None
        
        logger.info("ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‚’å–å¾—ä¸­...")
        
        while True:
            params = {}
            if pagination_key:
                params["pagination_key"] = pagination_key
            
            try:
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                response = requests.get(url, headers=headers, params=params, timeout=30)
                
                if response.status_code == 429:
                    logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                    time.sleep(120)
                    continue
                
                response.raise_for_status()
                data = response.json()
                
                if "info" in data:
                    for company in data["info"]:
                        code = company.get("Code", "")  # 5æ¡APIã‚³ãƒ¼ãƒ‰
                        name = company.get("CompanyName", "")
                        
                        # 5æ¡ã‚³ãƒ¼ãƒ‰ã‹ã‚‰4æ¡ã‚³ãƒ¼ãƒ‰ã‚’é€†ç®—ï¼ˆæœ€å¾Œã®0ã‚’é™¤åŽ»ï¼‰
                        if code and len(str(code)) == 5 and str(code).endswith('0'):
                            code_4digit = str(code)[:-1]  # æœ€å¾Œã®0ã‚’é™¤åŽ»
                        else:
                            continue
                        
                        # æ—¥çµŒ225éŠ˜æŸ„ã«å«ã¾ã‚Œã‚‹å ´åˆã®ã¿è¨˜éŒ²
                        if code_4digit in self.nikkei225_codes:
                            all_companies[code_4digit] = {
                                "api_code": code,  # 5æ¡APIã‚³ãƒ¼ãƒ‰
                                "name": name or f"éŠ˜æŸ„{code_4digit}",
                                "code_4digit": code_4digit
                            }
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                pagination_key = data.get("pagination_key")
                if not pagination_key:
                    break
                    
            except Exception as e:
                logger.error(f"ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(5)
                continue
        
        logger.info(f"æ—¥çµŒ225éŠ˜æŸ„ã®ãƒžãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {len(all_companies)}éŠ˜æŸ„")
        return all_companies
    
    def _fetch_stock_data_worker(self, company_info: tuple, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """å€‹åˆ¥éŠ˜æŸ„ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼‰"""
        code_4digit, info = company_info
        api_code = info["api_code"]
        company_name = info["name"]
        
        try:
            token = self._get_id_token()
            headers = {"Authorization": f"Bearer {token}"}
            
            url = f"{JQUANTS_BASE_URL}/prices/daily_quotes"
            params = {
                "code": api_code,
                "from": start_date,
                "to": end_date
            }
            
            all_data = []
            pagination_key = None
            
            while True:
                if pagination_key:
                    params["pagination_key"] = pagination_key
                
                try:
                    time.sleep(0.5)  # ä¸¦åˆ—å®Ÿè¡Œç”¨ã®ã‚ˆã‚ŠçŸ­ã„é–“éš”
                    response = requests.get(url, headers=headers, params=params, timeout=30)
                    
                    if response.status_code == 429:
                        logger.warning(f"éŠ˜æŸ„ {code_4digit}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š30ç§’å¾…æ©Ÿ...")
                        time.sleep(30)
                        continue
                    
                    if response.status_code == 404:
                        logger.warning(f"éŠ˜æŸ„ {code_4digit}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        return None
                    
                    response.raise_for_status()
                    data = response.json()
                    
                    if "daily_quotes" in data:
                        all_data.extend(data["daily_quotes"])
                    
                    pagination_key = data.get("pagination_key")
                    if not pagination_key:
                        break
                        
                except Exception as e:
                    logger.error(f"éŠ˜æŸ„ {code_4digit} ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(2)
                    return None
            
            if all_data:
                df = pd.DataFrame(all_data)
                # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                df["Date"] = pd.to_datetime(df["Date"])
                df = df.sort_values("Date").reset_index(drop=True)
                
                # å¿…è¦ãªåˆ—ã®ã¿é¸æŠž
                columns = ["Date", "Code", "Open", "High", "Low", "Close", "Volume", "AdjustmentFactor", "AdjustmentClose"]
                available_columns = [col for col in columns if col in df.columns]
                df = df[available_columns]
                
                # æ•°å€¤åž‹ã«å¤‰æ›
                numeric_columns = ["Open", "High", "Low", "Close", "Volume", "AdjustmentFactor", "AdjustmentClose"]
                for col in numeric_columns:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # 4æ¡ã‚³ãƒ¼ãƒ‰ã¨ä¼æ¥­åã‚’è¿½åŠ 
                df['Code'] = code_4digit
                df['CompanyName'] = company_name
                
                # é€²æ—æ›´æ–°
                with self.progress_lock:
                    self.progress_count += 1
                    logger.info(f"âœ… {code_4digit} ({company_name}): {len(df)}ä»¶å–å¾— [{self.progress_count}/{len(self.nikkei225_codes)}]")
                
                return df
            
            return None
            
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ {code_4digit} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _fetch_stock_data(self, code: str, company_name: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """æŒ‡å®šéŠ˜æŸ„ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        url = f"{JQUANTS_BASE_URL}/prices/daily_quotes"
        headers = {"Authorization": f"Bearer {self._get_id_token()}"}
        params = {
            "code": code,
            "from": start_date,
            "to": end_date
        }
        
        try:
            response = requests.get(url, headers=headers, params=params, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            if "daily_quotes" not in data:
                return None
                
            df = pd.DataFrame(data["daily_quotes"])
            if df.empty:
                return None
            
            # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
            df["Date"] = pd.to_datetime(df["Date"])
            df = df.sort_values("Date").reset_index(drop=True)
            
            # å¿…è¦ãªåˆ—ã®ã¿é¸æŠž
            columns = ["Date", "Code", "Open", "High", "Low", "Close", "Volume", "AdjustmentFactor", "AdjustmentClose"]
            available_columns = [col for col in columns if col in df.columns]
            df = df[available_columns]
            
            # æ•°å€¤åž‹ã«å¤‰æ›
            numeric_columns = ["Open", "High", "Low", "Close", "Volume", "AdjustmentFactor", "AdjustmentClose"]
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            df['CompanyName'] = company_name
            
            # é€²æ—æ›´æ–°
            with self.progress_lock:
                self.progress_count += 1
                logger.info(f"âœ… {code} ({company_name}): {len(df)}ä»¶å–å¾— [{self.progress_count}/{len(self.nikkei225_codes)}]")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ {code} ({company_name}): ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ - {e}")
            return None
    
    def _worker_thread(self, codes_companies: List[Tuple[str, str]], start_date: str, end_date: str):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰é–¢æ•°"""
        for code, company_name in codes_companies:
            try:
                df = self._fetch_stock_data(code, company_name, start_date, end_date)
                if df is not None and not df.empty:
                    self.results_queue.put(df)
                time.sleep(0.1)  # APIåˆ¶é™å¯¾å¿œ
            except Exception as e:
                logger.error(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ©ãƒ¼ {code}: {e}")
    
    def fetch_complete_nikkei225_data(self, years: int = 10) -> pd.DataFrame:
        """æ—¥çµŒ225å…¨225éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨å–å¾—"""
        # æœŸé–“è¨­å®š
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=365 * years + 30)  # ä½™è£•ã‚’ã‚‚ã£ã¦30æ—¥è¿½åŠ 
        
        start_date_str = start_date.strftime('%Y-%m-%d')
        end_date_str = end_date.strftime('%Y-%m-%d')
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“: {start_date_str} ã€œ {end_date_str}")
        
        # ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‚’å–å¾—
        companies_mapping = self._get_listed_companies()
        
        if not companies_mapping:
            logger.error("éŠ˜æŸ„ãƒžãƒƒãƒ”ãƒ³ã‚°å–å¾—ã«å¤±æ•—")
            return pd.DataFrame()
            
        logger.info(f"æ—¥çµŒ225éŠ˜æŸ„ã®ãƒžãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {len(companies_mapping)}éŠ˜æŸ„")
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®š  
        max_workers = 8
        logger.info(f"ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        logger.info(f"æ—¥çµŒ225å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—é–‹å§‹: {len(companies_mapping)}éŠ˜æŸ„")
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        all_stock_data = []
        company_items = list(companies_mapping.items())
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å…¨éŠ˜æŸ„ã®ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            futures = {
                executor.submit(self._fetch_stock_data_worker, company_info, start_date_str, end_date_str): company_info[0]
                for company_info in company_items
            }
            
            # çµæžœã‚’åŽé›†
            for future in as_completed(futures):
                code = futures[future]
                try:
                    result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    if result is not None and not result.empty:
                        all_stock_data.append(result)
                except Exception as e:
                    logger.error(f"éŠ˜æŸ„ {code} ã®ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        all_dataframes = all_stock_data
        
        if not all_dataframes:
            logger.error("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        result = pd.concat(all_dataframes, ignore_index=True)
        result = result.sort_values(['Code', 'Date']).reset_index(drop=True)
        
        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¿½åŠ 
        logger.info("ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—é–‹å§‹...")
        result = self._add_technical_indicators(result)
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        output_file = f"nikkei225_complete_10years_{end_date_str}.csv"
        result.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        return result
    
    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ """
        result_dfs = []
        
        for code in df['Code'].unique():
            group = df[df['Code'] == code].copy()
            group = group.sort_values('Date').reset_index(drop=True)
            
            # åŸºæœ¬æŒ‡æ¨™
            group['Returns'] = group['Close'].pct_change()
            group['Volume_MA_5'] = group['Volume'].rolling(window=5, min_periods=1).mean()
            
            # ç§»å‹•å¹³å‡
            for window in [5, 25, 75]:
                group[f'MA_{window}'] = group['Close'].rolling(window=window, min_periods=1).mean()
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            group['Volatility_20'] = group['Returns'].rolling(window=20, min_periods=1).std()
            
            # RSI
            def calculate_rsi(prices, window=14):
                delta = prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()
                rs = gain / loss
                rsi = 100 - (100 / (1 + rs))
                return rsi
            
            group['RSI_14'] = calculate_rsi(group['Close'])
            
            # MACD
            exp1 = group['Close'].ewm(span=12, min_periods=1).mean()
            exp2 = group['Close'].ewm(span=26, min_periods=1).mean()
            group['MACD'] = exp1 - exp2
            group['MACD_Signal'] = group['MACD'].ewm(span=9, min_periods=1).mean()
            group['MACD_Histogram'] = group['MACD'] - group['MACD_Signal']
            
            result_dfs.append(group)
        
        return pd.concat(result_dfs, ignore_index=True)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        fetcher = Nikkei225CompleteFetcher()
        
        # æ—¥çµŒ225å…¨225éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ10å¹´é–“ï¼‰
        df = fetcher.fetch_complete_nikkei225_data(years=10)
        
        if not df.empty:
            print(f"\nâœ… æ—¥çµŒ225å…¨225éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å®Œå…¨å–å¾—å®Œäº†")
            print(f"ðŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
            print(f"  - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}")
            print(f"  - éŠ˜æŸ„æ•°: {df['Code'].nunique()}")
            print(f"  - æœŸé–“: {df['Date'].min()} ã€œ {df['Date'].max()}")
            print(f"  - å¹³å‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°/éŠ˜æŸ„: {len(df) // df['Code'].nunique():,}")
            
            # å–å¾—éŠ˜æŸ„ã®è©³ç´°è¡¨ç¤º
            company_counts = df.groupby(['Code', 'CompanyName']).size().reset_index(name='Count')
            print(f"\nðŸ“‹ å–å¾—éŠ˜æŸ„è©³ç´°:")
            for _, row in company_counts.iterrows():
                print(f"  {row['Code']}: {row['CompanyName']} ({row['Count']:,}ä»¶)")
                
        else:
            print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()