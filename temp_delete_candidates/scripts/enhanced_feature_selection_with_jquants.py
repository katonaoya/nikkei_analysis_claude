#!/usr/bin/env python3
"""
J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã‚’å«ã‚€åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ
55.3%ä»¥ä¸Šã®ç²¾åº¦ç¢ºå®Ÿé”æˆç‰ˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class EnhancedJQuantsFeatureSelector:
    """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã‚’å«ã‚€åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
    def load_and_prepare_data(self, sample_size=50000):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size:,}ï¼‰")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(df) > sample_size:
            df = df.sort_values('Date').tail(sample_size)
            logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(df):,}ä»¶")
        
        return df
    
    def create_jquants_like_features(self, df):
        """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®å®Œå…¨å¾©å…ƒ"""
        logger.info("ğŸ”§ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # 1. å¸‚å ´å…¨ä½“æŒ‡æ¨™ï¼ˆæŒ‡æ•°çš„ç‰¹å¾´é‡ï¼‰
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std'],
            'Volume': ['mean', 'std'],
            'Returns': 'mean'
        }).round(6)
        
        daily_market.columns = [
            'Market_Close_Mean', 'Market_Close_Std', 
            'Market_Volume_Mean', 'Market_Volume_Std',
            'Market_Return_Mean'
        ]
        daily_market = daily_market.reset_index()
        
        # 2. ã‚»ã‚¯ã‚¿ãƒ¼æ¨¡æ“¬ï¼ˆã‚³ãƒ¼ãƒ‰å‰2æ¡ã§ã‚»ã‚¯ã‚¿ãƒ¼åˆ†é¡ï¼‰
        df['Sector_Code'] = df['Code'].astype(str).str[:2]
        sector_daily = df.groupby(['Date', 'Sector_Code'])['Close'].mean().reset_index()
        sector_daily.columns = ['Date', 'Sector_Code', 'Sector_Avg_Price']
        
        # 3. ä¿¡ç”¨å–å¼•æ¨¡æ“¬æŒ‡æ¨™
        df['Volume_MA5'] = df.groupby('Code')['Volume'].rolling(5).mean().reset_index(0, drop=True)
        df['Volume_Shock'] = df['Volume'] / (df['Volume_MA5'] + 1e-6)
        
        # ä¾¡æ ¼ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ç©ºå£²ã‚Šåœ§åŠ›ã®ä»£ç†æŒ‡æ¨™ã¨ã™ã‚‹
        df['Price_Volatility_5d'] = df.groupby('Code')['Close'].rolling(5).std().reset_index(0, drop=True)
        df['Volatility_Rank'] = df.groupby('Date')['Price_Volatility_5d'].rank(pct=True)
        
        # 4. å¸‚å ´ç›¸å¯¾æŒ‡æ¨™
        df = df.merge(daily_market, on='Date', how='left')
        df = df.merge(sector_daily, on=['Date', 'Sector_Code'], how='left')
        
        df['Market_Relative_Return'] = df['Returns'] - df['Market_Return_Mean'] 
        df['Market_Relative_Price'] = df['Close'] / (df['Market_Close_Mean'] + 1e-6)
        df['Sector_Relative_Price'] = df['Close'] / (df['Sector_Avg_Price'] + 1e-6)
        df['Market_Volume_Relative'] = df['Volume'] / (df['Market_Volume_Mean'] + 1e-6)
        
        # 5. å¤–å›½äººæŠ•è³‡å®¶æ¨¡æ“¬ï¼ˆå¤§å‹æ ªã§ã®ç‰¹åˆ¥ãªå‹•ãï¼‰
        df['Market_Cap_Proxy'] = df['Close'] * df['Volume']  # ç°¡æ˜“æ™‚ä¾¡ç·é¡
        df['Large_Cap_Flag'] = (df.groupby('Date')['Market_Cap_Proxy'].rank(pct=True) > 0.8).astype(int)
        
        # å¤§å‹æ ªã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ã¨å€‹åˆ¥éŠ˜æŸ„ã®ä¹–é›¢
        large_cap_return = df[df['Large_Cap_Flag'] == 1].groupby('Date')['Returns'].mean()
        large_cap_return = large_cap_return.reset_index()
        large_cap_return.columns = ['Date', 'Large_Cap_Return']
        
        df = df.merge(large_cap_return, on='Date', how='left')
        df['Foreign_Proxy'] = df['Returns'] - df['Large_Cap_Return']
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        logger.info(f"âœ… J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        return df
    
    def categorize_features(self, df):
        """ç‰¹å¾´é‡ã®åˆ†é¡"""
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector_Code'
        }
        
        all_features = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ç”±æ¥ï¼‰
        basic_features = [col for col in all_features if not any(
            keyword in col for keyword in ['Market', 'Sector', 'Volume_Shock', 'Volatility', 'Foreign', 'Large_Cap']
        )]
        
        # J-Quantsãƒ©ã‚¤ã‚¯æ‹¡å¼µç‰¹å¾´é‡
        jquants_features = [col for col in all_features if any(
            keyword in col for keyword in ['Market', 'Sector', 'Volume_Shock', 'Volatility', 'Foreign', 'Large_Cap']
        )]
        
        logger.info(f"åŸºæœ¬ç‰¹å¾´é‡: {len(basic_features)}å€‹")
        logger.info(f"J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡: {len(jquants_features)}å€‹") 
        logger.info(f"å…¨ç‰¹å¾´é‡: {len(all_features)}å€‹")
        
        return basic_features, jquants_features, all_features
    
    def quick_baseline_test(self, df, basic_features, jquants_features):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ç¢ºèª"""
        logger.info("âš¡ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ç¢ºèª...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        y = clean_df['Binary_Direction'].astype(int)
        
        results = {}
        
        # J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿ã§ãƒ†ã‚¹ãƒˆï¼ˆ55.3%ã‚’å†ç¾ã—ãŸã„ï¼‰
        if jquants_features:
            logger.info("ğŸ¯ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿ã§ãƒ†ã‚¹ãƒˆ...")
            X_jquants = clean_df[jquants_features]
            X_jquants_scaled = self.scaler.fit_transform(X_jquants)
            
            # LogisticRegressionã§è©•ä¾¡ï¼ˆ55.3%ã‚’é”æˆã—ãŸãƒ¢ãƒ‡ãƒ«ï¼‰
            tscv = TimeSeriesSplit(n_splits=2)
            lr_scores = []
            
            for train_idx, test_idx in tscv.split(X_jquants_scaled):
                X_train = X_jquants_scaled[train_idx]
                X_test = X_jquants_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                lr = LogisticRegression(
                    C=0.01, class_weight='balanced', 
                    max_iter=1000, random_state=42
                )
                lr.fit(X_train, y_train)
                pred = lr.predict(X_test)
                lr_scores.append(accuracy_score(y_test, pred))
            
            jquants_accuracy = np.mean(lr_scores)
            results['jquants_only'] = jquants_accuracy
            logger.info(f"J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿: {jquants_accuracy:.1%}")
        
        # åŸºæœ¬ç‰¹å¾´é‡ã®ã¿
        if basic_features:
            X_basic = clean_df[basic_features]
            X_basic_scaled = self.scaler.fit_transform(X_basic)
            
            lr_scores = []
            for train_idx, test_idx in tscv.split(X_basic_scaled):
                X_train = X_basic_scaled[train_idx]
                X_test = X_basic_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                lr = LogisticRegression(
                    C=0.01, class_weight='balanced',
                    max_iter=1000, random_state=42
                )
                lr.fit(X_train, y_train)
                pred = lr.predict(X_test)
                lr_scores.append(accuracy_score(y_test, pred))
            
            basic_accuracy = np.mean(lr_scores)
            results['basic_only'] = basic_accuracy
            logger.info(f"åŸºæœ¬ç‰¹å¾´é‡ã®ã¿: {basic_accuracy:.1%}")
        
        return results
    
    def comprehensive_feature_selection(self, df, all_features):
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ"""
        logger.info("ğŸ” åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠé–‹å§‹...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[all_features]
        y = clean_df['Binary_Direction'].astype(int)
        
        # æ¨™æº–åŒ–
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        feature_rankings = {}
        
        # 1. Fçµ±è¨ˆé‡
        logger.info("ğŸ“Š Fçµ±è¨ˆé‡ã«ã‚ˆã‚‹é¸æŠ...")
        f_scores = f_classif(X_scaled, y)[0]
        f_ranking = list(zip(X.columns, f_scores))
        f_ranking.sort(key=lambda x: x[1], reverse=True)
        feature_rankings['f_statistic'] = f_ranking
        
        # 2. ç›¸äº’æƒ…å ±é‡
        logger.info("ğŸ”— ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹é¸æŠ...")
        mi_scores = mutual_info_classif(X_scaled, y, random_state=42)
        mi_ranking = list(zip(X.columns, mi_scores))
        mi_ranking.sort(key=lambda x: x[1], reverse=True)
        feature_rankings['mutual_info'] = mi_ranking
        
        # 3. RandomForesté‡è¦åº¦
        logger.info("ğŸŒ² RandomForesté‡è¦åº¦ã«ã‚ˆã‚‹é¸æŠ...")
        rf = RandomForestClassifier(
            n_estimators=100, max_depth=8, 
            class_weight='balanced', random_state=42, n_jobs=-1
        )
        rf.fit(X_scaled, y)
        rf_ranking = list(zip(X.columns, rf.feature_importances_))
        rf_ranking.sort(key=lambda x: x[1], reverse=True)
        feature_rankings['random_forest'] = rf_ranking
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        ensemble_scores = self.create_ensemble_ranking(feature_rankings)
        
        return ensemble_scores
    
    def create_ensemble_ranking(self, rankings_dict):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        logger.info("ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦è¨ˆç®—ä¸­...")
        
        ensemble_scores = {}
        
        for method, rankings in rankings_dict.items():
            if rankings:
                scores = [score for name, score in rankings]
                if len(scores) > 0 and max(scores) > min(scores):
                    min_score, max_score = min(scores), max(scores)
                    for name, score in rankings:
                        normalized_score = (score - min_score) / (max_score - min_score)
                        if name not in ensemble_scores:
                            ensemble_scores[name] = []
                        ensemble_scores[name].append(normalized_score)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢
        final_scores = {}
        for name, scores in ensemble_scores.items():
            final_scores[name] = np.mean(scores)
        
        ensemble_ranking = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸Šä½15ç‰¹å¾´é‡:")
        for i, (feature, score) in enumerate(ensemble_ranking[:15]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {score:.4f}")
        
        return ensemble_ranking
    
    def progressive_testing(self, df, all_features, feature_ranking):
        """æ®µéšçš„ç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“ˆ æ®µéšçš„ç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        y = clean_df['Binary_Direction'].astype(int)
        
        # ç‰¹å¾´é‡æ•°ã‚’æ®µéšçš„ã«å¢—åŠ 
        feature_counts = [3, 5, 7, 10, 15, 20, 25]
        results = {}
        
        for n_features in feature_counts:
            if n_features > len(feature_ranking):
                continue
                
            # ä¸Šä½Nç‰¹å¾´é‡é¸æŠ
            selected_features = [name for name, score in feature_ranking[:n_features]]
            X = clean_df[selected_features]
            X_scaled = self.scaler.fit_transform(X)
            
            # æ™‚ç³»åˆ—åˆ†å‰²ã§è©•ä¾¡
            tscv = TimeSeriesSplit(n_splits=3)
            lr_scores = []
            rf_scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                # LogisticRegression
                lr = LogisticRegression(
                    C=0.01, class_weight='balanced',
                    max_iter=1000, random_state=42
                )
                lr.fit(X_train, y_train)
                lr_pred = lr.predict(X_test)
                lr_scores.append(accuracy_score(y_test, lr_pred))
                
                # RandomForest
                rf = RandomForestClassifier(
                    n_estimators=100, max_depth=8,
                    class_weight='balanced', random_state=42, n_jobs=-1
                )
                rf.fit(X_train, y_train)
                rf_pred = rf.predict(X_test)
                rf_scores.append(accuracy_score(y_test, rf_pred))
            
            lr_avg = np.mean(lr_scores)
            rf_avg = np.mean(rf_scores)
            best_score = max(lr_avg, rf_avg)
            best_model = "LogisticRegression" if lr_avg > rf_avg else "RandomForest"
            
            results[n_features] = {
                'lr_accuracy': lr_avg,
                'rf_accuracy': rf_avg,
                'best_score': best_score,
                'best_model': best_model,
                'features': selected_features
            }
            
            logger.info(f"ç‰¹å¾´é‡æ•°{n_features:2d}: LR={lr_avg:.1%}, RF={rf_avg:.1%}, æœ€é«˜={best_score:.1%}({best_model})")
        
        # æœ€é«˜æ€§èƒ½ã®ç‰¹å¾´é‡æ•°ç‰¹å®š
        best_n = max(results.keys(), key=lambda k: results[k]['best_score'])
        best_result = results[best_n]
        
        logger.info(f"\nğŸ¯ æœ€é«˜æ€§èƒ½: ç‰¹å¾´é‡æ•°{best_n}, ç²¾åº¦{best_result['best_score']:.1%} ({best_result['best_model']})")
        logger.info("æœ€é©ç‰¹å¾´é‡:")
        for i, feature in enumerate(best_result['features']):
            logger.info(f"  {i+1:2d}. {feature}")
        
        return results, best_result
    
    def final_validation(self, df, best_features, best_model_name):
        """æœ€çµ‚æ¤œè¨¼"""
        logger.info(f"ğŸ¯ æœ€çµ‚æ¤œè¨¼: {len(best_features)}ç‰¹å¾´é‡, {best_model_name}")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[best_features]
        y = clean_df['Binary_Direction'].astype(int)
        X_scaled = self.scaler.fit_transform(X)
        
        # æ™‚ç³»åˆ—åˆ†å‰²ã§æœ€çµ‚æ¤œè¨¼
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            if best_model_name == "LogisticRegression":
                model = LogisticRegression(
                    C=0.01, class_weight='balanced',
                    max_iter=1000, random_state=42
                )
            else:
                model = RandomForestClassifier(
                    n_estimators=100, max_depth=8,
                    class_weight='balanced', random_state=42, n_jobs=-1
                )
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            logger.info(f"Fold {fold+1}: {accuracy:.1%}")
        
        final_accuracy = np.mean(scores)
        std_accuracy = np.std(scores)
        
        logger.info(f"\nğŸ¯ æœ€çµ‚çµæœ: {final_accuracy:.1%} Â± {std_accuracy:.1%}")
        
        return final_accuracy, std_accuracy, scores

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã‚’å«ã‚€åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠé–‹å§‹")
    logger.info("ç›®æ¨™: 55.3%ä»¥ä¸Šã®ç²¾åº¦é”æˆ")
    
    selector = EnhancedJQuantsFeatureSelector()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = selector.load_and_prepare_data(sample_size=50000)
        if df is None:
            return
        
        # 2. J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆ
        df = selector.create_jquants_like_features(df)
        
        # 3. ç‰¹å¾´é‡åˆ†é¡
        basic_features, jquants_features, all_features = selector.categorize_features(df)
        
        # 4. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºèª
        baseline_results = selector.quick_baseline_test(df, basic_features, jquants_features)
        
        if 'jquants_only' in baseline_results:
            jquants_accuracy = baseline_results['jquants_only']
            logger.info(f"ğŸ¯ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿ç²¾åº¦: {jquants_accuracy:.1%}")
            
            if jquants_accuracy >= 0.553:  # 55.3%
                logger.info("âœ… ç›®æ¨™55.3%ã‚’é”æˆï¼J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ãŒæœ‰åŠ¹")
            else:
                logger.warning(f"âš ï¸  ç›®æ¨™55.3%ã«æœªé” (å·®: {(0.553 - jquants_accuracy)*100:.1f}%)")
        
        # 5. åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ
        feature_ranking = selector.comprehensive_feature_selection(df, all_features)
        
        # 6. æ®µéšçš„ãƒ†ã‚¹ãƒˆ
        progressive_results, best_result = selector.progressive_testing(df, all_features, feature_ranking)
        
        # 7. æœ€çµ‚æ¤œè¨¼
        final_accuracy, std_accuracy, fold_scores = selector.final_validation(
            df, best_result['features'], best_result['best_model']
        )
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*60)
        
        if 'jquants_only' in baseline_results:
            logger.info(f"J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿: {baseline_results['jquants_only']:.1%}")
        if 'basic_only' in baseline_results:
            logger.info(f"åŸºæœ¬ç‰¹å¾´é‡ã®ã¿: {baseline_results['basic_only']:.1%}")
        
        logger.info(f"æœ€é©ç‰¹å¾´é‡é¸æŠå¾Œ: {final_accuracy:.1%} Â± {std_accuracy:.1%}")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(best_result['features'])}")
        logger.info(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {best_result['best_model']}")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_accuracy = 0.553  # 55.3%
        if final_accuracy >= target_accuracy:
            logger.info(f"ğŸ‰ ç›®æ¨™é”æˆï¼ {final_accuracy:.1%} >= {target_accuracy:.1%}")
        else:
            logger.warning(f"âš ï¸  ç›®æ¨™æœªé”: {final_accuracy:.1%} < {target_accuracy:.1%}")
            logger.info(f"å·®: {(target_accuracy - final_accuracy)*100:.1f}%")
        
        logger.info("\næœ€é©ç‰¹å¾´é‡:")
        for i, feature in enumerate(best_result['features']):
            logger.info(f"  {i+1:2d}. {feature}")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()