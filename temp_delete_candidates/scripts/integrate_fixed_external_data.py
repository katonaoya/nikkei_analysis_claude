#!/usr/bin/env python3
"""
ä¿®æ­£ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

def integrate_external_data():
    """ä¿®æ­£ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ"""
    logger.info("ğŸ”— ä¿®æ­£ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ")
    
    # ä¿®æ­£ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    external_file = Path("data/external/external_macro_data_fixed.parquet")
    external_data = pd.read_parquet(external_file)
    external_data['Date'] = pd.to_datetime(external_data['Date'])
    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_data):,}ä»¶")
    logger.info(f"æœŸé–“: {external_data['Date'].min().date()} - {external_data['Date'].max().date()}")
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    processed_dir = Path("data/processed")
    existing_files = list(processed_dir.glob("*.parquet"))
    existing_data = pd.read_parquet(existing_files[0])
    existing_data['Date'] = pd.to_datetime(existing_data['Date'])
    logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data):,}ä»¶")
    logger.info(f"æœŸé–“: {existing_data['Date'].min().date()} - {existing_data['Date'].max().date()}")
    
    # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã§çµ±åˆ
    logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
    integrated_data = existing_data.merge(external_data, on='Date', how='left')
    
    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ç‰¹å®š
    external_cols = [col for col in external_data.columns if col != 'Date']
    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {len(external_cols)}å€‹")
    
    # çµ±åˆå‰ã®æ¬ æå€¤çµ±è¨ˆ
    initial_missing = integrated_data[external_cols].isnull().sum()
    logger.info(f"çµ±åˆç›´å¾Œã®æ¬ æå€¤:")
    for col in external_cols[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
        missing_count = initial_missing[col]
        logger.info(f"  {col}: {missing_count:,}ä»¶ ({missing_count/len(integrated_data)*100:.1f}%)")
    
    # å‰åŸ‹ã‚å‡¦ç†ï¼ˆå¹³æ—¥ã®æ ªå¼ãƒ‡ãƒ¼ã‚¿ã«é€±æœ«ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’é©ç”¨ï¼‰
    logger.info("ğŸ“… å‰åŸ‹ã‚å‡¦ç†å®Ÿè¡Œä¸­...")
    integrated_data[external_cols] = integrated_data[external_cols].fillna(method='ffill')
    
    # å‰åŸ‹ã‚å¾Œã®æ¬ æå€¤çµ±è¨ˆ
    after_ffill_missing = integrated_data[external_cols].isnull().sum()
    logger.info(f"å‰åŸ‹ã‚å¾Œã®æ¬ æå€¤:")
    for col in external_cols[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
        missing_count = after_ffill_missing[col]
        logger.info(f"  {col}: {missing_count:,}ä»¶ ({missing_count/len(integrated_data)*100:.1f}%)")
    
    # æ®‹ã‚Šã®æ¬ æå€¤ã¯0ã§åŸ‹ã‚ã‚‹
    integrated_data[external_cols] = integrated_data[external_cols].fillna(0)
    
    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_data):,}ä»¶")
    logger.info(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(integrated_data.columns)}")
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
    integrated_file = processed_dir / "integrated_with_external.parquet"
    integrated_data.to_parquet(integrated_file, index=False)
    logger.info(f"ğŸ’¾ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {integrated_file}")
    
    return integrated_data, external_cols

def validate_and_sample():
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º"""
    logger.info("âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")
    
    integrated_file = Path("data/processed/integrated_with_external.parquet")
    data = pd.read_parquet(integrated_file)
    
    logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(data):,}ä»¶, {len(data.columns)}ã‚«ãƒ©ãƒ ")
    
    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ç¢ºèª
    external_pattern_cols = [col for col in data.columns if any(pattern in col for pattern in ['us_10y', 'sp500', 'usd_jpy', 'nikkei', 'vix'])]
    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {len(external_pattern_cols)}å€‹")
    
    # å„ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦
    logger.info(f"\nğŸ“Š å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦:")
    for col in external_pattern_cols:
        non_null_count = data[col].notna().sum()
        non_zero_count = (data[col] != 0).sum()
        logger.info(f"  {col:20s}: éæ¬ æ {non_null_count:,}ä»¶ ({non_null_count/len(data)*100:.1f}%), éã‚¼ãƒ­ {non_zero_count:,}ä»¶ ({non_zero_count/len(data)*100:.1f}%)")
    
    # Binary_Directionå­˜åœ¨ç¢ºèª
    if 'Binary_Direction' in data.columns:
        valid_targets = data['Binary_Direction'].notna().sum()
        logger.info(f"\nğŸ¯ äºˆæ¸¬å¯¾è±¡: {valid_targets:,}ä»¶")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆæœ€æ–°10ä»¶ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    logger.info(f"\nğŸ“‹ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€æ–°10ä»¶ï¼‰:")
    sample_cols = ['Date'] + [col for col in external_pattern_cols if 'value' in col][:5]  # å€¤ã‚«ãƒ©ãƒ ã®ã¿
    sample_data = data[sample_cols].tail(10)
    print(sample_data.to_string(index=False))
    
    # çµ±è¨ˆæƒ…å ±
    value_cols = [col for col in external_pattern_cols if 'value' in col]
    if value_cols:
        logger.info(f"\nğŸ“Š å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆï¼ˆå€¤ã‚«ãƒ©ãƒ ï¼‰:")
        stats = data[value_cols].describe()
        print(stats.round(2).to_string())

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ ä¿®æ­£ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    
    try:
        # 1. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        integrated_data, external_cols = integrate_external_data()
        
        # 2. çµ±åˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        validate_and_sample()
        
        logger.info("\n" + "="*80)
        logger.info("âœ… å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
        logger.info("="*80)
        
        # å–å¾—æˆåŠŸãƒ‡ãƒ¼ã‚¿ã®è¦ç´„
        logger.info("ğŸ‰ å–å¾—æˆåŠŸãƒ‡ãƒ¼ã‚¿:")
        success_data = [
            ("ç±³å›½10å¹´å›½å‚µåˆ©å›ã‚Š", "^TNX", "4.2%ç¨‹åº¦"),
            ("S&P500æŒ‡æ•°", "^GSPC", "6,460ãƒã‚¤ãƒ³ãƒˆç¨‹åº¦"),
            ("USD/JPY", "JPY=X", "147å††ç¨‹åº¦"),
            ("æ—¥çµŒå¹³å‡", "^N225", "42,700å††ç¨‹åº¦"),
            ("VIXææ€–æŒ‡æ•°", "^VIX", "15ç¨‹åº¦")
        ]
        
        for name, ticker, latest in success_data:
            logger.info(f"  âœ… {name} ({ticker}): {latest}")
        
        logger.info(f"\nğŸ“ˆ è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡:")
        feature_types = ["value", "change", "volatility"]
        for feature_type in feature_types:
            type_cols = [col for col in external_cols if feature_type in col]
            logger.info(f"  {feature_type:12s}: {len(type_cols)}å€‹")
        
        logger.info(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ–°ç‰¹å¾´é‡ã§ã®ç²¾åº¦è©•ä¾¡")
        logger.info(f"æœŸå¾…ã•ã‚Œã‚‹ç²¾åº¦å‘ä¸Š: +1.0ï½2.5% (52.0ï½53.5%ç›®æ¨™)")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()