#!/usr/bin/env python3
"""
é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®å®Ÿè£…
60%è¶…ãˆã‚’ç›®æŒ‡ã™ç¬¬1æ®µéš: Stacking, Blending, å‹•çš„é‡ã¿èª¿æ•´
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class AdvancedEnsembleSystem:
    """é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
        # ç¾åœ¨ã®æœ€é©ç‰¹å¾´é‡ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆæ¸ˆã¿ï¼‰
        self.optimal_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20',
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
        
    def load_integrated_data(self):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        if not integrated_file.exists():
            logger.error("âŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(integrated_file)
        logger.info(f"âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df):,}ä»¶")
        
        return df
    
    def prepare_data_for_ensemble(self, df):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        logger.info("ğŸ”§ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèª
        missing_features = [f for f in self.optimal_features if f not in clean_df.columns]
        if missing_features:
            logger.error(f"âŒ ä¸è¶³ç‰¹å¾´é‡: {missing_features}")
            return None, None
            
        X = clean_df[self.optimal_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {len(self.optimal_features)}å€‹")
        
        return X, y
    
    def create_base_models(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤ã®ä½œæˆ"""
        logger.info("ğŸ§  ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤ä½œæˆ...")
        
        base_models = {
            'lr_l1': LogisticRegression(
                C=0.001, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'lr_l2': LogisticRegression(
                C=0.001, penalty='l2', solver='lbfgs',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'rf': RandomForestClassifier(
                n_estimators=100, max_depth=10,
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'rf_deep': RandomForestClassifier(
                n_estimators=200, max_depth=15,
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'xgb': xgb.XGBClassifier(
                n_estimators=100, max_depth=6,
                learning_rate=0.1, scale_pos_weight=1,
                random_state=42, n_jobs=-1, eval_metric='logloss'
            ),
            'svm': SVC(
                C=0.1, kernel='rbf', probability=True,
                class_weight='balanced', random_state=42
            )
        }
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ•°: {len(base_models)}å€‹")
        for name, model in base_models.items():
            logger.info(f"  {name}: {type(model).__name__}")
        
        return base_models
    
    def evaluate_base_models(self, X, y, base_models):
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å€‹åˆ¥è©•ä¾¡"""
        logger.info("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å€‹åˆ¥è©•ä¾¡...")
        
        X_scaled = self.scaler.fit_transform(X)
        tscv = TimeSeriesSplit(n_splits=5)
        base_results = {}
        
        for name, model in base_models.items():
            logger.info(f"  {name} è©•ä¾¡ä¸­...")
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            base_results[name] = {
                'avg': avg_score,
                'std': np.std(scores),
                'scores': scores
            }
            
            logger.info(f"    {name}: {avg_score:.3%} Â± {np.std(scores):.3%}")
        
        return base_results
    
    def implement_voting_classifier(self, X, y, base_models):
        """æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè£…"""
        logger.info("ğŸ—³ï¸ æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè£…...")
        
        X_scaled = self.scaler.fit_transform(X)
        
        # ãƒãƒ¼ãƒ‰ã¨ã‚½ãƒ•ãƒˆæŠ•ç¥¨ã®ä¸¡æ–¹ã‚’ãƒ†ã‚¹ãƒˆ
        voting_results = {}
        
        # ä¸Šä½3ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆäº‹å‰è©•ä¾¡çµæœã‹ã‚‰ï¼‰
        selected_models = [
            ('lr_l2', base_models['lr_l2']),
            ('rf', base_models['rf']),
            ('xgb', base_models['xgb'])
        ]
        
        for voting_type in ['hard', 'soft']:
            logger.info(f"  {voting_type.capitalize()}æŠ•ç¥¨ è©•ä¾¡ä¸­...")
            
            voting_clf = VotingClassifier(
                estimators=selected_models,
                voting=voting_type
            )
            
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                voting_clf.fit(X_train, y_train)
                pred = voting_clf.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            voting_results[f'voting_{voting_type}'] = {
                'avg': avg_score,
                'std': np.std(scores),
                'scores': scores
            }
            
            logger.info(f"    {voting_type.capitalize()}æŠ•ç¥¨: {avg_score:.3%} Â± {np.std(scores):.3%}")
        
        return voting_results
    
    def implement_stacking(self, X, y, base_models):
        """ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè£…"""
        logger.info("ğŸ—ï¸ ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè£…...")
        
        X_scaled = self.scaler.fit_transform(X)
        
        # ãƒ¬ãƒ™ãƒ«1ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰
        level_1_models = [
            ('lr_l2', base_models['lr_l2']),
            ('rf', base_models['rf']),
            ('xgb', base_models['xgb'])
        ]
        
        # ãƒ¬ãƒ™ãƒ«2ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¡ã‚¿å­¦ç¿’å™¨ï¼‰ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        meta_models = {
            'lr_meta': LogisticRegression(C=1.0, random_state=42, max_iter=1000),
            'rf_meta': RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)
        }
        
        stacking_results = {}
        
        for meta_name, meta_model in meta_models.items():
            logger.info(f"  {meta_name} ãƒ¡ã‚¿å­¦ç¿’å™¨ã§ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°...")
            
            stacking_clf = StackingClassifier(
                estimators=level_1_models,
                final_estimator=meta_model,
                cv=3,  # å†…éƒ¨ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                n_jobs=-1
            )
            
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                stacking_clf.fit(X_train, y_train)
                pred = stacking_clf.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            stacking_results[f'stacking_{meta_name}'] = {
                'avg': avg_score,
                'std': np.std(scores),
                'scores': scores
            }
            
            logger.info(f"    Stacking({meta_name}): {avg_score:.3%} Â± {np.std(scores):.3%}")
        
        return stacking_results
    
    def optimize_ensemble_weights(self, X, y, base_models):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–ï¼ˆOptunaä½¿ç”¨ï¼‰"""
        logger.info("âš–ï¸ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–...")
        
        X_scaled = self.scaler.fit_transform(X)
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        selected_models = ['lr_l2', 'rf', 'xgb']
        
        def objective(trial):
            # é‡ã¿ã‚’Optunaã§æœ€é©åŒ–
            weights = []
            for model_name in selected_models:
                weight = trial.suggest_float(f'weight_{model_name}', 0.1, 1.0)
                weights.append(weight)
            
            # é‡ã¿æ­£è¦åŒ–
            weights = np.array(weights)
            weights = weights / np.sum(weights)
            
            # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®è©•ä¾¡
            tscv = TimeSeriesSplit(n_splits=3)  # æœ€é©åŒ–ã§ã¯3åˆ†å‰²ã§é«˜é€ŸåŒ–
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å–å¾—
                predictions = []
                for i, model_name in enumerate(selected_models):
                    model = base_models[model_name]
                    model.fit(X_train, y_train)
                    pred_proba = model.predict_proba(X_test)[:, 1]
                    predictions.append(pred_proba)
                
                # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
                ensemble_pred_proba = np.average(predictions, axis=0, weights=weights)
                ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)
                
                scores.append(accuracy_score(y_test, ensemble_pred))
            
            return np.mean(scores)
        
        # Optunaæœ€é©åŒ–å®Ÿè¡Œ
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42)
        )
        study.optimize(objective, n_trials=50, show_progress_bar=False)
        
        # æœ€é©é‡ã¿ã§æœ€çµ‚è©•ä¾¡
        best_weights = []
        for model_name in selected_models:
            best_weights.append(study.best_params[f'weight_{model_name}'])
        
        best_weights = np.array(best_weights)
        best_weights = best_weights / np.sum(best_weights)
        
        logger.info(f"  æœ€é©é‡ã¿: {dict(zip(selected_models, best_weights))}")
        
        # æœ€é©é‡ã¿ã§ã®æœ€çµ‚è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=5)
        final_scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            predictions = []
            for i, model_name in enumerate(selected_models):
                model = base_models[model_name]
                model.fit(X_train, y_train)
                pred_proba = model.predict_proba(X_test)[:, 1]
                predictions.append(pred_proba)
            
            ensemble_pred_proba = np.average(predictions, axis=0, weights=best_weights)
            ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)
            final_scores.append(accuracy_score(y_test, ensemble_pred))
        
        avg_score = np.mean(final_scores)
        
        optimized_result = {
            'weighted_ensemble': {
                'avg': avg_score,
                'std': np.std(final_scores),
                'scores': final_scores,
                'weights': dict(zip(selected_models, best_weights))
            }
        }
        
        logger.info(f"    æœ€é©é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {avg_score:.3%} Â± {np.std(final_scores):.3%}")
        
        return optimized_result
    
    def compare_all_ensemble_methods(self, base_results, voting_results, stacking_results, optimized_result):
        """å…¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•æ¯”è¼ƒ"""
        logger.info("ğŸ“ˆ å…¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•æ¯”è¼ƒ...")
        
        all_results = {}
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆæœ€é«˜ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰
        best_base = max(base_results.keys(), key=lambda k: base_results[k]['avg'])
        all_results['best_base'] = base_results[best_base]
        all_results['best_base']['method'] = f"ãƒ™ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ({best_base})"
        
        # æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        for name, result in voting_results.items():
            all_results[name] = result
            all_results[name]['method'] = f"æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ({name})"
        
        # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
        for name, result in stacking_results.items():
            all_results[name] = result
            all_results[name]['method'] = f"ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚° ({name})"
        
        # æœ€é©åŒ–é‡ã¿ä»˜ã
        for name, result in optimized_result.items():
            all_results[name] = result
            all_results[name]['method'] = "æœ€é©é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"
        
        return all_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…ã‚·ã‚¹ãƒ†ãƒ ")
    logger.info("ğŸ¯ ç›®æ¨™: 59.4%ã‹ã‚‰62%è¶…ãˆã‚’ç›®æŒ‡ã™")
    
    ensemble_system = AdvancedEnsembleSystem()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = ensemble_system.load_integrated_data()
        if df is None:
            return
        
        # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X, y = ensemble_system.prepare_data_for_ensemble(df)
        if X is None:
            return
        
        # 3. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        base_models = ensemble_system.create_base_models()
        
        # 4. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        base_results = ensemble_system.evaluate_base_models(X, y, base_models)
        
        # 5. æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        voting_results = ensemble_system.implement_voting_classifier(X, y, base_models)
        
        # 6. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
        stacking_results = ensemble_system.implement_stacking(X, y, base_models)
        
        # 7. é‡ã¿æœ€é©åŒ–
        optimized_result = ensemble_system.optimize_ensemble_weights(X, y, base_models)
        
        # 8. å…¨æ‰‹æ³•æ¯”è¼ƒ
        all_results = ensemble_system.compare_all_ensemble_methods(
            base_results, voting_results, stacking_results, optimized_result
        )
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*100)
        logger.info("ğŸ† é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…çµæœ")
        logger.info("="*100)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¡¨ç¤º
        baseline_score = 59.4  # å‰å›ã®æœ€é«˜ã‚¹ã‚³ã‚¢
        logger.info(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰: {baseline_score:.1%}")
        
        # å…¨çµæœã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(all_results.items(), key=lambda x: x[1]['avg'], reverse=True)
        
        logger.info(f"\nğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•æ¯”è¼ƒçµæœ:")
        for i, (name, result) in enumerate(sorted_results, 1):
            improvement = (result['avg'] - baseline_score/100) * 100
            status = "ğŸš€" if improvement > 2.0 else "ğŸ“ˆ" if improvement > 0.5 else "ğŸ“Š"
            
            logger.info(f"  {i:2d}. {result['method']:30s}: {result['avg']:.3%} ({improvement:+.2f}%) {status}")
        
        # æœ€é«˜çµæœ
        best_method, best_result = sorted_results[0]
        final_improvement = (best_result['avg'] - baseline_score/100) * 100
        
        logger.info(f"\nğŸ† æœ€é«˜æ€§èƒ½:")
        logger.info(f"  æ‰‹æ³•: {best_result['method']}")
        logger.info(f"  ç²¾åº¦: {best_result['avg']:.3%} Â± {best_result['std']:.3%}")
        logger.info(f"  å‘ä¸Š: {final_improvement:+.2f}% (59.4% â†’ {best_result['avg']:.1%})")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_60 = 0.60
        target_62 = 0.62
        
        if best_result['avg'] >= target_62:
            logger.info(f"ğŸ‰ ç›®æ¨™å¤§å¹…é”æˆï¼ 62%è¶…ãˆ ({best_result['avg']:.1%} >= 62.0%)")
        elif best_result['avg'] >= target_60:
            logger.info(f"âœ… ç›®æ¨™é”æˆï¼ 60%è¶…ãˆ ({best_result['avg']:.1%} >= 60.0%)")
        else:
            logger.info(f"ğŸ“ˆ æ”¹å–„åŠ¹æœã‚ã‚Š ({best_result['avg']:.1%})")
        
        # è©³ç´°æƒ…å ±ï¼ˆé‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®å ´åˆï¼‰
        if 'weights' in best_result:
            logger.info(f"\nâš–ï¸ æœ€é©é‡ã¿:")
            for model, weight in best_result['weights'].items():
                logger.info(f"  {model}: {weight:.3f}")
        
        logger.info(f"\nâš–ï¸ ã“ã®çµæœã¯å…¨ãƒ‡ãƒ¼ã‚¿{len(df):,}ä»¶ã§ã®å³å¯†ãªæ™‚ç³»åˆ—æ¤œè¨¼ã§ã™")
        logger.info(f"âœ… ç¬¬1æ®µéšå®Œäº†: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Šé”æˆ")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()