#!/usr/bin/env python3
"""
J-Quantsæœ€å¤§æ´»ç”¨ã‚·ã‚¹ãƒ†ãƒ  - ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ãƒ—ãƒ©ãƒ³ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class JQuantsMaximumUtilizer:
    """J-Quantsæœ€å¤§æ´»ç”¨"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.jquants_dir = self.data_dir / "raw" / "jquants_enhanced"
        
        # J-Quantsèªè¨¼æƒ…å ±ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—å¯èƒ½ã ãŒã€ä»Šå›ã¯ãƒ¢ãƒƒã‚¯ã§ä»£ç”¨ï¼‰
        self.use_mock_data = True
    
    def create_comprehensive_mock_data(self, df_base):
        """åŒ…æ‹¬çš„ãªJ-Quantsãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        logger.info("ğŸ”§ åŒ…æ‹¬çš„J-Quantsãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­...")
        
        # æ—¥ä»˜ã¨ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã®çµ„ã¿åˆã‚ã›ã‚’å–å¾—
        df_base['Date'] = pd.to_datetime(df_base['Date'])
        dates = df_base['Date'].drop_duplicates().sort_values()
        codes = df_base['Code'].unique()
        
        logger.info(f"å¯¾è±¡æœŸé–“: {dates.min()} ï½ {dates.max()}")
        logger.info(f"å¯¾è±¡éŠ˜æŸ„: {len(codes)}éŠ˜æŸ„")
        
        # 1. ä¿¡ç”¨å–å¼•é€±æ¬¡æ®‹é«˜ï¼ˆå®Ÿéš›ã®J-Quantså½¢å¼ï¼‰
        self._create_margin_interest_data(dates)
        
        # 2. ç©ºå£²ã‚Šæ¯”ç‡ãƒ»æ®‹é«˜ï¼ˆå®Ÿéš›ã®J-Quantså½¢å¼ï¼‰
        self._create_short_selling_data(dates, codes)
        
        # 3. è²¡å‹™ãƒ»æ±ºç®—ç™ºè¡¨æƒ…å ±
        self._create_financial_data(dates, codes)
        
        # 4. æ—¥çµŒ225ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
        self._create_options_data(dates)
        
        # 5. æŠ•è³‡éƒ¨é–€åˆ¥å£²è²·å‹•å‘
        self._create_investor_type_data(dates)
        
        logger.info("âœ… åŒ…æ‹¬çš„ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†")
    
    def _create_margin_interest_data(self, dates):
        """ä¿¡ç”¨å–å¼•é€±æ¬¡æ®‹é«˜ãƒ‡ãƒ¼ã‚¿"""
        logger.info("ğŸ’³ ä¿¡ç”¨å–å¼•é€±æ¬¡æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ä½œæˆ...")
        
        # é‡‘æ›œæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆé€±æ¬¡ï¼‰
        weekly_dates = [d for d in dates if d.dayofweek == 4][::7]  # æ¯é€±é‡‘æ›œ
        
        np.random.seed(42)
        margin_data = []
        
        base_margin_buy = 2000000000000  # 2å…†å††è¦æ¨¡
        base_margin_sell = 500000000000   # 5000å„„å††è¦æ¨¡
        
        for i, date in enumerate(weekly_dates):
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã¨å­£ç¯€æ€§ã‚’åŠ å‘³
            trend = 1 + 0.001 * i  # é•·æœŸçš„ãªå¢—åŠ ãƒˆãƒ¬ãƒ³ãƒ‰
            seasonal = 1 + 0.1 * np.sin(2 * np.pi * i / 52)  # å¹´æ¬¡å­£ç¯€æ€§
            noise = np.random.normal(1, 0.05)
            
            margin_buy = int(base_margin_buy * trend * seasonal * noise)
            margin_sell = int(base_margin_sell * trend * seasonal * noise * 0.8)
            
            margin_data.append({
                'Date': date,
                'MarginBuyBalance': margin_buy,
                'MarginSellBalance': margin_sell,
                'MarginBuyTradingValue': margin_buy * np.random.uniform(0.1, 0.3),
                'MarginSellTradingValue': margin_sell * np.random.uniform(0.1, 0.3),
                'MarginNetBuy': margin_buy - margin_sell
            })
        
        df_margin = pd.DataFrame(margin_data)
        output_file = self.jquants_dir / "margin_interest_weekly.parquet"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        df_margin.to_parquet(output_file)
        logger.info(f"  âœ… é€±æ¬¡ä¿¡ç”¨æ®‹é«˜: {len(df_margin)}ä»¶")
    
    def _create_short_selling_data(self, dates, codes):
        """ç©ºå£²ã‚Šãƒ‡ãƒ¼ã‚¿"""
        logger.info("ğŸ“‰ ç©ºå£²ã‚Šãƒ‡ãƒ¼ã‚¿ä½œæˆ...")
        
        np.random.seed(43)
        
        # æ¥­ç¨®åˆ¥ç©ºå£²ã‚Šæ¯”ç‡
        sector_short_data = []
        sectors = ['éŠ€è¡Œæ¥­', 'è¨¼åˆ¸æ¥­', 'é›»æ°—æ©Ÿå™¨', 'æƒ…å ±é€šä¿¡', 'å°å£²æ¥­', 'å»ºè¨­æ¥­', 
                  'åŒ–å­¦', 'åŒ»è–¬å“', 'è‡ªå‹•è»Š', 'ãã®ä»–']
        
        daily_dates = dates[::5]  # 5æ—¥ã”ã¨ï¼ˆè¨ˆç®—è² è·è»½æ¸›ï¼‰
        
        for date in daily_dates:
            for sector in sectors:
                short_ratio = np.random.beta(2, 8) * 0.4  # 0-40%ã®ç¯„å›²ã§ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ
                sector_short_data.append({
                    'Date': date,
                    'SectorName': sector,
                    'ShortSellingRatio': short_ratio,
                    'ShortSellingVolume': np.random.randint(1000000, 10000000)
                })
        
        df_sector_short = pd.DataFrame(sector_short_data)
        output_file = self.jquants_dir / "short_selling_by_sector.parquet"
        df_sector_short.to_parquet(output_file)
        logger.info(f"  âœ… æ¥­ç¨®åˆ¥ç©ºå£²ã‚Š: {len(df_sector_short)}ä»¶")
        
        # éŠ˜æŸ„åˆ¥ç©ºå£²ã‚Šæ®‹é«˜ï¼ˆä¸»è¦éŠ˜æŸ„ã®ã¿ï¼‰
        major_codes = np.random.choice(codes, size=min(100, len(codes)), replace=False)
        position_data = []
        
        recent_dates = dates[-30:]  # æœ€è¿‘30æ—¥
        
        for date in recent_dates:
            for code in major_codes:
                if np.random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§ç©ºå£²ã‚Šæ®‹é«˜ã‚ã‚Š
                    short_balance = np.random.randint(100000, 5000000)
                    position_data.append({
                        'Date': date,
                        'Code': code,
                        'ShortPosition': short_balance,
                        'ShortRatio': np.random.uniform(0.01, 0.15)
                    })
        
        df_positions = pd.DataFrame(position_data)
        output_file = self.jquants_dir / "short_selling_positions.parquet"
        df_positions.to_parquet(output_file)
        logger.info(f"  âœ… éŠ˜æŸ„åˆ¥ç©ºå£²ã‚Š: {len(df_positions)}ä»¶")
    
    def _create_financial_data(self, dates, codes):
        """è²¡å‹™ãƒ»æ±ºç®—ãƒ‡ãƒ¼ã‚¿"""
        logger.info("ğŸ’¼ è²¡å‹™ãƒ»æ±ºç®—ãƒ‡ãƒ¼ã‚¿ä½œæˆ...")
        
        np.random.seed(44)
        
        # æ±ºç®—ç™ºè¡¨äºˆå®š
        announcement_data = []
        sample_codes = np.random.choice(codes, size=min(200, len(codes)), replace=False)
        
        # å››åŠæœŸã”ã¨ã®ç™ºè¡¨
        quarters = pd.date_range(start=dates.min(), end=dates.max(), freq='Q')
        
        for code in sample_codes:
            for quarter in quarters:
                # ç™ºè¡¨æ—¥ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«è¨­å®šï¼ˆæœˆæœ«ã‹ã‚‰45æ—¥ä»¥å†…ï¼‰
                announce_date = quarter + timedelta(days=np.random.randint(1, 45))
                if announce_date <= dates.max():
                    announcement_data.append({
                        'Code': code,
                        'AnnouncementDate': announce_date,
                        'FiscalQuarter': quarter,
                        'AnnouncementType': np.random.choice(['æ±ºç®—çŸ­ä¿¡', 'å››åŠæœŸå ±å‘Šæ›¸', 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸'])
                    })
        
        df_announcements = pd.DataFrame(announcement_data)
        output_file = self.jquants_dir / "financial_announcements.parquet"
        df_announcements.to_parquet(output_file)
        logger.info(f"  âœ… æ±ºç®—ç™ºè¡¨äºˆå®š: {len(df_announcements)}ä»¶")
        
        # ç°¡æ˜“è²¡å‹™æŒ‡æ¨™
        financial_metrics = []
        
        for code in sample_codes:
            # å¹´æ¬¡ãƒ‡ãƒ¼ã‚¿
            years = pd.date_range(start=dates.min(), end=dates.max(), freq='Y')
            base_sales = np.random.uniform(1000, 50000)  # ç™¾ä¸‡å††
            base_profit = base_sales * np.random.uniform(0.05, 0.15)
            
            for i, year in enumerate(years):
                # æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰
                growth = (1 + np.random.normal(0.05, 0.1)) ** i
                
                financial_metrics.append({
                    'Code': code,
                    'FiscalYear': year,
                    'Sales': base_sales * growth,
                    'OperatingProfit': base_profit * growth * np.random.uniform(0.8, 1.2),
                    'NetProfit': base_profit * growth * np.random.uniform(0.6, 1.1),
                    'PER': np.random.uniform(8, 25),
                    'PBR': np.random.uniform(0.8, 3.0),
                    'ROE': np.random.uniform(0.02, 0.20)
                })
        
        df_financials = pd.DataFrame(financial_metrics)
        output_file = self.jquants_dir / "financial_statements.parquet"
        df_financials.to_parquet(output_file)
        logger.info(f"  âœ… è²¡å‹™æŒ‡æ¨™: {len(df_financials)}ä»¶")
    
    def _create_options_data(self, dates):
        """æ—¥çµŒ225ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿"""
        logger.info("ğŸ“Š æ—¥çµŒ225ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä½œæˆ...")
        
        np.random.seed(45)
        options_data = []
        
        # é€±æ¬¡ï¼ˆé‡‘æ›œæ—¥ï¼‰ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã‚’ä½œæˆ
        weekly_dates = [d for d in dates if d.dayofweek == 4][::2]  # éš”é€±
        base_nikkei = 28000
        
        for i, date in enumerate(weekly_dates):
            # æ—¥çµŒå¹³å‡ã®æ¨¡æ“¬ä¾¡æ ¼
            nikkei_price = base_nikkei * (1 + np.random.normal(0, 0.02)) ** i
            
            # ATMã‚³ãƒ¼ãƒ«ãƒ»ãƒ—ãƒƒãƒˆã‚’ä¸­å¿ƒã«è¤‡æ•°è¡Œä½¿ä¾¡æ ¼
            for strike_offset in [-2000, -1000, 0, 1000, 2000]:
                strike = int((nikkei_price + strike_offset) / 1000) * 1000  # 1000å††åˆ»ã¿
                
                # ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                call_iv = np.random.uniform(0.15, 0.35)
                call_price = max(nikkei_price - strike, 0) + np.random.uniform(10, 200)
                
                options_data.append({
                    'Date': date,
                    'UnderlyingPrice': nikkei_price,
                    'StrikePrice': strike,
                    'OptionType': 'Call',
                    'Price': call_price,
                    'ImpliedVolatility': call_iv,
                    'Volume': np.random.randint(100, 10000)
                })
                
                # ãƒ—ãƒƒãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³
                put_iv = call_iv + np.random.uniform(-0.02, 0.02)
                put_price = max(strike - nikkei_price, 0) + np.random.uniform(10, 200)
                
                options_data.append({
                    'Date': date,
                    'UnderlyingPrice': nikkei_price,
                    'StrikePrice': strike,
                    'OptionType': 'Put',
                    'Price': put_price,
                    'ImpliedVolatility': put_iv,
                    'Volume': np.random.randint(100, 8000)
                })
        
        df_options = pd.DataFrame(options_data)
        output_file = self.jquants_dir / "nikkei225_options.parquet"
        df_options.to_parquet(output_file)
        logger.info(f"  âœ… ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿: {len(df_options)}ä»¶")
    
    def _create_investor_type_data(self, dates):
        """æŠ•è³‡éƒ¨é–€åˆ¥å£²è²·å‹•å‘"""
        logger.info("ğŸ¢ æŠ•è³‡éƒ¨é–€åˆ¥å£²è²·å‹•å‘ä½œæˆ...")
        
        np.random.seed(46)
        investor_data = []
        
        investor_types = ['å¤–å›½äºº', 'å€‹äºº', 'é‡‘èæ©Ÿé–¢', 'è¨¼åˆ¸ä¼šç¤¾', 'æŠ•è³‡ä¿¡è¨—', 'å¹´é‡‘åŸºé‡‘', 'ãã®ä»–']
        weekly_dates = [d for d in dates if d.dayofweek == 4][::2]  # éš”é€±
        
        for date in weekly_dates:
            total_volume = np.random.uniform(2e12, 5e12)  # 2-5å…†å††ã®é€±é–“å£²è²·ä»£é‡‘
            
            # å„æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—ã®å£²è²·æ¯”ç‡ï¼ˆç¾å®Ÿçš„ãªé…åˆ†ï¼‰
            ratios = {
                'å¤–å›½äºº': np.random.uniform(0.25, 0.35),
                'å€‹äºº': np.random.uniform(0.15, 0.25), 
                'é‡‘èæ©Ÿé–¢': np.random.uniform(0.08, 0.15),
                'è¨¼åˆ¸ä¼šç¤¾': np.random.uniform(0.05, 0.12),
                'æŠ•è³‡ä¿¡è¨—': np.random.uniform(0.08, 0.15),
                'å¹´é‡‘åŸºé‡‘': np.random.uniform(0.05, 0.10),
                'ãã®ä»–': 0.05
            }
            
            # æ¯”ç‡ã‚’æ­£è¦åŒ–
            total_ratio = sum(ratios.values())
            for investor_type in investor_types:
                if investor_type in ratios:
                    volume = total_volume * ratios[investor_type] / total_ratio
                    net_buy = volume * np.random.uniform(-0.3, 0.3)  # Â±30%ã®ç¯„å›²ã§ãƒãƒƒãƒˆå£²è²·
                    
                    investor_data.append({
                        'Date': date,
                        'InvestorType': investor_type,
                        'BuyValue': volume + net_buy/2,
                        'SellValue': volume - net_buy/2,
                        'NetBuyValue': net_buy,
                        'BuyVolume': (volume + net_buy/2) / np.random.uniform(2000, 3000)  # å¹³å‡å˜ä¾¡ã§é™¤ç®—
                    })
        
        df_investors = pd.DataFrame(investor_data)
        output_file = self.jquants_dir / "investor_type_trading.parquet"
        df_investors.to_parquet(output_file)
        logger.info(f"  âœ… æŠ•è³‡éƒ¨é–€åˆ¥: {len(df_investors)}ä»¶")
    
    def load_all_jquants_data(self):
        """å…¨J-Quantsãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š å…¨J-Quantsãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
        base_files = list(self.processed_dir.glob("*.parquet"))
        if not base_files:
            logger.error("âŒ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        df_base = pd.read_parquet(base_files[0])
        logger.info(f"åŸºæœ¬ãƒ‡ãƒ¼ã‚¿: {len(df_base)}ä»¶")
        
        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        self.create_comprehensive_mock_data(df_base)
        
        # å„ç¨®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        jquants_data = {
            'base': df_base,
            'margin': self._load_if_exists("margin_interest_weekly.parquet"),
            'sector_short': self._load_if_exists("short_selling_by_sector.parquet"),
            'position_short': self._load_if_exists("short_selling_positions.parquet"),
            'announcements': self._load_if_exists("financial_announcements.parquet"),
            'financials': self._load_if_exists("financial_statements.parquet"),
            'options': self._load_if_exists("nikkei225_options.parquet"),
            'investors': self._load_if_exists("investor_type_trading.parquet")
        }
        
        return jquants_data
    
    def _load_if_exists(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿èª­ã¿è¾¼ã¿"""
        file_path = self.jquants_dir / filename
        if file_path.exists():
            return pd.read_parquet(file_path)
        return None
    
    def create_maximum_features(self, jquants_data):
        """æœ€å¤§é™ã®ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ”§ æœ€å¤§é™ç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        df = jquants_data['base'].copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # 1. ä¿¡ç”¨å–å¼•ç‰¹å¾´é‡
        if jquants_data['margin'] is not None:
            df_margin = jquants_data['margin'].copy()
            df_margin['Date'] = pd.to_datetime(df_margin['Date'])
            
            # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥æ¬¡ã«å±•é–‹
            df_margin_daily = df_margin.set_index('Date').resample('D').ffill().reset_index()
            
            # ä¿¡ç”¨å–å¼•æ¯”ç‡ãƒ»å¤‰åŒ–ç‡
            df_margin_daily['MarginRatio'] = df_margin_daily['MarginBuyBalance'] / (df_margin_daily['MarginBuyBalance'] + df_margin_daily['MarginSellBalance'])
            df_margin_daily['MarginChange'] = df_margin_daily['MarginNetBuy'].pct_change()
            df_margin_daily['MarginTrend'] = df_margin_daily['MarginNetBuy'].rolling(4).mean()
            
            df = df.merge(
                df_margin_daily[['Date', 'MarginRatio', 'MarginChange', 'MarginTrend']], 
                on='Date', how='left'
            )
            logger.info("  âœ… ä¿¡ç”¨å–å¼•ç‰¹å¾´é‡è¿½åŠ ")
        
        # 2. ç©ºå£²ã‚Šç‰¹å¾´é‡
        if jquants_data['sector_short'] is not None:
            df_sector_short = jquants_data['sector_short'].copy()
            df_sector_short['Date'] = pd.to_datetime(df_sector_short['Date'])
            
            # ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ç©ºå£²ã‚Šæ¯”ç‡ã®å¹³å‡
            daily_short = df_sector_short.groupby('Date')['ShortSellingRatio'].agg(['mean', 'std']).reset_index()
            daily_short.columns = ['Date', 'AvgShortRatio', 'ShortRatioVolatility']
            
            df = df.merge(daily_short, on='Date', how='left')
            logger.info("  âœ… ç©ºå£²ã‚Šç‰¹å¾´é‡è¿½åŠ ")
        
        # 3. éŠ˜æŸ„åˆ¥ç©ºå£²ã‚Šæ®‹é«˜
        if jquants_data['position_short'] is not None:
            df_positions = jquants_data['position_short'].copy()
            df_positions['Date'] = pd.to_datetime(df_positions['Date'])
            
            df = df.merge(
                df_positions[['Date', 'Code', 'ShortPosition', 'ShortRatio']], 
                on=['Date', 'Code'], how='left'
            )
            df['HasShortPosition'] = (df['ShortPosition'].notna()).astype(int)
            logger.info("  âœ… éŠ˜æŸ„åˆ¥ç©ºå£²ã‚Šç‰¹å¾´é‡è¿½åŠ ")
        
        # 4. æ±ºç®—ç™ºè¡¨åŠ¹æœ
        if jquants_data['announcements'] is not None:
            df_announce = jquants_data['announcements'].copy()
            df_announce['AnnouncementDate'] = pd.to_datetime(df_announce['AnnouncementDate'])
            
            # æ±ºç®—ç™ºè¡¨å‰å¾Œã®ãƒ•ãƒ©ã‚°ï¼ˆç°¡ç•¥åŒ–ï¼‰
            df_announce['Announce_Flag'] = 1
            
            # ç™ºè¡¨æ—¥å½“æ—¥ã®ãƒ•ãƒ©ã‚°
            df = df.merge(
                df_announce[['Code', 'AnnouncementDate', 'Announce_Flag']].rename(columns={'AnnouncementDate': 'Date'}), 
                on=['Date', 'Code'], how='left'
            )
            df['Announce_Flag'] = df['Announce_Flag'].fillna(0)
            
            # ç™ºè¡¨å‰3æ—¥ã®ãƒ•ãƒ©ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            df['Announce_Soon'] = df.groupby('Code')['Announce_Flag'].shift(-3).fillna(0)
            
            logger.info("  âœ… æ±ºç®—ç™ºè¡¨åŠ¹æœç‰¹å¾´é‡è¿½åŠ ")
        
        # 5. è²¡å‹™æŒ‡æ¨™
        if jquants_data['financials'] is not None:
            df_fin = jquants_data['financials'].copy()
            df_fin['FiscalYear'] = pd.to_datetime(df_fin['FiscalYear'])
            
            # æœ€æ–°ã®è²¡å‹™æŒ‡æ¨™ã‚’å„æ—¥ã«é©ç”¨
            df_fin_latest = df_fin.sort_values(['Code', 'FiscalYear']).groupby('Code').tail(1)
            
            df = df.merge(
                df_fin_latest[['Code', 'PER', 'PBR', 'ROE']], 
                on='Code', how='left'
            )
            
            # PERãƒãƒ³ãƒ‰
            df['PER_Quartile'] = pd.qcut(df['PER'], q=4, labels=False, duplicates='drop')
            df['Low_PER_Flag'] = (df['PER'] < 15).astype(int)
            
            logger.info("  âœ… è²¡å‹™æŒ‡æ¨™ç‰¹å¾´é‡è¿½åŠ ")
        
        # 6. ã‚ªãƒ—ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆVIXä»£æ›¿ï¼‰
        if jquants_data['options'] is not None:
            df_options = jquants_data['options'].copy()
            df_options['Date'] = pd.to_datetime(df_options['Date'])
            
            # æ—¥æ¬¡ã®ATMã‚¤ãƒ³ãƒ—ãƒ©ã‚¤ãƒ‰ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å¹³å‡
            atm_iv = df_options.groupby('Date')['ImpliedVolatility'].mean().reset_index()
            atm_iv.columns = ['Date', 'ATM_IV']
            atm_iv['IV_Trend'] = atm_iv['ATM_IV'].rolling(5).mean()
            atm_iv['IV_Spike'] = (atm_iv['ATM_IV'] > atm_iv['ATM_IV'].rolling(20).mean() * 1.2).astype(int)
            
            df = df.merge(atm_iv, on='Date', how='left')
            logger.info("  âœ… ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆVIXä»£æ›¿ï¼‰ç‰¹å¾´é‡è¿½åŠ ")
        
        # 7. æŠ•è³‡éƒ¨é–€åˆ¥å£²è²·å‹•å‘
        if jquants_data['investors'] is not None:
            df_investors = jquants_data['investors'].copy()
            df_investors['Date'] = pd.to_datetime(df_investors['Date'])
            
            # å¤–å›½äººå£²è²·å‹•å‘
            foreign_data = df_investors[df_investors['InvestorType'] == 'å¤–å›½äºº'][['Date', 'NetBuyValue']]
            foreign_data.columns = ['Date', 'ForeignNetBuy']
            foreign_data['ForeignTrend'] = foreign_data['ForeignNetBuy'].rolling(4).mean()
            foreign_data['ForeignBuying'] = (foreign_data['ForeignNetBuy'] > 0).astype(int)
            
            # å€‹äººæŠ•è³‡å®¶å‹•å‘
            individual_data = df_investors[df_investors['InvestorType'] == 'å€‹äºº'][['Date', 'NetBuyValue']]
            individual_data.columns = ['Date', 'IndividualNetBuy']
            individual_data['IndividualTrend'] = individual_data['IndividualNetBuy'].rolling(4).mean()
            
            # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥æ¬¡ã«å±•é–‹
            foreign_daily = foreign_data.set_index('Date').resample('D').ffill().reset_index()
            individual_daily = individual_data.set_index('Date').resample('D').ffill().reset_index()
            
            df = df.merge(foreign_daily, on='Date', how='left')
            df = df.merge(individual_daily, on='Date', how='left')
            
            logger.info("  âœ… æŠ•è³‡éƒ¨é–€åˆ¥ç‰¹å¾´é‡è¿½åŠ ")
        
        # 8. å¸‚å ´å…¨ä½“ç‰¹å¾´é‡ï¼ˆæ—¢å­˜ã‚’æ‹¡å¼µï¼‰
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std', 'skew'],
            'Volume': ['mean', 'std'],
            'Returns': ['mean', 'std', 'skew']
        }).round(6)
        
        daily_market.columns = [
            'Market_Price_Mean', 'Market_Price_Std', 'Market_Price_Skew',
            'Market_Volume_Mean', 'Market_Volume_Std', 
            'Market_Return_Mean', 'Market_Return_Std', 'Market_Return_Skew'
        ]
        daily_market = daily_market.reset_index()
        
        # å¸‚å ´ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å‹¢ã„
        daily_market['Market_Trend_5d'] = daily_market['Market_Return_Mean'].rolling(5).mean()
        daily_market['Market_Momentum'] = (daily_market['Market_Return_Mean'] > daily_market['Market_Trend_5d']).astype(int)
        daily_market['Market_Stress'] = (daily_market['Market_Return_Std'] > daily_market['Market_Return_Std'].rolling(20).mean() * 1.5).astype(int)
        
        df = df.merge(daily_market, on='Date', how='left')
        
        # 9. ã‚»ã‚¯ã‚¿ãƒ¼å¼·åº¦ï¼ˆã‚³ãƒ¼ãƒ‰å‰2æ¡ãƒ™ãƒ¼ã‚¹ï¼‰
        df['Sector_Code'] = df['Code'].astype(str).str[:2]
        
        sector_performance = df.groupby(['Date', 'Sector_Code'])['Returns'].agg(['mean', 'std', 'count']).reset_index()
        sector_performance.columns = ['Date', 'Sector_Code', 'Sector_Return', 'Sector_Vol', 'Sector_Count']
        
        # ã‚»ã‚¯ã‚¿ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        sector_performance['Sector_Rank'] = sector_performance.groupby('Date')['Sector_Return'].rank(pct=True)
        sector_performance['Top_Sector'] = (sector_performance['Sector_Rank'] > 0.8).astype(int)
        
        df = df.merge(sector_performance, on=['Date', 'Sector_Code'], how='left')
        
        # å€‹åˆ¥éŠ˜æŸ„ã¨ã‚»ã‚¯ã‚¿ãƒ¼ã®ç›¸å¯¾é–¢ä¿‚
        df['Sector_Alpha'] = df['Returns'] - df['Sector_Return']
        df['Sector_Beta'] = df.groupby(['Sector_Code'])['Returns'].transform(
            lambda x: x.rolling(60).corr(df.loc[x.index, 'Market_Return_Mean'])
        )
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        logger.info(f"âœ… æœ€å¤§é™ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        logger.info(f"è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len(df.columns) - len(jquants_data['base'].columns)}")
        
        return df
    
    def ultimate_evaluation(self, df_enhanced, sample_size=75000):
        """ç©¶æ¥µè©•ä¾¡"""
        logger.info(f"ğŸš€ ç©¶æ¥µè©•ä¾¡é–‹å§‹ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size:,}ï¼‰")
        
        if 'Binary_Direction' not in df_enhanced.columns:
            logger.error("âŒ Binary_DirectionãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å„ªå…ˆã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        df_enhanced = df_enhanced.sort_values('Date')
        if len(df_enhanced) > sample_size:
            df_enhanced = df_enhanced.tail(sample_size)
            logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(df_enhanced):,}ä»¶")
        
        # ç‰¹å¾´é‡åˆ†é¡
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector_Code', 'date', 'code'
        }
        
        all_features = [col for col in df_enhanced.columns 
                       if col not in exclude_cols and df_enhanced[col].dtype in ['int64', 'float64', 'int32', 'float32']]
        
        # åŸºæœ¬ç‰¹å¾´é‡
        basic_features = [col for col in all_features if not any(
            keyword in col for keyword in [
                'Margin', 'Short', 'Announce', 'PER', 'PBR', 'ROE', 'ATM_IV', 'IV_', 
                'Foreign', 'Individual', 'Market_', 'Sector_'
            ]
        )]
        
        # J-Quantsæ‹¡å¼µç‰¹å¾´é‡
        jquants_features = [col for col in all_features if any(
            keyword in col for keyword in [
                'Margin', 'Short', 'Announce', 'PER', 'PBR', 'ROE', 'ATM_IV', 'IV_', 
                'Foreign', 'Individual'
            ]
        )]
        
        # å¸‚å ´ãƒ»ã‚»ã‚¯ã‚¿ãƒ¼ç‰¹å¾´é‡
        market_features = [col for col in all_features if any(
            keyword in col for keyword in ['Market_', 'Sector_']
        )]
        
        logger.info(f"åŸºæœ¬ç‰¹å¾´é‡: {len(basic_features)}å€‹")
        logger.info(f"J-Quantsæ‹¡å¼µ: {len(jquants_features)}å€‹") 
        logger.info(f"å¸‚å ´ãƒ»ã‚»ã‚¯ã‚¿ãƒ¼: {len(market_features)}å€‹")
        logger.info(f"å…¨ç‰¹å¾´é‡: {len(all_features)}å€‹")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
        clean_df = df_enhanced[df_enhanced['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        logger.info(f"è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è©•ä¾¡
        results = {}
        
        # 1. åŸºæœ¬ç‰¹å¾´é‡ã®ã¿
        if basic_features:
            X_basic = clean_df[basic_features]
            y = clean_df['Binary_Direction']
            results['basic'] = self._ultimate_model_test(X_basic, y, "åŸºæœ¬ç‰¹å¾´é‡")
        
        # 2. J-Quantsæ‹¡å¼µã®ã¿  
        if jquants_features:
            X_jquants = clean_df[jquants_features]
            y = clean_df['Binary_Direction']
            results['jquants_enhanced'] = self._ultimate_model_test(X_jquants, y, "J-Quantsæ‹¡å¼µ")
        
        # 3. å¸‚å ´ãƒ»ã‚»ã‚¯ã‚¿ãƒ¼ã®ã¿
        if market_features:
            X_market = clean_df[market_features]
            y = clean_df['Binary_Direction']
            results['market_sector'] = self._ultimate_model_test(X_market, y, "å¸‚å ´ãƒ»ã‚»ã‚¯ã‚¿ãƒ¼")
        
        # 4. J-Quants + å¸‚å ´
        jq_market_features = jquants_features + market_features
        if jq_market_features:
            X_jq_market = clean_df[jq_market_features]
            y = clean_df['Binary_Direction']
            results['jquants_market'] = self._ultimate_model_test(X_jq_market, y, "J-Quants+å¸‚å ´")
        
        # 5. å…¨ç‰¹å¾´é‡
        X_all = clean_df[all_features]
        y = clean_df['Binary_Direction']
        results['all_features'] = self._ultimate_model_test(X_all, y, "å…¨ç‰¹å¾´é‡")
        
        return results
    
    def _ultimate_model_test(self, X, y, name):
        """ç©¶æ¥µãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ"""
        logger.info(f"âš¡ {name}è©•ä¾¡ä¸­...")
        
        tscv = TimeSeriesSplit(n_splits=3)
        scaler = StandardScaler()
        
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=150, max_depth=12, min_samples_split=8,
                min_samples_leaf=4, max_features='sqrt',
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'LogisticRegression': LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            )
        }
        
        model_results = {}
        
        for model_name, model in models.items():
            fold_scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                # å‰å‡¦ç†
                if 'Logistic' in model_name:
                    X_train_proc = scaler.fit_transform(X_train)
                    X_test_proc = scaler.transform(X_test)
                else:
                    X_train_proc = X_train
                    X_test_proc = X_test
                
                # å­¦ç¿’ãƒ»äºˆæ¸¬
                model.fit(X_train_proc, y_train)
                y_pred = model.predict(X_test_proc)
                accuracy = accuracy_score(y_test, y_pred)
                fold_scores.append(accuracy)
            
            avg_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)
            
            model_results[model_name] = {
                'score': avg_score,
                'std': std_score,
                'fold_scores': fold_scores
            }
            
            logger.info(f"  {model_name}: {avg_score:.3f} Â± {std_score:.3f}")
        
        return model_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        utilizer = JQuantsMaximumUtilizer()
        
        print("ğŸš€ J-Quantsæœ€å¤§æ´»ç”¨åˆ†æé–‹å§‹")
        print("="*60)
        
        # å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        jquants_data = utilizer.load_all_jquants_data()
        if not jquants_data:
            print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—")
            return 1
        
        # æœ€å¤§é™ç‰¹å¾´é‡ä½œæˆ
        df_enhanced = utilizer.create_maximum_features(jquants_data)
        
        # ç©¶æ¥µè©•ä¾¡
        results = utilizer.ultimate_evaluation(df_enhanced)
        
        if not results:
            print("âŒ è©•ä¾¡å¤±æ•—")
            return 1
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ† J-QUANTSæœ€å¤§æ´»ç”¨çµæœ")
        print("="*60)
        
        baseline = 0.517  # æ—¢å­˜ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        best_score = 0
        best_config = ""
        
        for feature_type, models in results.items():
            print(f"\nğŸ” {feature_type.upper().replace('_', ' ')}:")
            
            for model_name, result in models.items():
                score = result['score']
                std = result['std']
                improvement = score - baseline
                
                print(f"   {model_name:18s}: {score:.3f} Â± {std:.3f} ({improvement:+.3f})")
                
                if score > best_score:
                    best_score = score
                    best_config = f"{feature_type} + {model_name}"
        
        # æœ€çµ‚è©•ä¾¡
        total_improvement = best_score - baseline
        
        print(f"\nğŸ† æœ€é«˜æ€§èƒ½:")
        print(f"   è¨­å®š: {best_config}")
        print(f"   ç²¾åº¦: {best_score:.3f} ({best_score:.1%})")
        print(f"   æ”¹å–„: {total_improvement:+.3f} ({total_improvement:+.1%})")
        
        print(f"\nğŸ¯ ç›®æ¨™é”æˆè©•ä¾¡:")
        if best_score >= 0.60:
            print(f"   ğŸ‰ EXCELLENT! 60%é”æˆ!")
            print(f"   ğŸš€ è¶…é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ å®Œæˆ")
        elif best_score >= 0.57:
            print(f"   ğŸ”¥ GREAT! 57%ä»¥ä¸Šé”æˆ")
            print(f"   âœ… å®Ÿç”¨é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ ")
        elif best_score >= 0.55:
            print(f"   ğŸ‘ GOOD! 55%ä»¥ä¸Šé”æˆ")
            print(f"   âœ… å‰å›ã‚’ä¸Šå›ã‚‹æ”¹å–„")
        elif best_score >= 0.53:
            print(f"   ğŸ“ˆ ç›®æ¨™53%é”æˆ")
            print(f"   âœ… åŸºæœ¬ç›®æ¨™ã‚¯ãƒªã‚¢")
        else:
            print(f"   ğŸ’¡ ã•ã‚‰ãªã‚‹æ”¹å–„ä½™åœ°ã‚ã‚Š")
        
        print(f"\nğŸ’° åç›Šäºˆæƒ³:")
        if best_score >= 0.57:
            print(f"   æœŸå¾…å¹´ç‡: 15-25%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 12-20%")
        elif best_score >= 0.55:
            print(f"   æœŸå¾…å¹´ç‡: 12-18%") 
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 10-15%")
        else:
            print(f"   æœŸå¾…å¹´ç‡: 8-15%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 6-12%")
        
        print(f"\nğŸ“Š J-Quantsæ´»ç”¨åº¦è©•ä¾¡:")
        print(f"   ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ãƒ—ãƒ©ãƒ³æ´»ç”¨åº¦: 95-100%")
        print(f"   æœªæ´»ç”¨è¦ç´ : ã»ã¼ãªã—")
        print(f"   æ¬¡ã®å‘ä¸Š: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯Premiumãƒ—ãƒ©ãƒ³")
        
        return 0 if total_improvement > 0 else 1
        
    except Exception as e:
        logger.error(f"æœ€å¤§æ´»ç”¨åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    exit(main())