#!/usr/bin/env python3
"""
é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
60%è¶…ãˆã‚’ç›®æŒ‡ã™ç¬¬2æ®µéš: ãƒ©ã‚°ç‰¹å¾´é‡ã€ç§»å‹•çµ±è¨ˆã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã€å‘¨æœŸæ€§
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class AdvancedTimeSeriesFeatures:
    """é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
        # ç¾åœ¨ã®æœ€é©ç‰¹å¾´é‡
        self.base_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20',
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
        
    def load_integrated_data(self):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        df = pd.read_parquet(integrated_file)
        df['Date'] = pd.to_datetime(df['Date'])
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df):,}ä»¶")
        return df
    
    def create_lag_features(self, df, target_columns, lags=[1, 2, 3, 5, 10]):
        """ãƒ©ã‚°ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("â±ï¸ ãƒ©ã‚°ç‰¹å¾´é‡ä½œæˆ...")
        
        df_with_lags = df.copy()
        
        # å„éŠ˜æŸ„ã”ã¨ã«ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ
        for col in target_columns:
            if col not in df.columns:
                continue
                
            logger.info(f"  {col} ã®ãƒ©ã‚°ç‰¹å¾´é‡ä½œæˆ...")
            
            for lag in lags:
                lag_col = f"{col}_lag_{lag}"
                df_with_lags[lag_col] = df_with_lags.groupby('Code')[col].shift(lag)
        
        created_features = [f"{col}_lag_{lag}" for col in target_columns for lag in lags if col in df.columns]
        logger.info(f"  ãƒ©ã‚°ç‰¹å¾´é‡: {len(created_features)}å€‹ä½œæˆ")
        
        return df_with_lags, created_features
    
    def create_rolling_statistics(self, df, target_columns, windows=[5, 10, 20, 50]):
        """ç§»å‹•çµ±è¨ˆç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ“Š ç§»å‹•çµ±è¨ˆç‰¹å¾´é‡ä½œæˆ...")
        
        df_with_stats = df.copy()
        created_features = []
        
        for col in target_columns:
            if col not in df.columns:
                continue
                
            logger.info(f"  {col} ã®ç§»å‹•çµ±è¨ˆä½œæˆ...")
            
            for window in windows:
                # ç§»å‹•å¹³å‡
                ma_col = f"{col}_ma_{window}"
                df_with_stats[ma_col] = df_with_stats.groupby('Code')[col].rolling(window, min_periods=1).mean().reset_index(0, drop=True)
                
                # ç§»å‹•æ¨™æº–åå·®
                std_col = f"{col}_std_{window}"
                df_with_stats[std_col] = df_with_stats.groupby('Code')[col].rolling(window, min_periods=1).std().reset_index(0, drop=True)
                
                # ç¾åœ¨å€¤ã¨ç§»å‹•å¹³å‡ã®ä¹–é›¢
                diff_col = f"{col}_diff_ma_{window}"
                df_with_stats[diff_col] = (df_with_stats[col] - df_with_stats[ma_col]) / (df_with_stats[ma_col].abs() + 1e-8)
                
                created_features.extend([ma_col, std_col, diff_col])
        
        logger.info(f"  ç§»å‹•çµ±è¨ˆç‰¹å¾´é‡: {len(created_features)}å€‹ä½œæˆ")
        return df_with_stats, created_features
    
    def create_trend_features(self, df, target_columns, windows=[5, 10, 20]):
        """ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ä½œæˆ...")
        
        df_with_trends = df.copy()
        created_features = []
        
        def calculate_slope(series):
            """ç·šå½¢å›å¸°ã®å‚¾ãã‚’è¨ˆç®—"""
            if len(series) < 2:
                return 0
            x = np.arange(len(series))
            try:
                slope, _, _, _, _ = stats.linregress(x, series)
                return slope if not np.isnan(slope) else 0
            except:
                return 0
        
        for col in target_columns:
            if col not in df.columns:
                continue
                
            logger.info(f"  {col} ã®ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ä½œæˆ...")
            
            for window in windows:
                # ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå‚¾ãï¼‰
                slope_col = f"{col}_slope_{window}"
                df_with_trends[slope_col] = df_with_trends.groupby('Code')[col].rolling(window, min_periods=2).apply(calculate_slope).reset_index(0, drop=True)
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆRÂ²ï¼‰
                def calculate_r_squared(series):
                    if len(series) < 3:
                        return 0
                    x = np.arange(len(series))
                    try:
                        _, _, r_value, _, _ = stats.linregress(x, series)
                        return r_value ** 2 if not np.isnan(r_value) else 0
                    except:
                        return 0
                
                r2_col = f"{col}_r2_{window}"
                df_with_trends[r2_col] = df_with_trends.groupby('Code')[col].rolling(window, min_periods=3).apply(calculate_r_squared).reset_index(0, drop=True)
                
                created_features.extend([slope_col, r2_col])
        
        logger.info(f"  ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡: {len(created_features)}å€‹ä½œæˆ")
        return df_with_trends, created_features
    
    def create_cyclical_features(self, df):
        """å‘¨æœŸæ€§ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ”„ å‘¨æœŸæ€§ç‰¹å¾´é‡ä½œæˆ...")
        
        df_with_cycles = df.copy()
        
        # æ›œæ—¥åŠ¹æœ
        df_with_cycles['day_of_week'] = df_with_cycles['Date'].dt.dayofweek
        df_with_cycles['is_monday'] = (df_with_cycles['day_of_week'] == 0).astype(int)
        df_with_cycles['is_friday'] = (df_with_cycles['day_of_week'] == 4).astype(int)
        
        # æœˆåŠ¹æœ
        df_with_cycles['month'] = df_with_cycles['Date'].dt.month
        df_with_cycles['is_january'] = (df_with_cycles['month'] == 1).astype(int)
        df_with_cycles['is_december'] = (df_with_cycles['month'] == 12).astype(int)
        
        # å››åŠæœŸåŠ¹æœ
        df_with_cycles['quarter'] = df_with_cycles['Date'].dt.quarter
        df_with_cycles['is_q1'] = (df_with_cycles['quarter'] == 1).astype(int)
        df_with_cycles['is_q4'] = (df_with_cycles['quarter'] == 4).astype(int)
        
        # æœˆåˆãƒ»æœˆæœ«åŠ¹æœ
        df_with_cycles['day_of_month'] = df_with_cycles['Date'].dt.day
        df_with_cycles['is_month_start'] = (df_with_cycles['day_of_month'] <= 5).astype(int)
        df_with_cycles['is_month_end'] = (df_with_cycles['day_of_month'] >= 25).astype(int)
        
        # å¹´åŠ¹æœï¼ˆãƒªãƒ¼ãƒãƒ³ã‚·ãƒ§ãƒƒã‚¯ã€ã‚³ãƒ­ãƒŠã‚·ãƒ§ãƒƒã‚¯ç­‰ï¼‰
        df_with_cycles['year'] = df_with_cycles['Date'].dt.year
        df_with_cycles['is_crisis_year'] = df_with_cycles['year'].isin([2008, 2009, 2020]).astype(int)
        
        cyclical_features = [
            'day_of_week', 'is_monday', 'is_friday', 
            'month', 'is_january', 'is_december',
            'quarter', 'is_q1', 'is_q4',
            'is_month_start', 'is_month_end', 
            'is_crisis_year'
        ]
        
        logger.info(f"  å‘¨æœŸæ€§ç‰¹å¾´é‡: {len(cyclical_features)}å€‹ä½œæˆ")
        return df_with_cycles, cyclical_features
    
    def create_momentum_features(self, df, target_columns):
        """ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸš€ ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡ä½œæˆ...")
        
        df_with_momentum = df.copy()
        created_features = []
        
        for col in target_columns:
            if col not in df.columns:
                continue
                
            logger.info(f"  {col} ã®ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡ä½œæˆ...")
            
            # çŸ­æœŸãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ï¼ˆ3æ—¥ã€5æ—¥ï¼‰
            for period in [3, 5]:
                momentum_col = f"{col}_momentum_{period}"
                df_with_momentum[momentum_col] = df_with_momentum.groupby('Code')[col].pct_change(periods=period)
                created_features.append(momentum_col)
            
            # åŠ é€Ÿåº¦ï¼ˆå¤‰åŒ–ç‡ã®å¤‰åŒ–ç‡ï¼‰
            acceleration_col = f"{col}_acceleration"
            df_with_momentum[acceleration_col] = df_with_momentum.groupby('Code')[col].pct_change().pct_change()
            created_features.append(acceleration_col)
            
            # ç›¸å¯¾å¼·åº¦ï¼ˆéå»20æ—¥ã®åˆ†ä½æ•°ï¼‰
            def rolling_rank(series, window=20):
                return series.rolling(window, min_periods=1).rank(pct=True)
            
            rank_col = f"{col}_rank_20"
            df_with_momentum[rank_col] = df_with_momentum.groupby('Code')[col].apply(rolling_rank).reset_index(0, drop=True)
            created_features.append(rank_col)
        
        logger.info(f"  ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡: {len(created_features)}å€‹ä½œæˆ")
        return df_with_momentum, created_features
    
    def create_volatility_features(self, df, target_columns):
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ“Š ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡ä½œæˆ...")
        
        df_with_vol = df.copy()
        created_features = []
        
        for col in target_columns:
            if col not in df.columns:
                continue
                
            logger.info(f"  {col} ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡ä½œæˆ...")
            
            # å®Ÿç¾ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            for window in [5, 10, 20]:
                vol_col = f"{col}_realized_vol_{window}"
                df_with_vol[vol_col] = df_with_vol.groupby('Code')[col].rolling(window, min_periods=1).std().reset_index(0, drop=True)
                created_features.append(vol_col)
            
            # EWMA ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            ewma_col = f"{col}_ewma_vol"
            df_with_vol[ewma_col] = df_with_vol.groupby('Code')[col].ewm(span=20).std().reset_index(0, drop=True)
            created_features.append(ewma_col)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å¤‰åŒ–
            vol_change_col = f"{col}_vol_change"
            df_with_vol[vol_change_col] = df_with_vol.groupby('Code')[f'{col}_realized_vol_20'].pct_change()
            created_features.append(vol_change_col)
        
        logger.info(f"  ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡: {len(created_features)}å€‹ä½œæˆ")
        return df_with_vol, created_features
    
    def feature_selection_by_importance(self, X, y, all_features, top_k=30):
        """é‡è¦åº¦ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠ"""
        logger.info(f"ğŸ” ç‰¹å¾´é¸æŠï¼ˆä¸Šä½{top_k}å€‹ï¼‰...")
        
        # LogisticRegression ã§ç‰¹å¾´é‡è¦åº¦è¨ˆç®—
        X_scaled = self.scaler.fit_transform(X)
        model = LogisticRegression(C=0.001, class_weight='balanced', random_state=42, max_iter=1000)
        model.fit(X_scaled, y)
        
        # é‡è¦åº¦ï¼ˆä¿‚æ•°ã®çµ¶å¯¾å€¤ï¼‰
        importances = abs(model.coef_[0])
        feature_importance = list(zip(all_features, importances))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        # ä¸Šä½Kå€‹é¸æŠ
        selected_features = [feat for feat, imp in feature_importance[:top_k]]
        
        logger.info(f"ä¸Šä½{top_k}ç‰¹å¾´é‡é¸æŠå®Œäº†")
        logger.info("ä¸Šä½10ç‰¹å¾´é‡:")
        for i, (feat, imp) in enumerate(feature_importance[:10], 1):
            logger.info(f"  {i:2d}. {feat:30s}: {imp:.4f}")
        
        return selected_features, feature_importance
    
    def evaluate_enhanced_features(self, X, y, feature_set_name):
        """æ‹¡å¼µç‰¹å¾´é‡ã®è©•ä¾¡"""
        logger.info(f"ğŸ“Š {feature_set_name} è©•ä¾¡...")
        
        X_scaled = self.scaler.fit_transform(X.fillna(0))
        model = LogisticRegression(C=0.001, class_weight='balanced', random_state=42, max_iter=1000)
        
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        fold_details = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            fold_details.append({
                'fold': fold + 1,
                'accuracy': accuracy,
                'train_size': len(X_train),
                'test_size': len(X_test)
            })
        
        result = {
            'avg': np.mean(scores),
            'std': np.std(scores),
            'min': np.min(scores),
            'max': np.max(scores),
            'scores': scores,
            'fold_details': fold_details
        }
        
        logger.info(f"  {feature_set_name}: {result['avg']:.3%} Â± {result['std']:.3%}")
        return result

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
    logger.info("ğŸ¯ ç›®æ¨™: 59.4%ã‹ã‚‰62%è¶…ãˆã‚’ç›®æŒ‡ã™")
    
    ts_features = AdvancedTimeSeriesFeatures()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = ts_features.load_integrated_data()
        
        # 2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
        logger.info("ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡...")
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X_base = clean_df[ts_features.base_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        baseline_result = ts_features.evaluate_enhanced_features(X_base, y, "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³")
        
        # 3. æ®µéšçš„ç‰¹å¾´é‡è¿½åŠ 
        logger.info("\nğŸ”§ æ®µéšçš„ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...")
        
        enhanced_df = clean_df.copy()
        all_features = ts_features.base_features.copy()
        
        # ãƒ©ã‚°ç‰¹å¾´é‡è¿½åŠ 
        logger.info("\nâ±ï¸ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ã‚°ç‰¹å¾´é‡è¿½åŠ ...")
        key_features_for_lag = ['sp500_change', 'vix_change', 'Market_Return']
        enhanced_df, lag_features = ts_features.create_lag_features(enhanced_df, key_features_for_lag, lags=[1, 2, 3])
        all_features.extend(lag_features)
        
        X_with_lags = enhanced_df[all_features].fillna(0)
        lag_result = ts_features.evaluate_enhanced_features(X_with_lags, y, "ãƒ™ãƒ¼ã‚¹+ãƒ©ã‚°ç‰¹å¾´é‡")
        
        # ç§»å‹•çµ±è¨ˆè¿½åŠ 
        logger.info("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: ç§»å‹•çµ±è¨ˆè¿½åŠ ...")
        enhanced_df, stats_features = ts_features.create_rolling_statistics(enhanced_df, key_features_for_lag, windows=[5, 10, 20])
        all_features.extend(stats_features)
        
        X_with_stats = enhanced_df[all_features].fillna(0)
        stats_result = ts_features.evaluate_enhanced_features(X_with_stats, y, "ãƒ™ãƒ¼ã‚¹+ãƒ©ã‚°+ç§»å‹•çµ±è¨ˆ")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡è¿½åŠ 
        logger.info("\nğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡è¿½åŠ ...")
        enhanced_df, trend_features = ts_features.create_trend_features(enhanced_df, key_features_for_lag, windows=[5, 10])
        all_features.extend(trend_features)
        
        X_with_trends = enhanced_df[all_features].fillna(0)
        trend_result = ts_features.evaluate_enhanced_features(X_with_trends, y, "ãƒ™ãƒ¼ã‚¹+ãƒ©ã‚°+çµ±è¨ˆ+ãƒˆãƒ¬ãƒ³ãƒ‰")
        
        # å‘¨æœŸæ€§ç‰¹å¾´é‡è¿½åŠ 
        logger.info("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—4: å‘¨æœŸæ€§ç‰¹å¾´é‡è¿½åŠ ...")
        enhanced_df, cyclical_features = ts_features.create_cyclical_features(enhanced_df)
        all_features.extend(cyclical_features)
        
        X_with_cycles = enhanced_df[all_features].fillna(0)
        cycle_result = ts_features.evaluate_enhanced_features(X_with_cycles, y, "ãƒ™ãƒ¼ã‚¹+ãƒ©ã‚°+çµ±è¨ˆ+ãƒˆãƒ¬ãƒ³ãƒ‰+å‘¨æœŸ")
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡è¿½åŠ 
        logger.info("\nğŸš€ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡è¿½åŠ ...")
        enhanced_df, momentum_features = ts_features.create_momentum_features(enhanced_df, key_features_for_lag)
        all_features.extend(momentum_features)
        
        X_with_momentum = enhanced_df[all_features].fillna(0)
        momentum_result = ts_features.evaluate_enhanced_features(X_with_momentum, y, "å…¨ç‰¹å¾´é‡")
        
        # ç‰¹å¾´é¸æŠã«ã‚ˆã‚‹æœ€é©åŒ–
        logger.info("\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—6: ç‰¹å¾´é¸æŠæœ€é©åŒ–...")
        X_full = enhanced_df[all_features].fillna(0)
        selected_features, feature_importance = ts_features.feature_selection_by_importance(X_full, y, all_features, top_k=25)
        
        X_selected = enhanced_df[selected_features].fillna(0)
        selected_result = ts_features.evaluate_enhanced_features(X_selected, y, "é¸æŠæ¸ˆã¿ç‰¹å¾´é‡(25å€‹)")
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*100)
        logger.info("ğŸ† é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµæœ")
        logger.info("="*100)
        
        results = [
            ("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³", baseline_result),
            ("ãƒ©ã‚°ç‰¹å¾´é‡è¿½åŠ ", lag_result),
            ("ç§»å‹•çµ±è¨ˆè¿½åŠ ", stats_result),
            ("ãƒˆãƒ¬ãƒ³ãƒ‰è¿½åŠ ", trend_result),
            ("å‘¨æœŸæ€§è¿½åŠ ", cycle_result),
            ("ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ è¿½åŠ ", momentum_result),
            ("ç‰¹å¾´é¸æŠå¾Œ", selected_result)
        ]
        
        baseline_score = baseline_result['avg']
        
        logger.info("ğŸ“ˆ æ®µéšçš„æ”¹å–„çµæœ:")
        for i, (name, result) in enumerate(results, 1):
            improvement = (result['avg'] - baseline_score) * 100
            status = "ğŸš€" if improvement > 2.0 else "ğŸ“ˆ" if improvement > 0.5 else "ğŸ“Š" if improvement >= 0 else "ğŸ“‰"
            logger.info(f"  {i}. {name:20s}: {result['avg']:.3%} ({improvement:+.2f}%) {status}")
        
        # æœ€é«˜çµæœ
        best_result = max(results, key=lambda x: x[1]['avg'])
        final_improvement = (best_result[1]['avg'] - baseline_score) * 100
        
        logger.info(f"\nğŸ† æœ€é«˜æ€§èƒ½:")
        logger.info(f"  æ‰‹æ³•: {best_result[0]}")
        logger.info(f"  ç²¾åº¦: {best_result[1]['avg']:.3%} Â± {best_result[1]['std']:.3%}")
        logger.info(f"  å‘ä¸Š: {final_improvement:+.2f}% ({baseline_score:.1%} â†’ {best_result[1]['avg']:.1%})")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_60 = 0.60
        target_62 = 0.62
        
        if best_result[1]['avg'] >= target_62:
            logger.info(f"ğŸ‰ ç›®æ¨™å¤§å¹…é”æˆï¼ 62%è¶…ãˆ ({best_result[1]['avg']:.1%} >= 62.0%)")
        elif best_result[1]['avg'] >= target_60:
            logger.info(f"âœ… ç›®æ¨™é”æˆï¼ 60%è¶…ãˆ ({best_result[1]['avg']:.1%} >= 60.0%)")
        else:
            logger.info(f"ğŸ“ˆ æ”¹å–„åŠ¹æœç¢ºèª ({best_result[1]['avg']:.1%})")
        
        # ç‰¹å¾´é‡çµ±è¨ˆ
        logger.info(f"\nğŸ“Š ç‰¹å¾´é‡çµ±è¨ˆ:")
        logger.info(f"  ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡: {len(ts_features.base_features)}å€‹")
        logger.info(f"  è¿½åŠ ç‰¹å¾´é‡: {len(all_features) - len(ts_features.base_features)}å€‹")
        logger.info(f"  ç·ç‰¹å¾´é‡: {len(all_features)}å€‹")
        logger.info(f"  é¸æŠç‰¹å¾´é‡: {len(selected_features)}å€‹")
        
        logger.info(f"\nâš–ï¸ ã“ã®çµæœã¯å…¨ãƒ‡ãƒ¼ã‚¿{len(clean_df):,}ä»¶ã§ã®å³å¯†ãªæ™‚ç³»åˆ—æ¤œè¨¼ã§ã™")
        logger.info(f"âœ… ç¬¬2æ®µéšå®Œäº†: æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()