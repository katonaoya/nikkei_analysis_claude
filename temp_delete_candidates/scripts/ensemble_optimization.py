#!/usr/bin/env python3
"""
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ– - è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class EnsembleOptimizer:
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–"""
    
    def __init__(self, data_dir: Path = None):
        self.data_dir = data_dir or Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
    
    def load_features(self, filename: str) -> pd.DataFrame:
        """ç‰¹å¾´é‡èª­ã¿è¾¼ã¿"""
        file_path = self.processed_dir / filename
        df = pd.read_parquet(file_path)
        logger.info(f"Loaded features: {df.shape}")
        return df
    
    def prepare_data(self, df: pd.DataFrame) -> tuple:
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        
        # é™¤å¤–åˆ—
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Volume',
            'Next_Day_Return', 'Binary_Direction'
        }
        
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[feature_cols].fillna(0)
        y = clean_df['Binary_Direction']
        dates = clean_df['Date']
        
        logger.info(f"Features: {len(feature_cols)}, Samples: {len(X)}")
        logger.info(f"Target balance: {y.value_counts().to_dict()}")
        
        return X, y, dates, feature_cols
    
    def create_optimized_models(self):
        """æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ç¾¤"""
        
        models = {
            # 1. ç·šå½¢ãƒ¢ãƒ‡ãƒ«
            'Logistic_L1': LogisticRegression(
                penalty='l1', solver='liblinear', C=0.1,
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'Logistic_L2': LogisticRegression(
                penalty='l2', C=1.0,
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            
            # 2. æœ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šæ§˜æ€§é‡è¦–ï¼‰
            'RandomForest_Conservative': RandomForestClassifier(
                n_estimators=200, max_depth=8, min_samples_split=50,
                min_samples_leaf=20, max_features='sqrt',
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'RandomForest_Aggressive': RandomForestClassifier(
                n_estimators=300, max_depth=15, min_samples_split=20,
                min_samples_leaf=10, max_features='log2',
                class_weight='balanced', random_state=43, n_jobs=-1
            ),
            'ExtraTrees': ExtraTreesClassifier(
                n_estimators=200, max_depth=10, min_samples_split=30,
                min_samples_leaf=15, max_features='sqrt',
                class_weight='balanced', random_state=44, n_jobs=-1
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=100, learning_rate=0.1, max_depth=6,
                min_samples_split=50, min_samples_leaf=20,
                random_state=45
            ),
            
            # 3. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            'MLP_Small': MLPClassifier(
                hidden_layer_sizes=(50, 25), activation='relu', 
                alpha=0.01, learning_rate='adaptive', max_iter=500,
                random_state=46
            ),
            'MLP_Medium': MLPClassifier(
                hidden_layer_sizes=(100, 50, 25), activation='tanh',
                alpha=0.001, learning_rate='adaptive', max_iter=500,
                random_state=47
            )
        }
        
        return models
    
    def evaluate_individual_models(self, X, y, dates, models, n_splits=3):
        """å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        logger.info("ğŸ” Evaluating individual models...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        results = {}
        
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        
        for model_name, model in models.items():
            logger.info(f"Testing {model_name}...")
            
            fold_accuracies = []
            fold_aucs = []
            
            for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
                X_train = X_scaled.iloc[train_idx] if 'MLP' in model_name or 'Logistic' in model_name else X.iloc[train_idx]
                X_test = X_scaled.iloc[test_idx] if 'MLP' in model_name or 'Logistic' in model_name else X.iloc[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                # è¨“ç·´
                model.fit(X_train, y_train)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                fold_accuracies.append(accuracy)
                
                # AUCï¼ˆå¯èƒ½ãªå ´åˆï¼‰
                if hasattr(model, 'predict_proba'):
                    y_proba = model.predict_proba(X_test)[:, 1]
                    auc = roc_auc_score(y_test, y_proba)
                    fold_aucs.append(auc)
            
            results[model_name] = {
                'accuracies': fold_accuracies,
                'avg_accuracy': np.mean(fold_accuracies),
                'std_accuracy': np.std(fold_accuracies),
                'avg_auc': np.mean(fold_aucs) if fold_aucs else None
            }
            
            logger.info(f"  {model_name}: {np.mean(fold_accuracies):.3f} Â± {np.std(fold_accuracies):.3f}")
        
        return results
    
    def create_ensemble_predictions(self, X, y, dates, models, n_splits=3):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""
        logger.info("ğŸ¯ Creating ensemble predictions...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        ensemble_results = []
        
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            logger.info(f"Ensemble fold {fold + 1}...")
            
            # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’åé›†
            model_predictions = {}
            model_probabilities = {}
            
            for model_name, model in models.items():
                X_train = X_scaled.iloc[train_idx] if 'MLP' in model_name or 'Logistic' in model_name else X.iloc[train_idx]
                X_test = X_scaled.iloc[test_idx] if 'MLP' in model_name or 'Logistic' in model_name else X.iloc[test_idx]
                y_train = y.iloc[train_idx]
                
                # è¨“ç·´ã¨äºˆæ¸¬
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                model_predictions[model_name] = y_pred
                
                if hasattr(model, 'predict_proba'):
                    y_proba = model.predict_proba(X_test)[:, 1]
                    model_probabilities[model_name] = y_proba
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥
            y_test = y.iloc[test_idx]
            
            # 1. å˜ç´”å¤šæ•°æ±º
            predictions_array = np.array(list(model_predictions.values()))
            majority_vote = np.round(np.mean(predictions_array, axis=0)).astype(int)
            majority_accuracy = accuracy_score(y_test, majority_vote)
            
            # 2. é‡ã¿ä»˜ãå¹³å‡ï¼ˆç¢ºç‡ãƒ™ãƒ¼ã‚¹ï¼‰
            if model_probabilities:
                prob_array = np.array(list(model_probabilities.values()))
                weighted_prob = np.mean(prob_array, axis=0)
                weighted_pred = (weighted_prob >= 0.5).astype(int)
                weighted_accuracy = accuracy_score(y_test, weighted_pred)
            else:
                weighted_accuracy = majority_accuracy
            
            # 3. ä¸Šä½ãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨
            top_models = ['RandomForest_Aggressive', 'GradientBoosting', 'ExtraTrees']
            top_predictions = [model_predictions[name] for name in top_models if name in model_predictions]
            if top_predictions:
                top_pred_array = np.array(top_predictions)
                top_vote = np.round(np.mean(top_pred_array, axis=0)).astype(int)
                top_accuracy = accuracy_score(y_test, top_vote)
            else:
                top_accuracy = majority_accuracy
            
            ensemble_results.append({
                'fold': fold + 1,
                'majority_accuracy': majority_accuracy,
                'weighted_accuracy': weighted_accuracy,
                'top_models_accuracy': top_accuracy,
                'test_samples': len(y_test)
            })
        
        return ensemble_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="Ensemble optimization")
    parser.add_argument("--features-file", required=True, help="Features file")
    
    args = parser.parse_args()
    
    try:
        optimizer = EnsembleOptimizer()
        
        print("ğŸ“Š Loading features...")
        df = optimizer.load_features(args.features_file)
        
        print("ğŸ”§ Preparing data...")
        X, y, dates, feature_cols = optimizer.prepare_data(df)
        
        print("ğŸ¤– Creating optimized models...")
        models = optimizer.create_optimized_models()
        
        print("ğŸ” Evaluating individual models...")
        individual_results = optimizer.evaluate_individual_models(X, y, dates, models)
        
        print("ğŸ¯ Creating ensemble predictions...")
        ensemble_results = optimizer.create_ensemble_predictions(X, y, dates, models)
        
        # ãƒ¬ãƒãƒ¼ãƒˆ
        print("\n" + "="*60)
        print("ğŸ“‹ ENSEMBLE OPTIMIZATION RESULTS")
        print("="*60)
        
        print("\nğŸ¤– Individual Model Performance:")
        for model_name, result in individual_results.items():
            auc_text = f", AUC: {result['avg_auc']:.3f}" if result['avg_auc'] else ""
            print(f"   {model_name:25s}: {result['avg_accuracy']:.3f} Â± {result['std_accuracy']:.3f}{auc_text}")
        
        print("\nğŸ¯ Ensemble Performance:")
        majority_accs = [r['majority_accuracy'] for r in ensemble_results]
        weighted_accs = [r['weighted_accuracy'] for r in ensemble_results]
        top_accs = [r['top_models_accuracy'] for r in ensemble_results]
        
        print(f"   Majority Vote:    {np.mean(majority_accs):.3f} Â± {np.std(majority_accs):.3f}")
        print(f"   Weighted Average: {np.mean(weighted_accs):.3f} Â± {np.std(weighted_accs):.3f}")
        print(f"   Top Models Only:  {np.mean(top_accs):.3f} Â± {np.std(top_accs):.3f}")
        
        best_ensemble = max(np.mean(majority_accs), np.mean(weighted_accs), np.mean(top_accs))
        best_individual = max([r['avg_accuracy'] for r in individual_results.values()])
        
        improvement = best_ensemble - best_individual
        print(f"\nğŸ“ˆ Best Ensemble vs Best Individual:")
        print(f"   Improvement: +{improvement:.3f} ({improvement*100:.1f}%)")
        
        if best_ensemble > 0.52:
            print("\nğŸ‰ SUCCESS: Achieved >52% accuracy target!")
        elif best_ensemble > 0.515:
            print("\nğŸ‘ GOOD: Significant improvement achieved")
        else:
            print("\nğŸ’¡ MODERATE: Some improvement, try additional techniques")
        
        print("\nâœ… Ensemble optimization completed!")
        
    except Exception as e:
        logger.error(f"Ensemble optimization failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())