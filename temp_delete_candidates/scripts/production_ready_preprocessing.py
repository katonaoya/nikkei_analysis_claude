#!/usr/bin/env python3
"""
æœ¬ç•ªé‹ç”¨ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
æ¬ æå€¤å‡¦ç†ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ageå®Œå…¨æ’é™¤ã€æ™‚ç³»åˆ—æ•´åˆæ€§ç¢ºä¿
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime, timedelta
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

class ProductionDataPreprocessor:
    """æœ¬ç•ªé‹ç”¨ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
    
    def __init__(self, data_dir: Path = None):
        self.data_dir = data_dir or Path("data")
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
    def load_and_clean_raw_data(self, file_pattern: str) -> pd.DataFrame:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        logger.info("ğŸ“Š Loading and cleaning raw data...")
        
        data_files = list(self.raw_dir.glob(f"{file_pattern}.parquet"))
        if not data_files:
            raise FileNotFoundError(f"No data files found matching pattern: {file_pattern}")
        
        logger.info(f"Found {len(data_files)} data files")
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
        dfs = []
        for file_path in sorted(data_files):
            logger.info(f"Loading: {file_path.name}")
            df_file = pd.read_parquet(file_path)
            
            # é‡è¤‡åˆ—ã®å•é¡Œã‚’ã“ã“ã§ä¿®æ­£
            if df_file.columns.duplicated().any():
                logger.warning(f"File {file_path.name} has duplicate columns, fixing...")
                df_file = df_file.loc[:, ~df_file.columns.duplicated()]
            
            df_file = self._standardize_columns(df_file)
            dfs.append(df_file)
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        df = pd.concat(dfs, ignore_index=True)
        
        # çµåˆå¾Œã®é‡è¤‡åˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€çµ‚ç¢ºèªï¼‰
        if df.columns.duplicated().any():
            logger.warning("Duplicate columns found after concat, removing...")
            df = df.loc[:, ~df.columns.duplicated()]
        
        logger.info(f"Final columns: {list(df.columns)}")
        
        # é‡è¤‡é™¤å»ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
        initial_count = len(df)
        df = df.drop_duplicates(subset=['Date', 'Code'], keep='last')
        duplicate_removed = initial_count - len(df)
        if duplicate_removed > 0:
            logger.info(f"Removed {duplicate_removed} duplicate records")
        
        # æ—¥ä»˜é †ã‚½ãƒ¼ãƒˆ
        df = df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        logger.info(f"Loaded {len(df)} records for {df['Code'].nunique()} stocks")
        logger.info(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
        
        return df
    
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚«ãƒ©ãƒ åã®æ¨™æº–åŒ–"""
        df = df.copy()
        
        # é‡è¤‡åˆ—ã®å‰Šé™¤
        df = df.loc[:, ~df.columns.duplicated()]
        
        # æ—¥ä»˜å¤‰æ›
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
        elif 'date' in df.columns:
            df['Date'] = pd.to_datetime(df['date'])
            df = df.drop('date', axis=1)
        
        # ã‚«ãƒ©ãƒ åã®çµ±ä¸€
        rename_map = {
            'close': 'Close', 'open': 'Open', 'high': 'High',
            'low': 'Low', 'volume': 'Volume', 'code': 'Code'
        }
        
        df = df.rename(columns=rename_map)
        return df
    
    def strict_missing_value_handling(self, df: pd.DataFrame) -> pd.DataFrame:
        """å³æ ¼ãªæ¬ æå€¤å‡¦ç†"""
        logger.info("ğŸ”§ Applying strict missing value handling...")
        
        # åŸºæœ¬çš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒæ¬ æã—ã¦ã„ã‚‹è¡Œã‚’é™¤å¤–
        critical_cols = ['Date', 'Code', 'Close']
        for col in critical_cols:
            if col in df.columns:
                missing_count = df[col].isnull().sum()
                if missing_count > 0:
                    logger.warning(f"Removing {missing_count} records with missing {col}")
                    df = df[df[col].notna()]
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
        if 'Close' in df.columns:
            # è² ã®ä¾¡æ ¼ã‚„0ã®é™¤å¤–
            invalid_price_count = (df['Close'] <= 0).sum()
            if invalid_price_count > 0:
                logger.warning(f"Removing {invalid_price_count} records with invalid prices")
                df = df[df['Close'] > 0]
            
            # æ¥µç«¯ã«é«˜ã„ä¾¡æ ¼ï¼ˆ10ä¸‡å††è¶…ï¼‰ã®é™¤å¤– - ã‚ˆã‚Šå³æ ¼
            extreme_price_count = (df['Close'] > 100000).sum()
            if extreme_price_count > 0:
                logger.warning(f"Removing {extreme_price_count} records with extreme prices (>100k)")
                df = df[df['Close'] <= 100000]
            
            # æ¥µç«¯ã«å®‰ã„ä¾¡æ ¼ï¼ˆ10å††æœªæº€ï¼‰ã®é™¤å¤–
            low_price_count = (df['Close'] < 10).sum()
            if low_price_count > 0:
                logger.warning(f"Removing {low_price_count} records with extremely low prices (<10)")
                df = df[df['Close'] >= 10]
        
        # éŠ˜æŸ„ã”ã¨ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ã¨æ—¥ä»˜ã‚®ãƒ£ãƒƒãƒ—å•é¡Œã®ä¿®æ­£
        df = df.sort_values(['Code', 'Date'])
        
        clean_stocks = []
        for code in df['Code'].unique():
            stock_data = df[df['Code'] == code].copy().sort_values('Date')
            
            # 5æ—¥ä»¥ä¸Šã®æ—¥ä»˜ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹å ´åˆã€ãã®ã‚®ãƒ£ãƒƒãƒ—ã‚’å‰Šé™¤
            stock_data['Date_Diff'] = stock_data['Date'].diff().dt.days
            large_gaps = stock_data['Date_Diff'] > 5
            
            if large_gaps.any():
                logger.warning(f"Stock {code}: Removing {large_gaps.sum()} records with large date gaps")
                stock_data = stock_data[~large_gaps]
            
            # æœ€ä½30æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            if len(stock_data) >= 30:
                stock_data = stock_data.drop('Date_Diff', axis=1)
                clean_stocks.append(stock_data)
        
        if not clean_stocks:
            raise ValueError("No stocks with sufficient data after cleaning")
        
        df = pd.concat(clean_stocks, ignore_index=True)
        
        # æ•°å€¤åˆ—ã®å‰æ–¹è£œå®Œï¼ˆæœ€å¤§5æ—¥ã¾ã§ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in ['Date', 'Code']]
        
        for code in df['Code'].unique():
            mask = df['Code'] == code
            stock_data = df[mask].copy()
            
            # å‰æ–¹è£œå®Œï¼ˆæœ€å¤§5å–¶æ¥­æ—¥ï¼‰
            for col in numeric_cols:
                if col in stock_data.columns:
                    stock_data[col] = stock_data[col].fillna(method='ffill', limit=5)
            
            df.loc[mask] = stock_data
        
        # æ®‹ã£ãŸæ¬ æå€¤ã¯é™¤å¤–ï¼ˆä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        initial_count = len(df)
        df = df.dropna(subset=numeric_cols[:10])  # ä¸»è¦ãªæ•°å€¤åˆ—ã®ã¿ãƒã‚§ãƒƒã‚¯
        removed_count = initial_count - len(df)
        
        if removed_count > 0:
            logger.info(f"Removed {removed_count} records with remaining missing values")
        
        # æœ€çµ‚çš„ãªæ¬ æå€¤ãƒ¬ãƒãƒ¼ãƒˆ
        missing_summary = df.isnull().sum()
        total_missing = missing_summary.sum()
        missing_rate = total_missing / (len(df) * len(df.columns))
        
        logger.info(f"Final missing values: {total_missing} ({missing_rate:.2%})")
        
        return df
    
    def generate_leak_free_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒªãƒ¼ã‚¯ageã‚’å®Œå…¨æ’é™¤ã—ãŸç‰¹å¾´é‡ç”Ÿæˆ"""
        logger.info("ğŸ”’ Generating leak-free features...")
        
        results = []
        
        for code in df['Code'].unique():
            stock_data = df[df['Code'] == code].copy().sort_values('Date')
            
            if len(stock_data) < 30:  # æœ€ä½30æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                continue
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
            dates = stock_data['Date'].values
            close = stock_data['Close'].values
            high = stock_data['High'].values if 'High' in stock_data.columns else close
            low = stock_data['Low'].values if 'Low' in stock_data.columns else close
            volume = stock_data['Volume'].values if 'Volume' in stock_data.columns else np.ones(len(stock_data))
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦æ§‹ç¯‰
            stock_df = pd.DataFrame({
                'Date': dates,
                'Code': code,  # æ–‡å­—åˆ—ã¨ã—ã¦ç›´æ¥æŒ‡å®š
                'Close': close,
                'High': high,
                'Low': low,
                'Volume': volume
            })
            
            # å³æ ¼ãªãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—ï¼ˆãƒªãƒ¼ã‚¯ageé˜²æ­¢ï¼‰
            returns = np.concatenate([[0], np.diff(np.log(close))])
            stock_df['Returns'] = returns
            
            # ç§»å‹•å¹³å‡ï¼ˆå³æ ¼ãªæ™‚ç³»åˆ—å‡¦ç†ï¼‰
            for period in [5, 10, 20]:
                ma = pd.Series(close).rolling(period, min_periods=period).mean().values
                stock_df[f'MA_{period}'] = ma
                
                # ä¾¡æ ¼ã¨ç§»å‹•å¹³å‡ã®ä¹–é›¢ï¼ˆå‰æ—¥æ™‚ç‚¹ã®æƒ…å ±ã®ã¿ä½¿ç”¨ï¼‰
                price_vs_ma = np.roll(close / ma - 1, 1)  # 1æ—¥ãƒ©ã‚°ã‚’é©ç”¨
                price_vs_ma[0] = 0  # æœ€åˆã®å€¤ã¯0
                stock_df[f'Price_vs_MA{period}'] = price_vs_ma
            
            # RSIï¼ˆå³æ ¼ãªå®Ÿè£…ï¼‰
            rsi = self._calculate_strict_rsi(close, 14)
            stock_df['RSI_14'] = rsi
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆéå»20æ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³ã‹ã‚‰è¨ˆç®—ï¼‰
            vol = pd.Series(returns).rolling(20, min_periods=20).std().values
            stock_df['Volatility_20'] = vol
            
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™
            vol_ma = pd.Series(volume).rolling(10, min_periods=10).mean().values
            stock_df['Volume_MA_10'] = vol_ma
            vol_ratio = volume / vol_ma
            vol_ratio[vol_ma == 0] = 1  # ã‚¼ãƒ­é™¤ç®—å¯¾ç­–
            stock_df['Volume_Ratio'] = vol_ratio
            
            # ä¾¡æ ¼ä½ç½®ï¼ˆéå»20æ—¥ã®ãƒ¬ãƒ³ã‚¸å†…ã§ã®ä½ç½®ï¼‰
            high_20 = pd.Series(high).rolling(20, min_periods=20).max().values
            low_20 = pd.Series(low).rolling(20, min_periods=20).min().values
            price_position = (close - low_20) / (high_20 - low_20)
            price_position[high_20 == low_20] = 0.5  # ãƒ¬ãƒ³ã‚¸ãŒãªã„å ´åˆã¯ä¸­å¤®å€¤
            stock_df['Price_Position'] = price_position
            
            # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆæ˜ç¤ºçš„ãªéå»ãƒ‡ãƒ¼ã‚¿ï¼‰
            for lag in [1, 2, 5]:
                stock_df[f'Return_Lag_{lag}'] = np.roll(returns, lag)
                stock_df[f'Close_Lag_{lag}'] = np.roll(close, lag)
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆå³æ ¼ãªæœªæ¥ãƒ‡ãƒ¼ã‚¿ï¼‰
            # æ¬¡æ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç¾åœ¨ã®Closeã‹ã‚‰æ¬¡æ—¥ã®Closeã¸ã®å¤‰åŒ–ï¼‰
            next_close = np.roll(close, -1)
            next_return = next_close / close - 1
            next_return[-1] = 0  # æœ€å¾Œã®æ—¥ã¯äºˆæ¸¬ä¸å¯èƒ½ãªã®ã§0
            
            stock_df['Next_Day_Return'] = next_return
            stock_df['Binary_Direction'] = (next_return > 0).astype(int)
            
            # ç„¡é™å¤§ãƒ»NaNã®å‡¦ç†
            stock_df = stock_df.replace([np.inf, -np.inf], np.nan)
            
            # æœ€åˆã®æ•°æ—¥ã¨æœ€å¾Œã®æ—¥ã¯äºˆæ¸¬ã«ä½¿ç”¨ã—ãªã„ï¼ˆä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ï¼‰
            stock_df = stock_df.iloc[20:-1]  # æœ€åˆã®20æ—¥ã¨æœ€å¾Œã®1æ—¥ã‚’é™¤å¤–
            
            if len(stock_df) > 0:
                results.append(stock_df)
        
        if not results:
            raise ValueError("No valid data after processing")
        
        final_df = pd.concat(results, ignore_index=True)
        
        # æœ€çµ‚çš„ãªæ¬ æå€¤å‡¦ç†
        final_df = final_df.fillna(0)
        
        logger.info(f"Generated features for {len(final_df)} records")
        logger.info(f"Feature count: {len(final_df.columns)}")
        
        return final_df
    
    def _calculate_strict_rsi(self, prices: np.array, period: int = 14) -> np.array:
        """å³æ ¼ãªRSIè¨ˆç®—ï¼ˆãƒªãƒ¼ã‚¯ageé˜²æ­¢ï¼‰"""
        if len(prices) < period + 1:
            return np.full(len(prices), 50.0)
        
        deltas = np.diff(prices)
        gains = np.where(deltas > 0, deltas, 0)
        losses = np.where(deltas < 0, -deltas, 0)
        
        # ç§»å‹•å¹³å‡ã®è¨ˆç®—
        avg_gains = pd.Series(gains).rolling(period, min_periods=period).mean()
        avg_losses = pd.Series(losses).rolling(period, min_periods=period).mean()
        
        # RSIã®è¨ˆç®—
        rs = avg_gains / avg_losses
        rs = rs.fillna(0)  # 0é™¤ç®—å¯¾ç­–
        
        rsi = 100 - (100 / (1 + rs))
        rsi = rsi.fillna(50)  # NaNã¯50ã§åŸ‹ã‚ã‚‹
        
        # æœ€åˆã®å€¤ã¯50ã§åŸ‹ã‚ã‚‹
        result = np.full(len(prices), 50.0)
        result[1:] = rsi.values
        
        return result
    
    def add_market_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¸‚å ´å…¨ä½“ã®ç‰¹å¾´é‡è¿½åŠ ï¼ˆãƒªãƒ¼ã‚¯ageé˜²æ­¢ï¼‰"""
        logger.info("ğŸ“ˆ Adding market-wide features...")
        
        # å„æ—¥ã®å¸‚å ´å…¨ä½“æŒ‡æ¨™ã‚’è¨ˆç®—
        market_features = []
        
        for date in sorted(df['Date'].unique()):
            day_data = df[df['Date'] == date]
            
            if len(day_data) > 1:
                # å‰æ—¥æ¯”ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—ï¼ˆãã®æ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ä½¿ç”¨ï¼‰
                returns = day_data['Returns'].values
                market_return = np.nanmean(returns)
                market_vol = np.nanstd(returns)
                market_breadth = np.nanmean(returns > 0)
                
                # å‰æ—¥ã®æƒ…å ±ã‚’ä½¿ç”¨ï¼ˆãƒªãƒ¼ã‚¯ageé˜²æ­¢ï¼‰
                market_features.append({
                    'Date': date,
                    'Market_Return': market_return if not np.isnan(market_return) else 0,
                    'Market_Volatility': market_vol if not np.isnan(market_vol) else 0,
                    'Market_Breadth': market_breadth if not np.isnan(market_breadth) else 0.5
                })
            else:
                market_features.append({
                    'Date': date,
                    'Market_Return': 0,
                    'Market_Volatility': 0,
                    'Market_Breadth': 0.5
                })
        
        market_df = pd.DataFrame(market_features)
        
        # 1æ—¥ãƒ©ã‚°ã‚’é©ç”¨ï¼ˆãƒªãƒ¼ã‚¯ageé˜²æ­¢ï¼‰
        market_df['Market_Return'] = market_df['Market_Return'].shift(1).fillna(0)
        market_df['Market_Volatility'] = market_df['Market_Volatility'].shift(1).fillna(0)
        market_df['Market_Breadth'] = market_df['Market_Breadth'].shift(1).fillna(0.5)
        
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãƒãƒ¼ã‚¸
        df = df.merge(market_df, on='Date', how='left')
        
        # ç›¸å¯¾ãƒªã‚¿ãƒ¼ãƒ³ã®è¨ˆç®—
        df['Relative_Return'] = df['Returns'] - df['Market_Return']
        
        return df
    
    def final_quality_check(self, df: pd.DataFrame) -> dict:
        """æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯"""
        logger.info("âœ… Performing final quality check...")
        
        issues = []
        stats = {}
        
        # åˆ—ã®å­˜åœ¨ç¢ºèª
        if 'Code' not in df.columns:
            raise ValueError("Required column 'Code' not found in dataframe")
        if 'Date' not in df.columns:
            raise ValueError("Required column 'Date' not found in dataframe")
        
        # åŸºæœ¬çµ±è¨ˆ
        stats['total_records'] = len(df)
        stats['unique_stocks'] = df['Code'].nunique()
        stats['date_range'] = {
            'start': df['Date'].min(),
            'end': df['Date'].max(),
            'span_days': (df['Date'].max() - df['Date'].min()).days
        }
        
        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_counts = df.isnull().sum()
        total_missing = missing_counts.sum()
        missing_rate = total_missing / (len(df) * len(df.columns))
        
        stats['missing_values'] = {
            'total': int(total_missing),
            'rate': float(missing_rate)
        }
        
        if missing_rate > 0.05:  # 5%è¶…ã®æ¬ æå€¤
            issues.append(f"High missing value rate: {missing_rate:.2%}")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒ
        if 'Binary_Direction' in df.columns:
            target_dist = df['Binary_Direction'].value_counts()
            stats['target_distribution'] = target_dist.to_dict()
            
            minority_ratio = target_dist.min() / target_dist.sum()
            if minority_ratio < 0.3:
                issues.append(f"Severe class imbalance: {minority_ratio:.1%}")
        
        # ç„¡é™å¤§ãƒ»ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col in df.columns]
        
        for col in numeric_cols:
            if col not in ['Date', 'Code']:
                inf_count = np.isinf(df[col]).sum()
                if inf_count > 0:
                    issues.append(f"Infinite values in {col}: {inf_count}")
                
                # æ¥µç«¯ãªå¤–ã‚Œå€¤
                if col == 'Returns':
                    extreme_returns = (np.abs(df[col]) > 0.5).sum()  # 50%è¶…ã®æ—¥æ¬¡å¤‰å‹•
                    if extreme_returns > 0:
                        issues.append(f"Extreme returns in {col}: {extreme_returns}")
        
        return {
            'status': 'PASS' if not issues else 'WARNING',
            'issues': issues,
            'stats': stats
        }
    
    def save_processed_data(self, df: pd.DataFrame, filename: str = None) -> Path:
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"production_features_{timestamp}.parquet"
        
        output_path = self.processed_dir / filename
        df.to_parquet(output_path, index=False)
        logger.info(f"Saved production-ready data to: {output_path}")
        
        return output_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="Production-ready data preprocessing")
    parser.add_argument("--file-pattern", type=str, default="nikkei225_historical_20*", help="Input file pattern")
    parser.add_argument("--output-filename", type=str, help="Output filename")
    
    args = parser.parse_args()
    
    try:
        preprocessor = ProductionDataPreprocessor()
        
        print("ğŸš€ PRODUCTION-READY DATA PREPROCESSING")
        print("="*60)
        
        # 1. ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        print("\nğŸ“Š Loading and cleaning raw data...")
        df = preprocessor.load_and_clean_raw_data(args.file_pattern)
        
        # 2. å³æ ¼ãªæ¬ æå€¤å‡¦ç†
        print("\nğŸ”§ Applying strict missing value handling...")
        df = preprocessor.strict_missing_value_handling(df)
        
        # 3. ãƒªãƒ¼ã‚¯freeãªç‰¹å¾´é‡ç”Ÿæˆ
        print("\nğŸ”’ Generating leak-free features...")
        df = preprocessor.generate_leak_free_features(df)
        
        # 4. å¸‚å ´å…¨ä½“ç‰¹å¾´é‡è¿½åŠ 
        print("\nğŸ“ˆ Adding market-wide features...")
        df = preprocessor.add_market_features(df)
        
        # 5. æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯
        print("\nâœ… Performing final quality check...")
        quality_report = preprocessor.final_quality_check(df)
        
        # 6. ä¿å­˜
        print("\nğŸ’¾ Saving production-ready data...")
        output_path = preprocessor.save_processed_data(df, args.output_filename)
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆ
        print("\n" + "="*60)
        print("ğŸ“‹ PREPROCESSING RESULTS")
        print("="*60)
        
        stats = quality_report['stats']
        print(f"ğŸ“„ Total records: {stats['total_records']:,}")
        print(f"ğŸ“ˆ Unique stocks: {stats['unique_stocks']}")
        print(f"ğŸ“… Date range: {stats['date_range']['start']} to {stats['date_range']['end']}")
        print(f"ğŸ”§ Features: {len(df.columns)}")
        print(f"ğŸ’¾ Output file: {output_path.name}")
        
        missing_info = stats['missing_values']
        print(f"âŒ Missing values: {missing_info['total']} ({missing_info['rate']:.2%})")
        
        if quality_report['issues']:
            print(f"\nâš ï¸ Issues found:")
            for issue in quality_report['issues']:
                print(f"  - {issue}")
        else:
            print(f"\nâœ… No critical issues found")
        
        print(f"\nQuality Status: {'âœ… PASS' if quality_report['status'] == 'PASS' else 'âš ï¸ WARNING'}")
        print("\nâœ… Production-ready preprocessing completed!")
        
    except Exception as e:
        logger.error(f"Preprocessing failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())