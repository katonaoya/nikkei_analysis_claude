#!/usr/bin/env python3
"""
æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
60%è¶…ãˆã‚’ç›®æŒ‡ã™ç¬¬3æ®µéš: LSTMå®Ÿè£…
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

# TensorFlow/Kerasã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    TENSORFLOW_AVAILABLE = True
    logger.info("âœ… TensorFlowåˆ©ç”¨å¯èƒ½")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    logger.warning("âš ï¸ TensorFlowæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ä»£æ›¿æ‰‹æ³•ã‚’ä½¿ç”¨")

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class StreamlinedDeepLearning:
    """ç°¡ç´ åŒ–æ·±å±¤å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        self.sequence_scaler = MinMaxScaler()
        
        # æœ€é©ç‰¹å¾´é‡
        self.optimal_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20',
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
        
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        df = pd.read_parquet(integrated_file)
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[self.optimal_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(clean_df):,}ä»¶, {len(self.optimal_features)}ç‰¹å¾´é‡")
        
        return X, y, clean_df
    
    def create_sequences(self, X, y, sequence_length=10):
        """æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ"""
        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆï¼ˆé•·ã•={sequence_length}ï¼‰...")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
        X_scaled = self.sequence_scaler.fit_transform(X)
        
        sequences = []
        targets = []
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        for i in range(sequence_length, len(X_scaled)):
            sequences.append(X_scaled[i-sequence_length:i])
            targets.append(y.iloc[i])
        
        X_seq = np.array(sequences)
        y_seq = np.array(targets)
        
        logger.info(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆå®Œäº†: {X_seq.shape}")
        return X_seq, y_seq
    
    def create_lstm_model(self, input_shape):
        """LSTMãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        if not TENSORFLOW_AVAILABLE:
            return None
            
        logger.info("ğŸ§  LSTMãƒ¢ãƒ‡ãƒ«ä½œæˆ...")
        
        model = keras.Sequential([
            layers.LSTM(32, return_sequences=False, input_shape=input_shape),
            layers.Dropout(0.2),
            layers.Dense(16, activation='relu'),
            layers.Dropout(0.1),
            layers.Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        logger.info(f"LSTMãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {model.count_params():,}ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        return model
    
    def evaluate_deep_learning(self, X_seq, y_seq):
        """æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("âš ï¸ TensorFlowãŒãªã„ãŸã‚ã€æ·±å±¤å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return None
            
        logger.info("ğŸš€ æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
        
        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_seq)):
            logger.info(f"  Fold {fold+1}/3...")
            
            X_train, X_test = X_seq[train_idx], X_seq[test_idx]
            y_train, y_test = y_seq[train_idx], y_seq[test_idx]
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            model = self.create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
            
            # æ—©æœŸåœæ­¢
            early_stopping = keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            )
            
            # å­¦ç¿’ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°å‰Šæ¸›ã§é«˜é€ŸåŒ–ï¼‰
            history = model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=20,  # é«˜é€ŸåŒ–ã®ãŸã‚å‰Šæ¸›
                batch_size=256,
                callbacks=[early_stopping],
                verbose=0
            )
            
            # äºˆæ¸¬ã¨è©•ä¾¡
            pred_proba = model.predict(X_test, verbose=0)
            pred = (pred_proba > 0.5).astype(int).flatten()
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            logger.info(f"    Fold {fold+1}: {accuracy:.3%}")
        
        result = {
            'avg': np.mean(scores),
            'std': np.std(scores),
            'scores': scores
        }
        
        logger.info(f"LSTMå¹³å‡ç²¾åº¦: {result['avg']:.3%} Â± {result['std']:.3%}")
        return result
    
    def create_enhanced_features(self, df):
        """ç°¡å˜ãªç‰¹å¾´é‡æ‹¡å¼µ"""
        logger.info("ğŸ”§ ç°¡å˜ãªç‰¹å¾´é‡æ‹¡å¼µ...")
        
        enhanced_df = df.copy()
        enhanced_features = self.optimal_features.copy()
        
        # ä¸»è¦ç‰¹å¾´é‡ã®ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆ1æ—¥ã®ã¿ï¼‰
        key_features = ['sp500_change', 'vix_change', 'Market_Return']
        for feature in key_features:
            if feature in enhanced_df.columns:
                lag_col = f"{feature}_lag1"
                enhanced_df[lag_col] = enhanced_df.groupby('Code')[feature].shift(1).fillna(0)
                enhanced_features.append(lag_col)
        
        # ç§»å‹•å¹³å‡ä¹–é›¢ï¼ˆçŸ­æœŸã®ã¿ï¼‰
        for feature in key_features:
            if feature in enhanced_df.columns:
                ma_col = f"{feature}_ma5"
                diff_col = f"{feature}_diff_ma5"
                enhanced_df[ma_col] = enhanced_df.groupby('Code')[feature].rolling(5, min_periods=1).mean().reset_index(0, drop=True)
                enhanced_df[diff_col] = (enhanced_df[feature] - enhanced_df[ma_col]) / (enhanced_df[ma_col].abs() + 1e-8)
                enhanced_features.append(diff_col)
        
        # æ›œæ—¥åŠ¹æœï¼ˆç°¡å˜ç‰ˆï¼‰
        enhanced_df['Date'] = pd.to_datetime(enhanced_df['Date'])
        enhanced_df['is_monday'] = (enhanced_df['Date'].dt.dayofweek == 0).astype(int)
        enhanced_df['is_friday'] = (enhanced_df['Date'].dt.dayofweek == 4).astype(int)
        enhanced_features.extend(['is_monday', 'is_friday'])
        
        logger.info(f"ç‰¹å¾´é‡æ‹¡å¼µå®Œäº†: {len(enhanced_features)}å€‹")
        return enhanced_df, enhanced_features
    
    def comprehensive_evaluation(self, X, y, enhanced_X):
        """åŒ…æ‹¬çš„è©•ä¾¡"""
        logger.info("ğŸ“Š åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
        
        # æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X)
        enhanced_X_scaled = self.scaler.fit_transform(enhanced_X)
        
        # è©•ä¾¡ç”¨ãƒ¢ãƒ‡ãƒ«
        base_model = LogisticRegression(C=0.001, class_weight='balanced', random_state=42, max_iter=1000)
        enhanced_model = LogisticRegression(C=0.001, class_weight='balanced', random_state=42, max_iter=1000)
        
        # æ™‚ç³»åˆ—åˆ†å‰²è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=5)
        
        results = {}
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
        logger.info("  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡...")
        base_scores = []
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            base_model.fit(X_train, y_train)
            pred = base_model.predict(X_test)
            base_scores.append(accuracy_score(y_test, pred))
        
        results['baseline'] = {
            'avg': np.mean(base_scores),
            'std': np.std(base_scores),
            'scores': base_scores
        }
        
        # æ‹¡å¼µç‰¹å¾´é‡è©•ä¾¡
        logger.info("  æ‹¡å¼µç‰¹å¾´é‡è©•ä¾¡...")
        enhanced_scores = []
        for train_idx, test_idx in tscv.split(enhanced_X_scaled):
            X_train, X_test = enhanced_X_scaled[train_idx], enhanced_X_scaled[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            enhanced_model.fit(X_train, y_train)
            pred = enhanced_model.predict(X_test)
            enhanced_scores.append(accuracy_score(y_test, pred))
        
        results['enhanced'] = {
            'avg': np.mean(enhanced_scores),
            'std': np.std(enhanced_scores),
            'scores': enhanced_scores
        }
        
        # çµæœè¡¨ç¤º
        for name, result in results.items():
            logger.info(f"    {name}: {result['avg']:.3%} Â± {result['std']:.3%}")
        
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ ç°¡ç´ åŒ–æ·±å±¤å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    logger.info("ğŸ¯ ç›®æ¨™: 59.4%ã‹ã‚‰62%è¶…ãˆã‚’ç›®æŒ‡ã™")
    
    dl_system = StreamlinedDeepLearning()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X, y, df = dl_system.load_and_prepare_data()
        
        # 2. æ‹¡å¼µç‰¹å¾´é‡ä½œæˆ
        enhanced_df, enhanced_features = dl_system.create_enhanced_features(df)
        enhanced_X = enhanced_df[enhanced_features].fillna(0)
        
        # 3. å¾“æ¥æ‰‹æ³•ã®è©•ä¾¡
        conventional_results = dl_system.comprehensive_evaluation(X, y, enhanced_X)
        
        # 4. æ·±å±¤å­¦ç¿’è©•ä¾¡ï¼ˆTensorFlowãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        deep_learning_result = None
        if TENSORFLOW_AVAILABLE:
            X_seq, y_seq = dl_system.create_sequences(enhanced_X, y, sequence_length=5)  # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            deep_learning_result = dl_system.evaluate_deep_learning(X_seq, y_seq)
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*100)
        logger.info("ğŸ† ç°¡ç´ åŒ–æ·±å±¤å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ çµæœ")
        logger.info("="*100)
        
        baseline_score = 59.4  # å‰å›ã®æœ€é«˜ã‚¹ã‚³ã‚¢
        logger.info(f"ğŸ“ å‚ç…§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline_score:.1%}")
        
        # å¾“æ¥æ‰‹æ³•çµæœ
        logger.info(f"\nğŸ“Š å¾“æ¥æ‰‹æ³•çµæœ:")
        for name, result in conventional_results.items():
            improvement = (result['avg'] - baseline_score/100) * 100
            status = "ğŸš€" if improvement > 2.0 else "ğŸ“ˆ" if improvement > 0.5 else "ğŸ“Š" if improvement >= 0 else "ğŸ“‰"
            logger.info(f"  {name:15s}: {result['avg']:.3%} ({improvement:+.2f}%) {status}")
        
        # æ·±å±¤å­¦ç¿’çµæœ
        if deep_learning_result:
            dl_improvement = (deep_learning_result['avg'] - baseline_score/100) * 100
            dl_status = "ğŸš€" if dl_improvement > 2.0 else "ğŸ“ˆ" if dl_improvement > 0.5 else "ğŸ“Š" if dl_improvement >= 0 else "ğŸ“‰"
            logger.info(f"\nğŸ§  æ·±å±¤å­¦ç¿’çµæœ:")
            logger.info(f"  LSTM           : {deep_learning_result['avg']:.3%} ({dl_improvement:+.2f}%) {dl_status}")
        
        # æœ€é«˜çµæœç‰¹å®š
        all_results = [('baseline', conventional_results['baseline']), ('enhanced', conventional_results['enhanced'])]
        if deep_learning_result:
            all_results.append(('LSTM', deep_learning_result))
        
        best_name, best_result = max(all_results, key=lambda x: x[1]['avg'])
        final_improvement = (best_result['avg'] - baseline_score/100) * 100
        
        logger.info(f"\nğŸ† æœ€é«˜æ€§èƒ½:")
        logger.info(f"  æ‰‹æ³•: {best_name}")
        logger.info(f"  ç²¾åº¦: {best_result['avg']:.3%} Â± {best_result['std']:.3%}")
        logger.info(f"  å‘ä¸Š: {final_improvement:+.2f}% (59.4% â†’ {best_result['avg']:.1%})")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_60 = 0.60
        target_62 = 0.62
        
        if best_result['avg'] >= target_62:
            logger.info(f"ğŸ‰ ç›®æ¨™å¤§å¹…é”æˆï¼ 62%è¶…ãˆ ({best_result['avg']:.1%} >= 62.0%)")
        elif best_result['avg'] >= target_60:
            logger.info(f"âœ… ç›®æ¨™é”æˆï¼ 60%è¶…ãˆ ({best_result['avg']:.1%} >= 60.0%)")
        else:
            logger.info(f"ğŸ“ˆ æ”¹å–„åŠ¹æœç¢ºèª ({best_result['avg']:.1%})")
        
        logger.info(f"\nğŸ“Š æ‰‹æ³•ã¾ã¨ã‚:")
        logger.info(f"  å¾“æ¥æ‰‹æ³•ã®é™ç•Œ: ç´„59.2-59.4%")
        if deep_learning_result:
            logger.info(f"  æ·±å±¤å­¦ç¿’ã®åŠ¹æœ: {deep_learning_result['avg']:.1%}")
        logger.info(f"  ç‰¹å¾´é‡æ‹¡å¼µã®åŠ¹æœ: é™å®šçš„")
        
        logger.info(f"\nâš–ï¸ ã“ã®çµæœã¯å…¨ãƒ‡ãƒ¼ã‚¿{len(X):,}ä»¶ã§ã®æ¤œè¨¼ã§ã™")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()