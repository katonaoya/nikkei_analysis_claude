#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„ãªå€™è£œçµã‚Šè¾¼ã¿æ‰‹æ³•ã®è©•ä¾¡
90éŠ˜æŸ„ â†’ 5éŠ˜æŸ„ã¸ã®å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class ComprehensiveFilteringEvaluation:
    """åŒ…æ‹¬çš„å€™è£œçµã‚Šè¾¼ã¿æ‰‹æ³•è©•ä¾¡"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        
        # æœ€é©ç‰¹å¾´é‡
        self.optimal_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20',
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
        
        # è©•ä¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.initial_candidates = 90  # åˆæœŸå€™è£œæ•°ï¼ˆæ¦‚ç®—ï¼‰
        self.target_candidates = 5    # æœ€çµ‚å€™è£œæ•°
        self.confidence_threshold = 0.55
        
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info("ğŸ“Š åŒ…æ‹¬çš„è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        df = pd.read_parquet(integrated_file)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # è¿½åŠ æŒ‡æ¨™è¨ˆç®—
        clean_df = self.calculate_additional_metrics(clean_df)
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
        X = clean_df[self.optimal_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(clean_df):,}ä»¶, {len(self.optimal_features)}ç‰¹å¾´é‡")
        
        return clean_df, X, y
    
    def calculate_additional_metrics(self, df):
        """è¿½åŠ æŒ‡æ¨™ã®è¨ˆç®—"""
        logger.info("ğŸ”§ è¿½åŠ æŒ‡æ¨™è¨ˆç®—...")
        
        df = df.sort_values(['Code', 'Date']).copy()
        
        # åŸºæœ¬çš„ãªè¿½åŠ æŒ‡æ¨™
        df['Volume_MA5'] = df.groupby('Code')['Volume'].rolling(5, min_periods=1).mean().reset_index(0, drop=True)
        df['Price_Change_5d'] = df.groupby('Code')['Close'].pct_change(5).fillna(0)
        df['RSI'] = self.calculate_rsi(df)
        
        # ã‚»ã‚¯ã‚¿ãƒ¼æƒ…å ±ï¼ˆä»®æƒ³çš„ã«ç”Ÿæˆï¼‰
        np.random.seed(42)
        unique_codes = df['Code'].unique()
        sectors = ['Technology', 'Finance', 'Healthcare', 'Consumer', 'Industrial', 'Materials', 'Energy', 'Utilities']
        sector_mapping = {code: np.random.choice(sectors) for code in unique_codes}
        df['Sector'] = df['Code'].map(sector_mapping)
        
        # æ™‚ä¾¡ç·é¡ï¼ˆä»®æƒ³çš„ã«è¨ˆç®—ï¼‰
        df['Market_Cap'] = df['Close'] * np.random.uniform(1000000, 100000000, size=len(df))
        
        return df
    
    def calculate_rsi(self, df, period=14):
        """RSIè¨ˆç®—"""
        delta = df.groupby('Code')['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)
    
    def get_base_predictions(self, df, X, y, test_start_date):
        """åŸºæœ¬äºˆæ¸¬ã®å–å¾—"""
        train_mask = df['Date'] < test_start_date
        test_mask = df['Date'] >= test_start_date
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test = X[test_mask]
        
        scaler = StandardScaler()
        model = LogisticRegression(C=0.001, class_weight='balanced', random_state=42, max_iter=1000)
        
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model.fit(X_train_scaled, y_train)
        pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        return pred_proba, test_mask
    
    def filter_method_1_confidence(self, day_data, n_candidates=5):
        """æ‰‹æ³•1: å˜ç´”ç¢ºä¿¡åº¦ä¸Šä½é¸æŠ"""
        if 'pred_proba' not in day_data.columns:
            return []
            
        # é«˜ç¢ºä¿¡åº¦ã®ã¿ï¼ˆä¸Šæ˜‡ãƒ»ä¸‹è½ä¸¡æ–¹å‘ï¼‰
        high_conf_up = day_data[day_data['pred_proba'] >= self.confidence_threshold]
        high_conf_down = day_data[day_data['pred_proba'] <= (1 - self.confidence_threshold)]
        
        # ç¢ºä¿¡åº¦ã®çµ¶å¯¾å€¤ã§è©•ä¾¡
        high_conf_up = high_conf_up.copy()
        high_conf_down = high_conf_down.copy()
        high_conf_up['abs_confidence'] = high_conf_up['pred_proba']
        high_conf_down['abs_confidence'] = 1 - high_conf_down['pred_proba']
        
        all_high_conf = pd.concat([high_conf_up, high_conf_down])
        
        if len(all_high_conf) == 0:
            return []
            
        selected = all_high_conf.nlargest(n_candidates, 'abs_confidence')
        return selected['Code'].tolist()
    
    def filter_method_2_sector_diversity(self, day_data, n_candidates=5):
        """æ‰‹æ³•2: ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æ•£ + ç¢ºä¿¡åº¦"""
        if 'pred_proba' not in day_data.columns or 'Sector' not in day_data.columns:
            return []
        
        high_conf = day_data[
            (day_data['pred_proba'] >= self.confidence_threshold) | 
            (day_data['pred_proba'] <= (1 - self.confidence_threshold))
        ].copy()
        
        if len(high_conf) == 0:
            return []
        
        high_conf['abs_confidence'] = np.maximum(high_conf['pred_proba'], 1 - high_conf['pred_proba'])
        
        # ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ã«æœ€é«˜ç¢ºä¿¡åº¦ã‚’1ã¤ãšã¤é¸æŠ
        selected = []
        sector_groups = high_conf.groupby('Sector')
        
        for sector, group in sector_groups:
            best_in_sector = group.loc[group['abs_confidence'].idxmax()]
            selected.append(best_in_sector)
            
            if len(selected) >= n_candidates:
                break
        
        # ä¸è¶³åˆ†ã¯å…¨ä½“ã‹ã‚‰è¿½åŠ 
        if len(selected) < n_candidates:
            remaining_codes = [s['Code'] for s in selected]
            remaining_data = high_conf[~high_conf['Code'].isin(remaining_codes)]
            additional = remaining_data.nlargest(n_candidates - len(selected), 'abs_confidence')
            selected.extend(additional.to_dict('records'))
        
        return [s['Code'] if isinstance(s, dict) else s.name for s in selected[:n_candidates]]
    
    def filter_method_3_risk_adjusted(self, day_data, n_candidates=5):
        """æ‰‹æ³•3: ãƒªã‚¹ã‚¯èª¿æ•´ (ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ + ç¢ºä¿¡åº¦)"""
        if 'pred_proba' not in day_data.columns or 'Volatility_20' not in day_data.columns:
            return []
        
        high_conf = day_data[
            (day_data['pred_proba'] >= self.confidence_threshold) | 
            (day_data['pred_proba'] <= (1 - self.confidence_threshold))
        ].copy()
        
        if len(high_conf) == 0:
            return []
        
        high_conf['abs_confidence'] = np.maximum(high_conf['pred_proba'], 1 - high_conf['pred_proba'])
        
        # ãƒªã‚¹ã‚¯èª¿æ•´ã‚¹ã‚³ã‚¢ = ç¢ºä¿¡åº¦ / ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        high_conf['risk_adjusted_score'] = high_conf['abs_confidence'] / (high_conf['Volatility_20'] + 0.01)
        
        selected = high_conf.nlargest(n_candidates, 'risk_adjusted_score')
        return selected['Code'].tolist()
    
    def filter_method_4_momentum(self, day_data, n_candidates=5):
        """æ‰‹æ³•4: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  + ç¢ºä¿¡åº¦"""
        if 'pred_proba' not in day_data.columns or 'Price_Change_5d' not in day_data.columns:
            return []
        
        high_conf = day_data[
            (day_data['pred_proba'] >= self.confidence_threshold) | 
            (day_data['pred_proba'] <= (1 - self.confidence_threshold))
        ].copy()
        
        if len(high_conf) == 0:
            return []
        
        high_conf['abs_confidence'] = np.maximum(high_conf['pred_proba'], 1 - high_conf['pred_proba'])
        
        # äºˆæ¸¬æ–¹å‘ã¨ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®ä¸€è‡´åº¦
        high_conf['predicted_up'] = high_conf['pred_proba'] > 0.5
        high_conf['momentum_up'] = high_conf['Price_Change_5d'] > 0
        high_conf['momentum_alignment'] = (high_conf['predicted_up'] == high_conf['momentum_up']).astype(float)
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ èª¿æ•´ã‚¹ã‚³ã‚¢
        high_conf['momentum_score'] = high_conf['abs_confidence'] * (1 + high_conf['momentum_alignment'])
        
        selected = high_conf.nlargest(n_candidates, 'momentum_score')
        return selected['Code'].tolist()
    
    def filter_method_5_liquidity(self, day_data, n_candidates=5):
        """æ‰‹æ³•5: æµå‹•æ€§ + ç¢ºä¿¡åº¦"""
        if 'pred_proba' not in day_data.columns or 'Volume_MA5' not in day_data.columns:
            return []
        
        high_conf = day_data[
            (day_data['pred_proba'] >= self.confidence_threshold) | 
            (day_data['pred_proba'] <= (1 - self.confidence_threshold))
        ].copy()
        
        if len(high_conf) == 0:
            return []
        
        high_conf['abs_confidence'] = np.maximum(high_conf['pred_proba'], 1 - high_conf['pred_proba'])
        
        # æµå‹•æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå‡ºæ¥é«˜ä¸Šä½50%ã®ã¿ï¼‰
        volume_threshold = high_conf['Volume_MA5'].quantile(0.5)
        high_liquidity = high_conf[high_conf['Volume_MA5'] >= volume_threshold]
        
        if len(high_liquidity) < n_candidates:
            high_liquidity = high_conf  # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒå³ã—ã™ãã‚‹å ´åˆã¯å…¨ä½“ã‹ã‚‰é¸æŠ
        
        selected = high_liquidity.nlargest(n_candidates, 'abs_confidence')
        return selected['Code'].tolist()
    
    def filter_method_6_technical(self, day_data, n_candidates=5):
        """æ‰‹æ³•6: ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ + ç¢ºä¿¡åº¦"""
        if 'pred_proba' not in day_data.columns or 'RSI' not in day_data.columns:
            return []
        
        high_conf = day_data[
            (day_data['pred_proba'] >= self.confidence_threshold) | 
            (day_data['pred_proba'] <= (1 - self.confidence_threshold))
        ].copy()
        
        if len(high_conf) == 0:
            return []
        
        high_conf['abs_confidence'] = np.maximum(high_conf['pred_proba'], 1 - high_conf['pred_proba'])
        
        # RSIãƒ™ãƒ¼ã‚¹ã®æŠ€è¡“çš„è©•ä¾¡
        high_conf['predicted_up'] = high_conf['pred_proba'] > 0.5
        high_conf['rsi_signal'] = 0.0
        
        # ä¸Šæ˜‡äºˆæ¸¬ + RSI oversold
        high_conf.loc[(high_conf['predicted_up'] == True) & (high_conf['RSI'] < 30), 'rsi_signal'] = 1.0
        # ä¸‹è½äºˆæ¸¬ + RSI overbought  
        high_conf.loc[(high_conf['predicted_up'] == False) & (high_conf['RSI'] > 70), 'rsi_signal'] = 1.0
        # é©æ­£ç¯„å›²
        high_conf.loc[(high_conf['RSI'] >= 30) & (high_conf['RSI'] <= 70), 'rsi_signal'] = 0.5
        
        high_conf['technical_score'] = high_conf['abs_confidence'] * (1 + high_conf['rsi_signal'])
        
        selected = high_conf.nlargest(n_candidates, 'technical_score')
        return selected['Code'].tolist()
    
    def filter_method_7_hybrid(self, day_data, n_candidates=5):
        """æ‰‹æ³•7: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆè¤‡æ•°è¦ç´ çµ±åˆï¼‰"""
        if 'pred_proba' not in day_data.columns:
            return []
        
        required_cols = ['Volatility_20', 'Volume_MA5', 'Price_Change_5d', 'RSI', 'Sector']
        if not all(col in day_data.columns for col in required_cols):
            return self.filter_method_1_confidence(day_data, n_candidates)
        
        high_conf = day_data[
            (day_data['pred_proba'] >= self.confidence_threshold) | 
            (day_data['pred_proba'] <= (1 - self.confidence_threshold))
        ].copy()
        
        if len(high_conf) == 0:
            return []
        
        # è¤‡æ•°ã‚¹ã‚³ã‚¢ã®çµ±åˆ
        high_conf['abs_confidence'] = np.maximum(high_conf['pred_proba'], 1 - high_conf['pred_proba'])
        
        # æ­£è¦åŒ–
        high_conf['conf_norm'] = (high_conf['abs_confidence'] - high_conf['abs_confidence'].min()) / (high_conf['abs_confidence'].max() - high_conf['abs_confidence'].min() + 1e-8)
        high_conf['vol_norm'] = 1 - (high_conf['Volatility_20'] - high_conf['Volatility_20'].min()) / (high_conf['Volatility_20'].max() - high_conf['Volatility_20'].min() + 1e-8)  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒé«˜ã‚¹ã‚³ã‚¢
        high_conf['volume_norm'] = (high_conf['Volume_MA5'] - high_conf['Volume_MA5'].min()) / (high_conf['Volume_MA5'].max() - high_conf['Volume_MA5'].min() + 1e-8)
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢
        high_conf['hybrid_score'] = (0.5 * high_conf['conf_norm'] + 
                                    0.2 * high_conf['vol_norm'] + 
                                    0.2 * high_conf['volume_norm'] + 
                                    0.1 * (50 - np.abs(high_conf['RSI'] - 50)) / 50)  # RSIä¸­ç«‹ãŒé«˜ã‚¹ã‚³ã‚¢
        
        # ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æ•£ã‚’è€ƒæ…®
        selected_codes = []
        selected_sectors = set()
        
        for _, stock in high_conf.sort_values('hybrid_score', ascending=False).iterrows():
            if len(selected_codes) >= n_candidates:
                break
            
            # åŒã˜ã‚»ã‚¯ã‚¿ãƒ¼ã¯æœ€å¤§2éŠ˜æŸ„ã¾ã§
            sector_count = sum(1 for code in selected_codes if day_data[day_data['Code'] == code]['Sector'].iloc[0] == stock['Sector'])
            
            if sector_count < 2:
                selected_codes.append(stock['Code'])
                selected_sectors.add(stock['Sector'])
        
        # ä¸è¶³åˆ†ã¯åˆ¶ç´„ãªã—ã§è¿½åŠ 
        if len(selected_codes) < n_candidates:
            remaining = high_conf[~high_conf['Code'].isin(selected_codes)]
            additional = remaining.nlargest(n_candidates - len(selected_codes), 'hybrid_score')
            selected_codes.extend(additional['Code'].tolist())
        
        return selected_codes[:n_candidates]
    
    def evaluate_all_methods(self, df, X, y):
        """å…¨æ‰‹æ³•ã®è©•ä¾¡"""
        logger.info("ğŸ§ª å…¨çµã‚Šè¾¼ã¿æ‰‹æ³•ã®åŒ…æ‹¬çš„è©•ä¾¡...")
        
        # è©•ä¾¡æœŸé–“è¨­å®š
        dates = sorted(df['Date'].unique())
        test_start_idx = int(len(dates) * 0.8)
        test_start_date = dates[test_start_idx]
        test_dates = dates[test_start_idx:]
        
        logger.info(f"è©•ä¾¡æœŸé–“: {test_start_date} - {dates[-1]} ({len(test_dates)}æ—¥)")
        
        # åŸºæœ¬äºˆæ¸¬å–å¾—
        pred_proba, test_mask = self.get_base_predictions(df, X, y, test_start_date)
        test_df = df[test_mask].copy()
        test_df['pred_proba'] = pred_proba
        
        # æ‰‹æ³•å®šç¾©
        methods = {
            'Method1_Confidence': self.filter_method_1_confidence,
            'Method2_SectorDiversity': self.filter_method_2_sector_diversity,
            'Method3_RiskAdjusted': self.filter_method_3_risk_adjusted,
            'Method4_Momentum': self.filter_method_4_momentum,
            'Method5_Liquidity': self.filter_method_5_liquidity,
            'Method6_Technical': self.filter_method_6_technical,
            'Method7_Hybrid': self.filter_method_7_hybrid
        }
        
        # å„æ‰‹æ³•ã‚’è©•ä¾¡
        method_results = {}
        
        for method_name, method_func in methods.items():
            logger.info(f"  ğŸ“Š {method_name} è©•ä¾¡ä¸­...")
            
            daily_results = []
            total_predictions = 0
            correct_predictions = 0
            selected_count = 0
            
            for date in test_dates[:100]:  # æœ€åˆã®100æ—¥ã§è©•ä¾¡ï¼ˆè¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰
                day_data = test_df[test_df['Date'] == date]
                if len(day_data) == 0:
                    continue
                
                # æ‰‹æ³•é©ç”¨
                selected_codes = method_func(day_data, self.target_candidates)
                
                if len(selected_codes) == 0:
                    continue
                
                # é¸æŠã•ã‚ŒãŸéŠ˜æŸ„ã®è©•ä¾¡
                selected_data = day_data[day_data['Code'].isin(selected_codes)]
                
                for _, stock in selected_data.iterrows():
                    prediction = stock['pred_proba'] > 0.5
                    actual = stock['Binary_Direction'] == 1
                    
                    total_predictions += 1
                    if prediction == actual:
                        correct_predictions += 1
                
                selected_count += len(selected_codes)
                
                daily_results.append({
                    'date': date,
                    'selected_count': len(selected_codes),
                    'avg_confidence': np.maximum(selected_data['pred_proba'], 1 - selected_data['pred_proba']).mean(),
                    'predictions': len(selected_data),
                })
            
            # çµæœé›†è¨ˆ
            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
            avg_daily_selections = selected_count / len(daily_results) if daily_results else 0
            avg_confidence = np.mean([r['avg_confidence'] for r in daily_results if not np.isnan(r['avg_confidence'])]) if daily_results else 0
            
            method_results[method_name] = {
                'accuracy': accuracy,
                'total_predictions': total_predictions,
                'correct_predictions': correct_predictions,
                'avg_daily_selections': avg_daily_selections,
                'avg_confidence': avg_confidence,
                'evaluation_days': len(daily_results)
            }
        
        return method_results
    
    def display_evaluation_results(self, results):
        """è©•ä¾¡çµæœè¡¨ç¤º"""
        logger.info("\\n" + "="*120)
        logger.info("ğŸ† å€™è£œçµã‚Šè¾¼ã¿æ‰‹æ³•åŒ…æ‹¬è©•ä¾¡çµæœ")
        logger.info("="*120)
        
        logger.info(f"\\nğŸ¯ è©•ä¾¡è¨­å®š:")
        logger.info(f"  åˆæœŸå€™è£œæ•°: ~{self.initial_candidates}éŠ˜æŸ„")
        logger.info(f"  æœ€çµ‚å€™è£œæ•°: {self.target_candidates}éŠ˜æŸ„")
        logger.info(f"  ç¢ºä¿¡åº¦é–¾å€¤: {self.confidence_threshold*100:.0f}%")
        
        # çµæœã‚’ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        
        logger.info(f"\\nğŸ“Š æ‰‹æ³•åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆç²¾åº¦é †ï¼‰:")
        logger.info(f"{'é †ä½':>4s} {'æ‰‹æ³•å':25s} {'ç²¾åº¦':>8s} {'äºˆæ¸¬æ•°':>8s} {'å¹³å‡ç¢ºä¿¡åº¦':>12s} {'æ—¥æ¬¡é¸æŠæ•°':>12s}")
        logger.info("-" * 80)
        
        for i, (method_name, result) in enumerate(sorted_results, 1):
            logger.info(f"{i:4d} {method_name:25s} {result['accuracy']:8.1%} {result['total_predictions']:8,d} {result['avg_confidence']:11.1%} {result['avg_daily_selections']:11.1f}")
        
        # æœ€é«˜æ‰‹æ³•ã®è©³ç´°
        best_method, best_result = sorted_results[0]
        logger.info(f"\\nğŸ¥‡ æœ€é«˜ç²¾åº¦æ‰‹æ³•: {best_method}")
        logger.info(f"  ç²¾åº¦: {best_result['accuracy']:.2%}")
        logger.info(f"  æ­£è§£æ•°: {best_result['correct_predictions']:,}/{best_result['total_predictions']:,}")
        logger.info(f"  å¹³å‡ç¢ºä¿¡åº¦: {best_result['avg_confidence']:.1%}")
        logger.info(f"  è©•ä¾¡æ—¥æ•°: {best_result['evaluation_days']:,}æ—¥")
        
        # æ‰‹æ³•ã®èª¬æ˜
        method_descriptions = {
            'Method1_Confidence': 'å˜ç´”ç¢ºä¿¡åº¦ä¸Šä½é¸æŠ',
            'Method2_SectorDiversity': 'ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æ•£ + ç¢ºä¿¡åº¦',
            'Method3_RiskAdjusted': 'ãƒªã‚¹ã‚¯èª¿æ•´ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰',
            'Method4_Momentum': 'ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  + ç¢ºä¿¡åº¦',
            'Method5_Liquidity': 'æµå‹•æ€§ + ç¢ºä¿¡åº¦',
            'Method6_Technical': 'ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ + ç¢ºä¿¡åº¦',
            'Method7_Hybrid': 'ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆè¤‡æ•°è¦ç´ çµ±åˆï¼‰'
        }
        
        logger.info(f"\\nğŸ“‹ æ‰‹æ³•èª¬æ˜:")
        for method_name, description in method_descriptions.items():
            status = "ğŸ¥‡" if method_name == best_method else "ğŸ“Š"
            logger.info(f"  {status} {method_name}: {description}")
        
        logger.info("="*120)
        
        return best_method, best_result
    
    def implement_best_method(self, best_method_name, df, X, y):
        """æœ€é«˜æ‰‹æ³•ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ"""
        logger.info(f"\\nğŸš€ æœ€é«˜æ‰‹æ³• '{best_method_name}' ã®å®Ÿè£…...")
        
        method_mapping = {
            'Method1_Confidence': self.filter_method_1_confidence,
            'Method2_SectorDiversity': self.filter_method_2_sector_diversity,
            'Method3_RiskAdjusted': self.filter_method_3_risk_adjusted,
            'Method4_Momentum': self.filter_method_4_momentum,
            'Method5_Liquidity': self.filter_method_5_liquidity,
            'Method6_Technical': self.filter_method_6_technical,
            'Method7_Hybrid': self.filter_method_7_hybrid
        }
        
        best_method_func = method_mapping[best_method_name]
        
        # å®Ÿè£…ä¾‹ã®ç”Ÿæˆ
        logger.info(f"âœ… æœ€é«˜ç²¾åº¦æ‰‹æ³• '{best_method_name}' ã®å®Ÿè£…æº–å‚™å®Œäº†")
        logger.info(f"ğŸ’¡ ã“ã®æ‰‹æ³•ã‚’å®Ÿéš›ã®å–å¼•ã‚·ã‚¹ãƒ†ãƒ ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã‚’æ¨å¥¨")
        
        return best_method_func

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸ§ª åŒ…æ‹¬çš„å€™è£œçµã‚Šè¾¼ã¿æ‰‹æ³•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    
    evaluator = ComprehensiveFilteringEvaluation()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df, X, y = evaluator.load_and_prepare_data()
        
        # å…¨æ‰‹æ³•è©•ä¾¡
        results = evaluator.evaluate_all_methods(df, X, y)
        
        # çµæœè¡¨ç¤º
        best_method, best_result = evaluator.display_evaluation_results(results)
        
        # æœ€é«˜æ‰‹æ³•å®Ÿè£…
        evaluator.implement_best_method(best_method, df, X, y)
        
        logger.info("\\nâœ… åŒ…æ‹¬çš„è©•ä¾¡å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()