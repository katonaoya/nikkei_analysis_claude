#!/usr/bin/env python3
"""
é«˜é€Ÿã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ - åŠ¹ç‡çš„ãªç²¾åº¦å‘ä¸Šæ¤œè¨¼
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class QuickEnsembleTester:
    """é«˜é€Ÿã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self, data_dir: Path = None):
        self.data_dir = data_dir or Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
    
    def load_and_prepare(self, filename: str, sample_ratio: float = 0.2) -> tuple:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        file_path = self.processed_dir / filename
        df = pd.read_parquet(file_path)
        logger.info(f"Loaded features: {df.shape}")
        
        # é«˜é€ŸåŒ–ã®ãŸã‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ™‚ç³»åˆ—ã‚’ä¿æŒï¼‰
        if sample_ratio < 1.0:
            unique_dates = sorted(df['Date'].unique())
            sample_dates = unique_dates[::int(1/sample_ratio)]
            df = df[df['Date'].isin(sample_dates)]
            logger.info(f"Sampled to: {df.shape}")
        
        # ç‰¹å¾´é‡æº–å‚™
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Volume',
            'Next_Day_Return', 'Binary_Direction'
        }
        
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[feature_cols].fillna(0)
        y = clean_df['Binary_Direction']
        dates = clean_df['Date']
        
        logger.info(f"Features: {len(feature_cols)}, Samples: {len(X)}")
        return X, y, dates, feature_cols
    
    def quick_model_comparison(self, X, y, dates):
        """é«˜é€Ÿãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        logger.info("ğŸš€ Quick model comparison...")
        
        # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ç¾¤
        models = {
            'Logistic_Enhanced': LogisticRegression(
                C=0.1, class_weight='balanced', random_state=42, max_iter=500
            ),
            'RandomForest_Fast': RandomForestClassifier(
                n_estimators=100, max_depth=10, min_samples_split=30,
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'ExtraTrees_Fast': ExtraTreesClassifier(
                n_estimators=100, max_depth=8, min_samples_split=40,
                class_weight='balanced', random_state=43, n_jobs=-1
            )
        }
        
        tscv = TimeSeriesSplit(n_splits=2)  # é«˜é€ŸåŒ–ã®ãŸã‚2åˆ†å‰²
        results = {}
        
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        
        for model_name, model in models.items():
            logger.info(f"Testing {model_name}...")
            
            fold_accuracies = []
            fold_aucs = []
            predictions_all = []
            
            for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
                X_train = X_scaled.iloc[train_idx] if 'Logistic' in model_name else X.iloc[train_idx]
                X_test = X_scaled.iloc[test_idx] if 'Logistic' in model_name else X.iloc[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                # è¨“ç·´
                model.fit(X_train, y_train)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                fold_accuracies.append(accuracy)
                
                # ç¢ºç‡äºˆæ¸¬
                if hasattr(model, 'predict_proba'):
                    y_proba = model.predict_proba(X_test)[:, 1]
                    auc = roc_auc_score(y_test, y_proba)
                    fold_aucs.append(auc)
                    predictions_all.extend(y_proba)
                else:
                    predictions_all.extend(y_pred)
            
            results[model_name] = {
                'avg_accuracy': np.mean(fold_accuracies),
                'std_accuracy': np.std(fold_accuracies),
                'avg_auc': np.mean(fold_aucs) if fold_aucs else None,
                'predictions': predictions_all
            }
            
            auc_text = f", AUC: {np.mean(fold_aucs):.3f}" if fold_aucs else ""
            logger.info(f"  {model_name}: {np.mean(fold_accuracies):.3f} Â± {np.std(fold_accuracies):.3f}{auc_text}")
        
        return results
    
    def ensemble_combination(self, individual_results, y):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ¯ Testing ensemble combinations...")
        
        # äºˆæ¸¬å€¤ã‚’é…åˆ—åŒ–ï¼ˆé•·ã•ã‚’æƒãˆã‚‹ï¼‰
        model_names = list(individual_results.keys())
        
        # æœ€å°é•·ã‚’å–å¾—
        min_length = min(len(individual_results[name]['predictions']) for name in model_names)
        logger.info(f"Aligning predictions to length: {min_length}")
        
        # yã‚‚åŒã˜é•·ã•ã«èª¿æ•´
        y_aligned = y.iloc[:min_length] if len(y) > min_length else y
        
        predictions_matrix = np.array([
            individual_results[name]['predictions'][:min_length] 
            for name in model_names
        ])
        
        # æ§˜ã€…ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥
        ensemble_strategies = {
            'Simple_Average': np.mean(predictions_matrix, axis=0),
        }
        
        # æ€§èƒ½é‡ã¿ä»˜ã‘
        accuracies = [individual_results[name]['avg_accuracy'] for name in model_names]
        weights = np.array(accuracies) / np.sum(accuracies)
        ensemble_strategies['Weighted_Performance'] = np.average(predictions_matrix, axis=0, weights=weights)
        
        # ä¸Šä½2ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡
        top_2_indices = np.argsort(accuracies)[-2:]
        ensemble_strategies['Best_Two_Average'] = np.mean(predictions_matrix[top_2_indices], axis=0)
        
        # å„æˆ¦ç•¥ã®è©•ä¾¡
        ensemble_results = {}
        
        for strategy_name, ensemble_pred in ensemble_strategies.items():
            if ensemble_pred is not None:
                # äºŒå€¤åŒ–
                binary_pred = (ensemble_pred >= 0.5).astype(int)
                accuracy = accuracy_score(y_aligned, binary_pred)
                
                ensemble_results[strategy_name] = {
                    'accuracy': accuracy,
                    'predictions': ensemble_pred
                }
                
                logger.info(f"  {strategy_name}: {accuracy:.3f}")
        
        return ensemble_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="Quick ensemble test")
    parser.add_argument("--features-file", required=True, help="Features file")
    parser.add_argument("--sample-ratio", type=float, default=0.3, help="Sampling ratio for speed")
    
    args = parser.parse_args()
    
    try:
        tester = QuickEnsembleTester()
        
        print("ğŸ“Š Loading and preparing data...")
        X, y, dates, feature_cols = tester.load_and_prepare(args.features_file, args.sample_ratio)
        
        print("ğŸ¤– Running quick model comparison...")
        individual_results = tester.quick_model_comparison(X, y, dates)
        
        print("ğŸ¯ Testing ensemble combinations...")
        ensemble_results = tester.ensemble_combination(individual_results, y)
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆ
        print("\n" + "="*60)
        print("ğŸ“‹ QUICK ENSEMBLE TEST RESULTS")
        print("="*60)
        
        print("\nğŸ¤– Individual Models:")
        for name, result in individual_results.items():
            auc_text = f", AUC: {result['avg_auc']:.3f}" if result['avg_auc'] else ""
            print(f"   {name:25s}: {result['avg_accuracy']:.3f} Â± {result['std_accuracy']:.3f}{auc_text}")
        
        print("\nğŸ¯ Ensemble Strategies:")
        for name, result in ensemble_results.items():
            print(f"   {name:25s}: {result['accuracy']:.3f}")
        
        # æœ€è‰¯çµæœ
        all_results = {**{k: v['avg_accuracy'] for k, v in individual_results.items()},
                      **{k: v['accuracy'] for k, v in ensemble_results.items()}}
        
        best_model = max(all_results, key=all_results.get)
        best_accuracy = all_results[best_model]
        
        print(f"\nğŸ† Best Performance:")
        print(f"   Model: {best_model}")
        print(f"   Accuracy: {best_accuracy:.3f} ({best_accuracy:.1%})")
        
        # ç›®æ¨™é”æˆåˆ¤å®š
        if best_accuracy >= 0.53:
            print("\nğŸ‰ SUCCESS: Target accuracy 53%+ achieved!")
        elif best_accuracy >= 0.52:
            print("\nğŸ‘ GOOD: Significant improvement, close to target!")
        elif best_accuracy >= 0.515:
            print("\nğŸ“ˆ PROGRESS: Noticeable improvement achieved")
        else:
            print("\nğŸ’¡ MODERATE: Some improvement, need additional techniques")
        
        baseline = 0.505  # å…ƒã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        improvement = best_accuracy - baseline
        print(f"\nğŸ“ˆ Improvement over baseline: +{improvement:.3f} (+{improvement*100:.1f}%)")
        
        print("\nâœ… Quick ensemble test completed!")
        
        return 0 if best_accuracy >= 0.52 else 1
        
    except Exception as e:
        logger.error(f"Ensemble test failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())