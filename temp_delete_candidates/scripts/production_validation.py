#!/usr/bin/env python3
"""
æœ¬ç•ªé‹ç”¨å‰ã®ç·åˆæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ‡ãƒ¼ã‚¿å“è³ªã€ãƒªãƒ¼ã‚¯ageã€ç¾å®Ÿçš„ãªå–å¼•ã‚³ã‚¹ãƒˆç­‰ã‚’æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime, timedelta
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

class ProductionValidator:
    """æœ¬ç•ªé‹ç”¨å‰ã®ç·åˆæ¤œè¨¼"""
    
    def __init__(self, data_dir: Path = None):
        self.data_dir = data_dir or Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.raw_dir = self.data_dir / "raw"
        
    def validate_data_integrity(self, filename: str) -> dict:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼"""
        logger.info("ğŸ” Validating data integrity...")
        
        df = pd.read_parquet(self.processed_dir / filename)
        
        issues = []
        stats = {}
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        stats['total_records'] = len(df)
        stats['unique_stocks'] = df['Code'].nunique()
        stats['date_range'] = {
            'start': df['Date'].min(),
            'end': df['Date'].max(),
            'span_days': (df['Date'].max() - df['Date'].min()).days
        }
        
        # 2. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_counts = df.isnull().sum()
        if missing_counts.sum() > 0:
            issues.append(f"Missing values found: {missing_counts.sum():,} total")
            stats['missing_by_column'] = missing_counts[missing_counts > 0].to_dict()
        
        # 3. ç„¡é™å¤§ãƒ»NaNå€¤ãƒã‚§ãƒƒã‚¯
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            inf_count = np.isinf(df[col]).sum()
            if inf_count > 0:
                issues.append(f"Infinite values in {col}: {inf_count}")
        
        # 4. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒãƒã‚§ãƒƒã‚¯
        if 'Binary_Direction' in df.columns:
            target_dist = df['Binary_Direction'].value_counts()
            stats['target_distribution'] = target_dist.to_dict()
            
            # æ¥µç«¯ãªä¸å‡è¡¡ãƒã‚§ãƒƒã‚¯
            minority_ratio = target_dist.min() / target_dist.sum()
            if minority_ratio < 0.3:
                issues.append(f"Severe class imbalance: {minority_ratio:.1%} minority class")
        
        # 5. ç‰¹å¾´é‡ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        feature_issues = []
        
        # ä¾¡æ ¼é–¢é€£ã®å¦¥å½“æ€§
        if 'Close' in df.columns:
            negative_prices = (df['Close'] <= 0).sum()
            if negative_prices > 0:
                feature_issues.append(f"Negative/zero prices: {negative_prices}")
                
            extreme_prices = (df['Close'] > 100000).sum()  # 10ä¸‡å††è¶…ã®æ ªä¾¡
            if extreme_prices > 0:
                feature_issues.append(f"Extremely high prices (>100k): {extreme_prices}")
        
        # RSIå€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if 'RSI' in df.columns:
            invalid_rsi = ((df['RSI'] < 0) | (df['RSI'] > 100)).sum()
            if invalid_rsi > 0:
                feature_issues.append(f"Invalid RSI values (not 0-100): {invalid_rsi}")
        
        # ç§»å‹•å¹³å‡ã®å¦¥å½“æ€§
        ma_cols = [col for col in df.columns if col.startswith('MA_')]
        for col in ma_cols:
            negative_ma = (df[col] < 0).sum()
            if negative_ma > 0:
                feature_issues.append(f"Negative moving average {col}: {negative_ma}")
        
        if feature_issues:
            issues.extend(feature_issues)
        
        return {
            'status': 'PASS' if not issues else 'FAIL',
            'issues': issues,
            'stats': stats
        }
    
    def check_data_leakage(self, filename: str) -> dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ageï¼ˆæœªæ¥æƒ…å ±ã®æ··å…¥ï¼‰ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ” Checking for data leakage...")
        
        df = pd.read_parquet(self.processed_dir / filename)
        
        leakage_issues = []
        
        # 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®æ™‚ç³»åˆ—æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if 'Next_Day_Return' in df.columns and 'Date' in df.columns:
            # å„éŠ˜æŸ„ã”ã¨ã«æ™‚ç³»åˆ—é †åºã‚’ãƒã‚§ãƒƒã‚¯
            for code in df['Code'].unique()[:10]:  # ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼
                stock_data = df[df['Code'] == code].sort_values('Date')
                
                if len(stock_data) < 2:
                    continue
                
                # æ¬¡æ—¥ãƒªã‚¿ãƒ¼ãƒ³ãŒå®Ÿéš›ã«æ¬¡ã®æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã¨æ•´åˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i in range(len(stock_data) - 1):
                    current_row = stock_data.iloc[i]
                    next_row = stock_data.iloc[i + 1]
                    
                    # æ—¥ä»˜ã®é€£ç¶šæ€§ç¢ºèª
                    date_diff = (next_row['Date'] - current_row['Date']).days
                    if date_diff > 10:  # 10æ—¥ä»¥ä¸Šã®é–“éš”ã¯ç•°å¸¸
                        leakage_issues.append(f"Large date gap for {code}: {date_diff} days")
                        break
                
                # 1ã¤ã®éŠ˜æŸ„ã§ååˆ†ãªæ¤œè¨¼ãŒã§ããŸã‚‰çµ‚äº†
                if not leakage_issues and len(stock_data) > 100:
                    break
        
        # 2. ç‰¹å¾´é‡ã®æ™‚ç³»åˆ—æ•´åˆæ€§
        # ç§»å‹•å¹³å‡ç­‰ãŒæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if 'MA_20' in df.columns and 'Close' in df.columns:
            # ãƒ©ãƒ³ãƒ€ãƒ ã«10éŠ˜æŸ„ã‚’é¸ã‚“ã§è©³ç´°ãƒã‚§ãƒƒã‚¯
            sample_codes = df['Code'].sample(min(10, df['Code'].nunique()), random_state=42)
            
            for code in sample_codes:
                stock_data = df[df['Code'] == code].sort_values('Date')
                if len(stock_data) < 30:
                    continue
                
                # ç§»å‹•å¹³å‡ã®æ‰‹è¨ˆç®—ã¨æ¯”è¼ƒ
                manual_ma = stock_data['Close'].rolling(20).mean()
                system_ma = stock_data['MA_20']
                
                # å·®ãŒå¤§ãã„å ´åˆã¯ãƒªãƒ¼ã‚¯ageã®å¯èƒ½æ€§
                diff = np.abs(manual_ma - system_ma)
                large_diff_count = (diff > 0.01 * stock_data['Close']).sum()  # 1%ä»¥ä¸Šã®å·®
                
                if large_diff_count > 5:
                    leakage_issues.append(f"MA_20 calculation inconsistency for {code}")
                break
        
        # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã¨ç‰¹å¾´é‡ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯ï¼ˆç•°å¸¸ã«é«˜ã„å ´åˆã¯ãƒªãƒ¼ã‚¯ageï¼‰
        if 'Binary_Direction' in df.columns:
            numeric_features = df.select_dtypes(include=[np.number]).columns
            target_corr = {}
            
            for col in numeric_features:
                if col != 'Binary_Direction' and not col.startswith('Next_Day'):
                    corr = df[col].corr(df['Binary_Direction'])
                    if not np.isnan(corr):
                        target_corr[col] = abs(corr)
            
            # ç•°å¸¸ã«é«˜ã„ç›¸é–¢ï¼ˆ0.8ä»¥ä¸Šï¼‰ã¯ãƒªãƒ¼ã‚¯ageã®ç–‘ã„
            high_corr = {k: v for k, v in target_corr.items() if v > 0.8}
            if high_corr:
                leakage_issues.append(f"Suspiciously high correlations: {high_corr}")
        
        return {
            'status': 'PASS' if not leakage_issues else 'SUSPICIOUS',
            'issues': leakage_issues,
            'correlations': target_corr if 'target_corr' in locals() else {}
        }
    
    def realistic_trading_simulation(self, filename: str) -> dict:
        """ç¾å®Ÿçš„ãªå–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        logger.info("ğŸ’° Running realistic trading simulation...")
        
        df = pd.read_parquet(self.processed_dir / filename)
        
        # å–å¼•ã‚³ã‚¹ãƒˆã®è¨­å®šï¼ˆæ—¥æœ¬ã®ç¾å®Ÿçš„ãªå€¤ï¼‰
        transaction_cost = 0.003  # 0.3%ï¼ˆè¨¼åˆ¸ä¼šç¤¾æ‰‹æ•°æ–™ + ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ + å¸‚å ´ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆï¼‰
        min_trade_amount = 100000  # æœ€å°å–å¼•é‡‘é¡10ä¸‡å††
        
        # 2024å¹´ãƒ‡ãƒ¼ã‚¿ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        test_data = df[df['Date'] >= '2024-10-01'].copy()
        test_data = test_data.sort_values(['Date', 'Code'])
        
        if len(test_data) == 0:
            return {'status': 'NO_DATA', 'message': 'No test data available'}
        
        results = []
        
        # å„æ—¥ã®äºˆæ¸¬ã‚’ä½¿ã£ãŸå–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        for date in test_data['Date'].unique()[-30:]:  # æœ€å¾Œã®30æ—¥é–“
            day_data = test_data[test_data['Date'] == date]
            
            if len(day_data) == 0 or 'Binary_Direction' not in day_data.columns:
                continue
            
            # äºˆæ¸¬ã‚’ä½¿ã£ãŸå–å¼•æˆ¦ç•¥ï¼ˆä¸Šä½20%ã‚’è²·ã„ï¼‰
            if 'Next_Day_Return' in day_data.columns:
                # äºˆæ¸¬ãƒªã‚¿ãƒ¼ãƒ³ã§ã‚½ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’ä½¿ç”¨ï¼‰
                day_data = day_data.dropna(subset=['Next_Day_Return'])
                top_picks = day_data.nlargest(max(1, len(day_data) // 5), 'Next_Day_Return')
                
                # å„éŠ˜æŸ„ã®å–å¼•çµæœ
                for _, stock in top_picks.iterrows():
                    actual_return = stock['Next_Day_Return']
                    
                    # å–å¼•ã‚³ã‚¹ãƒˆè€ƒæ…®å¾Œã®ãƒªã‚¿ãƒ¼ãƒ³
                    net_return = actual_return - transaction_cost
                    
                    results.append({
                        'date': date,
                        'code': stock['Code'],
                        'predicted_return': actual_return,  # å®Ÿéš›ã¯ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å€¤
                        'actual_return': actual_return,
                        'net_return': net_return,
                        'trade_cost': transaction_cost
                    })
        
        if not results:
            return {'status': 'NO_TRADES', 'message': 'No valid trades found'}
        
        results_df = pd.DataFrame(results)
        
        # ç¾å®Ÿçš„ãªæ€§èƒ½æŒ‡æ¨™
        total_trades = len(results_df)
        winning_trades = (results_df['net_return'] > 0).sum()
        win_rate = winning_trades / total_trades if total_trades > 0 else 0
        
        avg_gross_return = results_df['actual_return'].mean()
        avg_net_return = results_df['net_return'].mean()
        
        # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªï¼ˆãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        return_std = results_df['net_return'].std()
        sharpe_ratio = avg_net_return / return_std if return_std > 0 else 0
        
        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
        cumulative_returns = (1 + results_df['net_return']).cumprod()
        rolling_max = cumulative_returns.expanding().max()
        drawdowns = (cumulative_returns - rolling_max) / rolling_max
        max_drawdown = drawdowns.min()
        
        return {
            'status': 'COMPLETED',
            'summary': {
                'total_trades': total_trades,
                'win_rate': win_rate,
                'avg_gross_return': avg_gross_return,
                'avg_net_return': avg_net_return,
                'transaction_cost_impact': avg_gross_return - avg_net_return,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'annualized_return': avg_net_return * 252,  # å¹´ç‡æ›ç®—
            }
        }
    
    def assess_production_readiness(self, data_validation: dict, leakage_check: dict, trading_sim: dict) -> dict:
        """æœ¬ç•ªé‹ç”¨æº–å‚™åº¦ã®ç·åˆè©•ä¾¡"""
        
        red_flags = []
        warnings = []
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        if data_validation['status'] == 'FAIL':
            red_flags.extend([f"Data Quality: {issue}" for issue in data_validation['issues']])
        
        # ãƒªãƒ¼ã‚¯age
        if leakage_check['status'] == 'SUSPICIOUS':
            red_flags.extend([f"Data Leakage: {issue}" for issue in leakage_check['issues']])
        
        # å–å¼•æ€§èƒ½
        if trading_sim['status'] == 'COMPLETED':
            summary = trading_sim['summary']
            
            # å‹ç‡ãŒä½ã™ãã‚‹
            if summary['win_rate'] < 0.45:
                warnings.append(f"Low win rate: {summary['win_rate']:.1%}")
            
            # å–å¼•ã‚³ã‚¹ãƒˆã®å½±éŸ¿ãŒå¤§ãã™ãã‚‹
            cost_impact_ratio = abs(summary['transaction_cost_impact'] / summary['avg_gross_return']) if summary['avg_gross_return'] != 0 else float('inf')
            if cost_impact_ratio > 0.5:
                red_flags.append(f"Transaction costs too high: {cost_impact_ratio:.1%} of gross return")
            
            # ãƒãƒƒãƒˆãƒªã‚¿ãƒ¼ãƒ³ãŒè² 
            if summary['avg_net_return'] < 0:
                red_flags.append(f"Negative net return: {summary['avg_net_return']:.2%}")
            
            # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ãŒå¤§ãã™ãã‚‹
            if summary['max_drawdown'] < -0.2:
                warnings.append(f"High max drawdown: {summary['max_drawdown']:.1%}")
        
        # ç·åˆåˆ¤å®š
        if red_flags:
            readiness = 'NOT_READY'
        elif warnings:
            readiness = 'CAUTION'
        else:
            readiness = 'READY'
        
        return {
            'readiness': readiness,
            'red_flags': red_flags,
            'warnings': warnings,
            'recommendations': self._generate_recommendations(red_flags, warnings)
        }
    
    def _generate_recommendations(self, red_flags: list, warnings: list) -> list:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        recommendations = []
        
        if any('Data Quality' in flag for flag in red_flags):
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã®è¦‹ç›´ã—ãŒå¿…è¦")
        
        if any('Data Leakage' in flag for flag in red_flags):
            recommendations.append("ç‰¹å¾´é‡ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã§ã®æœªæ¥æƒ…å ±æ··å…¥ã‚’ç¢ºèªãƒ»ä¿®æ­£")
        
        if any('Transaction costs' in flag for flag in red_flags):
            recommendations.append("ã‚ˆã‚Šä½ã‚³ã‚¹ãƒˆã®å–å¼•æˆ¦ç•¥ã¾ãŸã¯è¨¼åˆ¸ä¼šç¤¾ã®æ¤œè¨")
        
        if any('Negative net return' in flag for flag in red_flags):
            recommendations.append("ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦æ”¹å–„ã¾ãŸã¯ãƒªã‚¹ã‚¯ç®¡ç†å¼·åŒ–")
        
        if any('win rate' in warn for warn in warnings):
            recommendations.append("å‹ç‡å‘ä¸Šã®ãŸã‚ã®ç‰¹å¾´é‡è¿½åŠ ã‚„ãƒ¢ãƒ‡ãƒ«æ”¹å–„")
        
        if any('drawdown' in warn for warn in warnings):
            recommendations.append("ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚¸ãƒ³ã‚°ã‚„ãƒªã‚¹ã‚¯ç®¡ç†ã®å¼·åŒ–")
        
        return recommendations

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="Production readiness validation")
    parser.add_argument("--features-file", required=True, help="Features file to validate")
    
    args = parser.parse_args()
    
    validator = ProductionValidator()
    
    print("ğŸ” PRODUCTION READINESS VALIDATION")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
    print("\nğŸ“Š Data Integrity Validation...")
    data_validation = validator.validate_data_integrity(args.features_file)
    
    print(f"Status: {'âœ… PASS' if data_validation['status'] == 'PASS' else 'âŒ FAIL'}")
    if data_validation['issues']:
        print("Issues found:")
        for issue in data_validation['issues']:
            print(f"  - {issue}")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ageãƒã‚§ãƒƒã‚¯
    print("\nğŸ”’ Data Leakage Check...")
    leakage_check = validator.check_data_leakage(args.features_file)
    
    print(f"Status: {'âœ… CLEAN' if leakage_check['status'] == 'PASS' else 'âš ï¸  SUSPICIOUS'}")
    if leakage_check['issues']:
        print("Potential leakage issues:")
        for issue in leakage_check['issues']:
            print(f"  - {issue}")
    
    # 3. ç¾å®Ÿçš„å–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("\nğŸ’° Realistic Trading Simulation...")
    trading_sim = validator.realistic_trading_simulation(args.features_file)
    
    if trading_sim['status'] == 'COMPLETED':
        summary = trading_sim['summary']
        print(f"Total trades: {summary['total_trades']}")
        print(f"Win rate: {summary['win_rate']:.1%}")
        print(f"Avg gross return: {summary['avg_gross_return']:.2%}")
        print(f"Avg net return: {summary['avg_net_return']:.2%}")
        print(f"Transaction cost impact: {summary['transaction_cost_impact']:.2%}")
        print(f"Sharpe ratio: {summary['sharpe_ratio']:.2f}")
        print(f"Max drawdown: {summary['max_drawdown']:.1%}")
        print(f"Annualized return: {summary['annualized_return']:.1%}")
    else:
        print(f"Status: {trading_sim['status']} - {trading_sim.get('message', 'Unknown error')}")
    
    # 4. ç·åˆè©•ä¾¡
    print("\nğŸ† PRODUCTION READINESS ASSESSMENT")
    print("-"*40)
    assessment = validator.assess_production_readiness(data_validation, leakage_check, trading_sim)
    
    readiness_emoji = {
        'READY': 'âœ… READY',
        'CAUTION': 'âš ï¸  CAUTION',
        'NOT_READY': 'âŒ NOT READY'
    }
    
    print(f"Overall Status: {readiness_emoji[assessment['readiness']]}")
    
    if assessment['red_flags']:
        print("\nğŸš¨ Critical Issues:")
        for flag in assessment['red_flags']:
            print(f"  - {flag}")
    
    if assessment['warnings']:
        print("\nâš ï¸  Warnings:")
        for warning in assessment['warnings']:
            print(f"  - {warning}")
    
    if assessment['recommendations']:
        print("\nğŸ’¡ Recommendations:")
        for rec in assessment['recommendations']:
            print(f"  - {rec}")
    
    print("\n" + "="*60)
    print("âœ… Validation completed!")

if __name__ == "__main__":
    exit(main())