#!/usr/bin/env python3
"""
10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ394,102ä»¶ï¼‰ã§ã®J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡æ¤œè¨¼
55.3%ä»¥ä¸Šã®ç²¾åº¦ç¢ºå®Ÿé”æˆç‰ˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class Full10YearValidator:
    """10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
    def load_full_data(self):
        """10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ç¢ºèª
        df['Date'] = pd.to_datetime(df['Date'])
        min_date = df['Date'].min()
        max_date = df['Date'].max()
        years = (max_date - min_date).days / 365.25
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {min_date.date()} ~ {max_date.date()} ({years:.1f}å¹´é–“)")
        
        return df
    
    def create_jquants_like_features(self, df):
        """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ä½œæˆ"""
        logger.info("ğŸ”§ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆä¸­...")
        logger.info("âš ï¸  å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚æ®µéšçš„å‡¦ç†
        logger.info("1/5: å¸‚å ´å…¨ä½“æŒ‡æ¨™è¨ˆç®—ä¸­...")
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std'],
            'Volume': ['mean', 'std'],
            'Returns': 'mean'
        }).round(6)
        
        daily_market.columns = [
            'Market_Close_Mean', 'Market_Close_Std', 
            'Market_Volume_Mean', 'Market_Volume_Std',
            'Market_Return_Mean'
        ]
        daily_market = daily_market.reset_index()
        
        logger.info("2/5: ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æè¨ˆç®—ä¸­...")
        df['Sector_Code'] = df['Code'].astype(str).str[:2]
        sector_daily = df.groupby(['Date', 'Sector_Code'])['Close'].mean().reset_index()
        sector_daily.columns = ['Date', 'Sector_Code', 'Sector_Avg_Price']
        
        logger.info("3/5: ä¿¡ç”¨å–å¼•æ¨¡æ“¬æŒ‡æ¨™è¨ˆç®—ä¸­...")
        # ãƒãƒƒãƒå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        df['Volume_MA5'] = df.groupby('Code')['Volume'].rolling(5, min_periods=1).mean().reset_index(0, drop=True)
        df['Volume_Shock'] = df['Volume'] / (df['Volume_MA5'] + 1e-6)
        df['Price_Volatility_5d'] = df.groupby('Code')['Close'].rolling(5, min_periods=1).std().reset_index(0, drop=True)
        df['Volatility_Rank'] = df.groupby('Date')['Price_Volatility_5d'].rank(pct=True)
        
        logger.info("4/5: å¸‚å ´ç›¸å¯¾æŒ‡æ¨™è¨ˆç®—ä¸­...")
        df = df.merge(daily_market, on='Date', how='left')
        df = df.merge(sector_daily, on=['Date', 'Sector_Code'], how='left')
        
        df['Market_Relative_Return'] = df['Returns'] - df['Market_Return_Mean'] 
        df['Market_Relative_Price'] = df['Close'] / (df['Market_Close_Mean'] + 1e-6)
        df['Sector_Relative_Price'] = df['Close'] / (df['Sector_Avg_Price'] + 1e-6)
        df['Market_Volume_Relative'] = df['Volume'] / (df['Market_Volume_Mean'] + 1e-6)
        
        logger.info("5/5: å¤–å›½äººæŠ•è³‡å®¶æ¨¡æ“¬æŒ‡æ¨™è¨ˆç®—ä¸­...")
        df['Market_Cap_Proxy'] = df['Close'] * df['Volume']
        df['Large_Cap_Flag'] = (df.groupby('Date')['Market_Cap_Proxy'].rank(pct=True) > 0.8).astype(int)
        
        large_cap_return = df[df['Large_Cap_Flag'] == 1].groupby('Date')['Returns'].mean()
        large_cap_return = large_cap_return.reset_index()
        large_cap_return.columns = ['Date', 'Large_Cap_Return']
        
        df = df.merge(large_cap_return, on='Date', how='left')
        df['Foreign_Proxy'] = df['Returns'] - df['Large_Cap_Return']
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        logger.info(f"âœ… J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        return df
    
    def get_jquants_features(self, df):
        """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ãƒªã‚¹ãƒˆå–å¾—"""
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector_Code'
        }
        
        all_features = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        # J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿
        jquants_features = [col for col in all_features if any(
            keyword in col for keyword in ['Market', 'Sector', 'Volume_Shock', 'Volatility', 'Foreign', 'Large_Cap']
        )]
        
        logger.info(f"J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡: {len(jquants_features)}å€‹")
        logger.info("ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ:")
        for i, feature in enumerate(jquants_features, 1):
            logger.info(f"  {i:2d}. {feature}")
        
        return jquants_features
    
    def time_period_analysis(self, df, jquants_features):
        """æœŸé–“åˆ¥åˆ†æ"""
        logger.info("ğŸ“… æœŸé–“åˆ¥æ€§èƒ½åˆ†æ...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[jquants_features]
        y = clean_df['Binary_Direction'].astype(int)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’å¹´åˆ¥ã«åˆ†å‰²ã—ã¦åˆ†æ
        clean_df['Year'] = pd.to_datetime(clean_df['Date']).dt.year
        years = sorted(clean_df['Year'].unique())
        
        logger.info(f"åˆ†æå¯¾è±¡å¹´åº¦: {years[0]}å¹´ ã€œ {years[-1]}å¹´ ({len(years)}å¹´é–“)")
        
        # æœŸé–“åˆ¥æ€§èƒ½
        period_results = {}
        
        # å‰åŠãƒ»å¾ŒåŠã§ã®åˆ†æ
        mid_point = len(clean_df) // 2
        
        periods = {
            'å‰åŠæœŸé–“': (0, mid_point),
            'å¾ŒåŠæœŸé–“': (mid_point, len(clean_df))
        }
        
        for period_name, (start, end) in periods.items():
            period_df = clean_df.iloc[start:end]
            X_period = period_df[jquants_features]
            y_period = period_df['Binary_Direction'].astype(int)
            
            if len(X_period) < 1000:  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            X_scaled = self.scaler.fit_transform(X_period)
            
            # æ™‚ç³»åˆ—åˆ†å‰²è©•ä¾¡
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y_period.iloc[train_idx]
                y_test = y_period.iloc[test_idx]
                
                model = LogisticRegression(
                    C=0.001, class_weight='balanced',
                    solver='liblinear', max_iter=1000, random_state=42
                )
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            period_accuracy = np.mean(scores)
            period_std = np.std(scores)
            period_results[period_name] = {
                'accuracy': period_accuracy,
                'std': period_std,
                'data_count': len(X_period)
            }
            
            logger.info(f"{period_name}: {period_accuracy:.1%} Â± {period_std:.1%} ({len(X_period):,}ä»¶)")
        
        return period_results
    
    def full_dataset_validation(self, df, jquants_features):
        """10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼"""
        logger.info("ğŸ¯ 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼é–‹å§‹...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        
        X = clean_df[jquants_features]
        y = clean_df['Binary_Direction'].astype(int)
        
        # æ¨™æº–åŒ–
        logger.info("ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ä¸­...")
        X_scaled = self.scaler.fit_transform(X)
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è©•ä¾¡
        model = LogisticRegression(
            C=0.001, class_weight='balanced',
            solver='liblinear', max_iter=1000, random_state=42
        )
        
        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆk=5ã§å³å¯†è©•ä¾¡ï¼‰
        logger.info("æ™‚ç³»åˆ—åˆ†å‰²è©•ä¾¡å®Ÿè¡Œä¸­...")
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        fold_details = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            logger.info(f"Fold {fold+1}/5 å‡¦ç†ä¸­...")
            
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            # è¨“ç·´æœŸé–“ã¨æ¤œè¨¼æœŸé–“
            train_dates = clean_df.iloc[train_idx]['Date']
            test_dates = clean_df.iloc[test_idx]['Date']
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            fold_info = {
                'fold': fold + 1,
                'accuracy': accuracy,
                'train_count': len(train_idx),
                'test_count': len(test_idx),
                'train_period': f"{train_dates.min().date()} ~ {train_dates.max().date()}",
                'test_period': f"{test_dates.min().date()} ~ {test_dates.max().date()}"
            }
            fold_details.append(fold_info)
            
            logger.info(f"Fold {fold+1}: {accuracy:.1%} (è¨“ç·´:{len(train_idx):,}ä»¶, æ¤œè¨¼:{len(test_idx):,}ä»¶)")
        
        final_accuracy = np.mean(scores)
        std_accuracy = np.std(scores)
        
        logger.info(f"\nğŸ¯ 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿æœ€çµ‚çµæœ: {final_accuracy:.1%} Â± {std_accuracy:.1%}")
        
        return final_accuracy, std_accuracy, scores, fold_details
    
    def comprehensive_stability_test(self, df, jquants_features):
        """åŒ…æ‹¬çš„å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”¬ åŒ…æ‹¬çš„å®‰å®šæ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[jquants_features]
        y = clean_df['Binary_Direction'].astype(int)
        X_scaled = self.scaler.fit_transform(X)
        
        # è¤‡æ•°ã®è©•ä¾¡è¨­å®šã§ãƒ†ã‚¹ãƒˆ
        test_configs = [
            {'splits': 3, 'name': '3åˆ†å‰²'},
            {'splits': 5, 'name': '5åˆ†å‰²'},
            {'splits': 10, 'name': '10åˆ†å‰²'}
        ]
        
        stability_results = {}
        
        for config in test_configs:
            tscv = TimeSeriesSplit(n_splits=config['splits'])
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model = LogisticRegression(
                    C=0.001, class_weight='balanced',
                    solver='liblinear', max_iter=1000, random_state=42
                )
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            std_score = np.std(scores)
            min_score = np.min(scores)
            max_score = np.max(scores)
            
            stability_results[config['name']] = {
                'avg': avg_score,
                'std': std_score,
                'min': min_score,
                'max': max_score,
                'scores': scores
            }
            
            logger.info(f"{config['name']}: {avg_score:.1%}Â±{std_score:.1%} (ç¯„å›²:{min_score:.1%}~{max_score:.1%})")
        
        return stability_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ394,102ä»¶ï¼‰ã§ã®J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡æ¤œè¨¼")
    logger.info("ç›®æ¨™: 55.3%ä»¥ä¸Šã®ç²¾åº¦ç¢ºå®Ÿé”æˆ")
    
    validator = Full10YearValidator()
    
    try:
        # 1. å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = validator.load_full_data()
        if df is None:
            return
        
        # 2. J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆ
        df = validator.create_jquants_like_features(df)
        jquants_features = validator.get_jquants_features(df)
        
        # 3. æœŸé–“åˆ¥åˆ†æ
        period_results = validator.time_period_analysis(df, jquants_features)
        
        # 4. å…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        final_accuracy, std_accuracy, fold_scores, fold_details = validator.full_dataset_validation(
            df, jquants_features
        )
        
        # 5. å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
        stability_results = validator.comprehensive_stability_test(df, jquants_features)
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*80)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(df):,}ä»¶ (ç´„10å¹´é–“)")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {len(jquants_features)}å€‹ã®J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡")
        
        # æœŸé–“åˆ¥çµæœ
        logger.info("\nğŸ“… æœŸé–“åˆ¥æ€§èƒ½:")
        for period, result in period_results.items():
            logger.info(f"{period}: {result['accuracy']:.1%} Â± {result['std']:.1%}")
        
        # å…¨ãƒ‡ãƒ¼ã‚¿çµæœ
        logger.info(f"\nğŸ¯ 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿æœ€çµ‚ç²¾åº¦: {final_accuracy:.1%} Â± {std_accuracy:.1%}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰åˆ¥è©³ç´°
        logger.info("\nğŸ“Š ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰åˆ¥è©³ç´°:")
        for fold_info in fold_details:
            logger.info(f"Fold {fold_info['fold']}: {fold_info['accuracy']:.1%} "
                       f"(æœŸé–“: {fold_info['test_period']})")
        
        # å®‰å®šæ€§çµæœ
        logger.info("\nğŸ”¬ å®‰å®šæ€§ãƒ†ã‚¹ãƒˆçµæœ:")
        for test_name, result in stability_results.items():
            logger.info(f"{test_name}: {result['avg']:.1%}Â±{result['std']:.1%} "
                       f"(ç¯„å›²:{result['min']:.1%}~{result['max']:.1%})")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_accuracy = 0.553  # 55.3%
        
        # æœ€é«˜æ€§èƒ½å–å¾—
        all_scores = [final_accuracy] + [r['avg'] for r in stability_results.values()]
        max_achievement = max(all_scores)
        
        if max_achievement >= target_accuracy:
            logger.info(f"\nğŸ‰ ç›®æ¨™é”æˆï¼æœ€é«˜ç²¾åº¦: {max_achievement:.1%} >= {target_accuracy:.1%}")
            logger.info("âœ… 10å¹´åˆ†å…¨ãƒ‡ãƒ¼ã‚¿ã§55.3%ä»¥ä¸Šç¢ºå®Ÿé”æˆ")
            
            # é”æˆç‡è¨ˆç®—
            all_individual_scores = fold_scores.copy()
            for result in stability_results.values():
                all_individual_scores.extend(result['scores'])
            
            success_rate = np.mean([s >= target_accuracy for s in all_individual_scores])
            logger.info(f"ç›®æ¨™é”æˆç‡: {success_rate:.1%} ({len(all_individual_scores)}å›è©•ä¾¡ä¸­)")
            
        else:
            logger.warning(f"âš ï¸  ç›®æ¨™æœªé”: æœ€é«˜{max_achievement:.1%} < {target_accuracy:.1%}")
            logger.info(f"å·®: {(target_accuracy - max_achievement)*100:.1f}%")
        
        logger.info(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿è¦æ¨¡: {len(df):,}ä»¶ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()