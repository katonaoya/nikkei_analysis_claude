#!/usr/bin/env python3
"""
ç©¶æ¥µã®ç²¾åº¦æœ€å¤§åŒ–ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ç‰ˆ
ã‚ã‚‰ã‚†ã‚‹æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€å¤§ç²¾åº¦é”æˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, PowerTransformer
from itertools import combinations, product
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class UltimatePrecisionMaximizer:
    """ç©¶æ¥µã®ç²¾åº¦æœ€å¤§åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡
        self.baseline_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'RSI', 'Price_vs_MA20'
        ]
        
        # è¿½åŠ å¯èƒ½ç‰¹å¾´é‡
        self.additional_features = [
            'Returns', 'Volume_Ratio', 'Above_MA20', 'Price_vs_MA10', 'Relative_Return'
        ]
        
        # æœ€é«˜çµæœè¿½è·¡
        self.best_score = 0
        self.best_config = {}
        
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ394,102ä»¶ï¼‰")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        df = pd.read_parquet(processed_files[0])
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ç¢ºèª
        available_additional = [f for f in self.additional_features if f in clean_df.columns]
        self.all_available_features = self.baseline_features + available_additional
        
        logger.info(f"åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {len(self.all_available_features)}å€‹")
        logger.info(f"ç‰¹å¾´é‡: {self.all_available_features}")
        
        X_full = clean_df[self.all_available_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        return X_full, y, clean_df
    
    def exhaustive_feature_combinations(self, X_full, y):
        """å¾¹åº•çš„ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ” å¾¹åº•çš„ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ...")
        
        # ç‰¹å¾´é‡æ•°åˆ¥ãƒ†ã‚¹ãƒˆ
        feature_counts = [3, 4, 5, 6, 7, 8]
        best_combinations = {}
        
        for n_features in feature_counts:
            logger.info(f"  {n_features}ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›...")
            
            best_score_n = 0
            best_combination_n = None
            
            # çµ„ã¿åˆã‚ã›æ•°åˆ¶é™ï¼ˆè¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰
            feature_combinations = list(combinations(self.all_available_features, n_features))
            max_combinations = min(20, len(feature_combinations))
            
            # é‡è¦ãªçµ„ã¿åˆã‚ã›ã‚’å„ªå…ˆ
            priority_combinations = []
            for combo in feature_combinations:
                if all(f in combo for f in self.baseline_features[:3]):  # é‡è¦ç‰¹å¾´é‡å«ã‚€
                    priority_combinations.append(combo)
            
            test_combinations = priority_combinations[:max_combinations] if priority_combinations else feature_combinations[:max_combinations]
            
            for i, feature_combo in enumerate(test_combinations):
                X_subset = X_full[list(feature_combo)]
                
                # é«˜é€Ÿè©•ä¾¡
                score = self._quick_evaluate(X_subset, y)
                
                if score > best_score_n:
                    best_score_n = score
                    best_combination_n = feature_combo
                
                if score > self.best_score:
                    self.best_score = score
                    self.best_config = {
                        'features': feature_combo,
                        'score': score,
                        'n_features': n_features
                    }
                
                if (i + 1) % 5 == 0:
                    logger.info(f"    {i+1}/{len(test_combinations)} å®Œäº† (æœ€é«˜: {best_score_n:.1%})")
            
            best_combinations[n_features] = {
                'score': best_score_n,
                'features': best_combination_n
            }
            
            logger.info(f"  {n_features}ç‰¹å¾´é‡æœ€é«˜: {best_score_n:.1%}")
        
        return best_combinations
    
    def _quick_evaluate(self, X, y):
        """é«˜é€Ÿè©•ä¾¡"""
        X_scaled = StandardScaler().fit_transform(X)
        
        model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=500, random_state=42)
        
        tscv = TimeSeriesSplit(n_splits=2)  # é«˜é€ŸåŒ–ã®ãŸã‚2åˆ†å‰²
        scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            scores.append(accuracy_score(y_test, pred))
        
        return np.mean(scores)
    
    def advanced_preprocessing_optimization(self, X, y, best_features):
        """é«˜åº¦å‰å‡¦ç†æœ€é©åŒ–"""
        logger.info("ğŸ”§ é«˜åº¦å‰å‡¦ç†æœ€é©åŒ–...")
        
        X_best = X[list(best_features)]
        
        preprocessing_methods = {
            'Standard': StandardScaler(),
            'MinMax': MinMaxScaler(),
            'Quantile_1000': QuantileTransformer(n_quantiles=1000, random_state=42),
            'Quantile_500': QuantileTransformer(n_quantiles=500, random_state=42),
            'PowerYeoJohnson': PowerTransformer(method='yeo-johnson', standardize=True),
            'RobustClip_95': 'robust_95',
            'RobustClip_90': 'robust_90',
            'Zscore_3': 'zscore_3',
            'Zscore_2.5': 'zscore_2.5'
        }
        
        preprocessing_results = {}
        
        for name, method in preprocessing_methods.items():
            try:
                logger.info(f"  {name}...")
                
                if name.startswith('RobustClip_'):
                    percentile = int(name.split('_')[1])
                    lower_p = (100 - percentile) / 2
                    upper_p = 100 - lower_p
                    X_processed = X_best.clip(
                        lower=X_best.quantile(lower_p/100), 
                        upper=X_best.quantile(upper_p/100), 
                        axis=0
                    )
                    X_scaled = StandardScaler().fit_transform(X_processed)
                elif name.startswith('Zscore_'):
                    threshold = float(name.split('_')[1])
                    X_processed = X_best.copy()
                    z_scores = np.abs((X_processed - X_processed.mean()) / X_processed.std())
                    X_processed[z_scores > threshold] = np.nan
                    X_processed = X_processed.fillna(X_processed.median())
                    X_scaled = StandardScaler().fit_transform(X_processed)
                else:
                    X_scaled = method.fit_transform(X_best)
                
                # è©•ä¾¡
                score = self._quick_evaluate_with_data(X_scaled, y)
                preprocessing_results[name] = score
                
                logger.info(f"    {name}: {score:.1%}")
                
                if score > self.best_score:
                    self.best_score = score
                    self.best_config.update({
                        'preprocessing': name,
                        'score': score
                    })
                    
            except Exception as e:
                logger.info(f"    {name}: ã‚¨ãƒ©ãƒ¼ ({str(e)[:30]})")
                continue
        
        return preprocessing_results
    
    def _quick_evaluate_with_data(self, X_scaled, y):
        """ãƒ‡ãƒ¼ã‚¿ä»˜ãé«˜é€Ÿè©•ä¾¡"""
        model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=500, random_state=42)
        
        tscv = TimeSeriesSplit(n_splits=2)
        scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            scores.append(accuracy_score(y_test, pred))
        
        return np.mean(scores)
    
    def hyperparameter_grid_search(self, X, y, best_features, best_preprocessing):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ"""
        logger.info("âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ...")
        
        X_best = X[list(best_features)]
        X_processed = self._apply_preprocessing(X_best, best_preprocessing)
        
        # LogisticRegression ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        lr_params = {
            'C': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0],
            'class_weight': [
                'balanced',
                {0: 1, 1: 1.1},
                {0: 1, 1: 1.2},
                {0: 1, 1: 1.3},
                {0: 1, 1: 1.4},
                {0: 1, 1: 1.5},
                {0: 1, 1: 1.7},
                {0: 1, 1: 2.0}
            ]
        }
        
        # æœ€é©LRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
        best_lr_score = 0
        best_lr_params = None
        
        param_combinations = list(product(lr_params['C'], lr_params['class_weight']))
        
        for C, class_weight in param_combinations:
            try:
                model = LogisticRegression(C=C, class_weight=class_weight, max_iter=1000, random_state=42)
                score = self._quick_evaluate_with_data(X_processed, y)
                
                if score > best_lr_score:
                    best_lr_score = score
                    best_lr_params = {'C': C, 'class_weight': class_weight}
                
                if score > self.best_score:
                    self.best_score = score
                    self.best_config.update({
                        'model': 'LogisticRegression',
                        'model_params': {'C': C, 'class_weight': class_weight},
                        'score': score
                    })
                    
            except Exception as e:
                continue
        
        logger.info(f"  æœ€é©LR: {best_lr_score:.1%} {best_lr_params}")
        
        # RandomForest ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        rf_params = [
            {'n_estimators': 50, 'max_depth': 6, 'min_samples_split': 5},
            {'n_estimators': 100, 'max_depth': 8, 'min_samples_split': 2},
            {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5},
            {'n_estimators': 150, 'max_depth': 12, 'min_samples_split': 10},
            {'n_estimators': 80, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 2},
        ]
        
        best_rf_score = 0
        best_rf_params = None
        
        for params in rf_params:
            try:
                model = RandomForestClassifier(**params, class_weight='balanced', random_state=42, n_jobs=-1)
                score = self._quick_evaluate_with_data(X_processed, y)
                
                if score > best_rf_score:
                    best_rf_score = score
                    best_rf_params = params
                
                if score > self.best_score:
                    self.best_score = score
                    self.best_config.update({
                        'model': 'RandomForest',
                        'model_params': params,
                        'score': score
                    })
                    
            except Exception as e:
                continue
        
        logger.info(f"  æœ€é©RF: {best_rf_score:.1%} {best_rf_params}")
        
        return {'LR': (best_lr_score, best_lr_params), 'RF': (best_rf_score, best_rf_params)}
    
    def _apply_preprocessing(self, X, preprocessing_name):
        """å‰å‡¦ç†é©ç”¨"""
        if preprocessing_name == 'Standard':
            return StandardScaler().fit_transform(X)
        elif preprocessing_name == 'MinMax':
            return MinMaxScaler().fit_transform(X)
        elif preprocessing_name.startswith('Quantile_'):
            n_quantiles = int(preprocessing_name.split('_')[1])
            return QuantileTransformer(n_quantiles=n_quantiles, random_state=42).fit_transform(X)
        elif preprocessing_name == 'PowerYeoJohnson':
            return PowerTransformer(method='yeo-johnson', standardize=True).fit_transform(X)
        elif preprocessing_name.startswith('RobustClip_'):
            percentile = int(preprocessing_name.split('_')[1])
            lower_p = (100 - percentile) / 2
            upper_p = 100 - lower_p
            X_processed = X.clip(
                lower=X.quantile(lower_p/100), 
                upper=X.quantile(upper_p/100), 
                axis=0
            )
            return StandardScaler().fit_transform(X_processed)
        else:
            return StandardScaler().fit_transform(X)
    
    def ensemble_optimization(self, X, y, best_features, best_preprocessing):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–"""
        logger.info("ğŸ§  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–...")
        
        X_best = X[list(best_features)]
        X_processed = self._apply_preprocessing(X_best, best_preprocessing)
        
        # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«
        models = {
            'LR': LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, random_state=42),
            'RF': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1),
            'GB': GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42),
            'ET': ExtraTreesClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)
        }
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ„ã¿åˆã‚ã›
        ensemble_combinations = [
            ['LR', 'RF'],
            ['LR', 'GB'],
            ['RF', 'GB'],
            ['LR', 'RF', 'GB'],
            ['LR', 'RF', 'ET'],
            ['RF', 'GB', 'ET'],
            ['LR', 'RF', 'GB', 'ET']
        ]
        
        ensemble_results = {}
        
        for combo in ensemble_combinations:
            try:
                combo_name = '+'.join(combo)
                logger.info(f"  {combo_name}...")
                
                estimators = [(name, models[name]) for name in combo]
                
                # Hard Voting
                voting_hard = VotingClassifier(estimators=estimators, voting='hard')
                score_hard = self._quick_evaluate_with_data(X_processed, y, voting_hard)
                ensemble_results[f'{combo_name}_Hard'] = score_hard
                
                # Soft Voting
                voting_soft = VotingClassifier(estimators=estimators, voting='soft')
                score_soft = self._quick_evaluate_with_data(X_processed, y, voting_soft)
                ensemble_results[f'{combo_name}_Soft'] = score_soft
                
                logger.info(f"    Hard: {score_hard:.1%}, Soft: {score_soft:.1%}")
                
                # æœ€é«˜ã‚¹ã‚³ã‚¢æ›´æ–°
                best_ensemble_score = max(score_hard, score_soft)
                if best_ensemble_score > self.best_score:
                    self.best_score = best_ensemble_score
                    voting_type = 'hard' if score_hard > score_soft else 'soft'
                    self.best_config.update({
                        'model': 'VotingClassifier',
                        'model_params': {'estimators': combo, 'voting': voting_type},
                        'score': best_ensemble_score
                    })
                    
            except Exception as e:
                logger.info(f"    {combo_name}: ã‚¨ãƒ©ãƒ¼ ({str(e)[:30]})")
                continue
        
        return ensemble_results
    
    def _quick_evaluate_with_data(self, X_processed, y, model=None):
        """ãƒ¢ãƒ‡ãƒ«æŒ‡å®šå¯èƒ½ãªé«˜é€Ÿè©•ä¾¡"""
        if model is None:
            model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=500, random_state=42)
        
        tscv = TimeSeriesSplit(n_splits=2)
        scores = []
        
        for train_idx, test_idx in tscv.split(X_processed):
            X_train = X_processed[train_idx]
            X_test = X_processed[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            scores.append(accuracy_score(y_test, pred))
        
        return np.mean(scores)
    
    def final_rigorous_validation(self, X, y):
        """æœ€çµ‚å³å¯†æ¤œè¨¼"""
        logger.info("ğŸ¯ æœ€çµ‚å³å¯†æ¤œè¨¼...")
        logger.info(f"æœ€é©æ§‹æˆ: {self.best_config}")
        
        # æœ€é©è¨­å®šé©ç”¨
        X_best = X[list(self.best_config['features'])]
        X_processed = self._apply_preprocessing(X_best, self.best_config.get('preprocessing', 'Standard'))
        
        # æœ€é©ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        if self.best_config.get('model') == 'LogisticRegression':
            model = LogisticRegression(**self.best_config['model_params'], max_iter=2000, random_state=42)
        elif self.best_config.get('model') == 'RandomForest':
            model = RandomForestClassifier(**self.best_config['model_params'], class_weight='balanced', random_state=42, n_jobs=-1)
        elif self.best_config.get('model') == 'VotingClassifier':
            base_models = {
                'LR': LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, random_state=42),
                'RF': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1),
                'GB': GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42),
                'ET': ExtraTreesClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)
            }
            estimators = [(name, base_models[name]) for name in self.best_config['model_params']['estimators']]
            model = VotingClassifier(estimators=estimators, voting=self.best_config['model_params']['voting'])
        else:
            model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=2000, random_state=42)
        
        # 5åˆ†å‰²å³å¯†æ¤œè¨¼
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        
        logger.info("5åˆ†å‰²æ™‚ç³»åˆ—æ¤œè¨¼å®Ÿè¡Œä¸­...")
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_processed)):
            X_train = X_processed[train_idx]
            X_test = X_processed[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            logger.info(f"  Fold {fold+1}: {accuracy:.1%}")
        
        final_accuracy = np.mean(scores)
        final_std = np.std(scores)
        
        logger.info(f"\nğŸ¯ æœ€çµ‚å³å¯†çµæœ: {final_accuracy:.1%} Â± {final_std:.1%}")
        
        return final_accuracy, final_std, scores

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ ç©¶æ¥µã®ç²¾åº¦æœ€å¤§åŒ–ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ç‰ˆ")
    logger.info("âš¡ ã‚ã‚‰ã‚†ã‚‹æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€å¤§ç²¾åº¦é”æˆ")
    
    maximizer = UltimatePrecisionMaximizer()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X_full, y, clean_df = maximizer.load_and_prepare_data()
        
        # 2. å¾¹åº•çš„ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ
        feature_combinations = maximizer.exhaustive_feature_combinations(X_full, y)
        
        logger.info(f"\nğŸ¯ ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›æœ€é«˜: {maximizer.best_score:.1%}")
        logger.info(f"æœ€é©ç‰¹å¾´é‡: {maximizer.best_config['features']}")
        
        # 3. é«˜åº¦å‰å‡¦ç†æœ€é©åŒ–
        preprocessing_results = maximizer.advanced_preprocessing_optimization(
            X_full, y, maximizer.best_config['features']
        )
        
        logger.info(f"\nğŸ”§ å‰å‡¦ç†æœ€é©åŒ–å¾Œ: {maximizer.best_score:.1%}")
        
        # 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        hyperparameter_results = maximizer.hyperparameter_grid_search(
            X_full, y, maximizer.best_config['features'], 
            maximizer.best_config.get('preprocessing', 'Standard')
        )
        
        logger.info(f"\nâš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å¾Œ: {maximizer.best_score:.1%}")
        
        # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–
        ensemble_results = maximizer.ensemble_optimization(
            X_full, y, maximizer.best_config['features'], 
            maximizer.best_config.get('preprocessing', 'Standard')
        )
        
        logger.info(f"\nğŸ§  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–å¾Œ: {maximizer.best_score:.1%}")
        
        # 6. æœ€çµ‚å³å¯†æ¤œè¨¼
        final_accuracy, final_std, fold_scores = maximizer.final_rigorous_validation(X_full, y)
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ ç©¶æ¥µã®ç²¾åº¦æœ€å¤§åŒ–çµæœ")
        logger.info("="*80)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(clean_df):,}ä»¶ (å…¨ãƒ‡ãƒ¼ã‚¿)")
        logger.info(f"æœ€é©ç‰¹å¾´é‡æ•°: {len(maximizer.best_config['features'])}å€‹")
        logger.info(f"æœ€é©ç‰¹å¾´é‡: {list(maximizer.best_config['features'])}")
        
        if 'preprocessing' in maximizer.best_config:
            logger.info(f"æœ€é©å‰å‡¦ç†: {maximizer.best_config['preprocessing']}")
        
        if 'model' in maximizer.best_config:
            logger.info(f"æœ€é©ãƒ¢ãƒ‡ãƒ«: {maximizer.best_config['model']}")
            logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {maximizer.best_config.get('model_params', {})}")
        
        logger.info(f"\nğŸ† ç©¶æ¥µã®é”æˆç²¾åº¦: {final_accuracy:.1%} Â± {final_std:.1%}")
        logger.info(f"é–‹ç™ºä¸­æœ€é«˜ç²¾åº¦: {maximizer.best_score:.1%}")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
        baseline = 0.505  # ä»¥å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        improvement = (final_accuracy - baseline) * 100
        
        logger.info(f"\nğŸ“Š æ”¹å–„çµæœ:")
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline:.1%}")
        logger.info(f"æœ€çµ‚ç²¾åº¦: {final_accuracy:.1%}")
        logger.info(f"æ”¹å–„å¹…: {improvement:+.1f}%")
        
        if improvement > 1.0:
            logger.info("ğŸ‰ å¤§å¹…ãªæ”¹å–„ã‚’é”æˆã—ã¾ã—ãŸï¼")
        elif improvement > 0.5:
            logger.info("âœ… æœ‰æ„ãªæ”¹å–„ã‚’é”æˆã—ã¾ã—ãŸ")
        elif improvement > 0.2:
            logger.info("ğŸ”„ é™å®šçš„ã§ã™ãŒæ”¹å–„ã‚’é”æˆã—ã¾ã—ãŸ")
        else:
            logger.info("âš ï¸ æ”¹å–„ã¯é™å®šçš„ã§ã—ãŸ")
        
        logger.info(f"\nâš ï¸ ã“ã®çµæœã¯394,102ä»¶ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å³å¯†æ¤œè¨¼ã§ã™")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()