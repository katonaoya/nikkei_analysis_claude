#!/usr/bin/env python3
"""
åŠ¹ç‡çš„ç©¶æ¥µç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ç‰ˆ
æœ€å¤§ç²¾åº¦ã‚’åŠ¹ç‡çš„ã«é”æˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from concurrent.futures import ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class EfficientUltimateFeatureSelector:
    """åŠ¹ç‡çš„ç©¶æ¥µç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
    def load_full_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ394,102ä»¶ï¼‰")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ç¢ºèª
        df['Date'] = pd.to_datetime(df['Date'])
        min_date = df['Date'].min()
        max_date = df['Date'].max()
        years = (max_date - min_date).days / 365.25
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {min_date.date()} ~ {max_date.date()} ({years:.1f}å¹´é–“)")
        
        return df
    
    def create_advanced_features(self, df):
        """é«˜åº¦ãªç‰¹å¾´é‡ä½œæˆï¼ˆåŠ¹ç‡çš„ç‰ˆï¼‰"""
        logger.info("ğŸ”§ é«˜åº¦ç‰¹å¾´é‡ä½œæˆä¸­ï¼ˆåŠ¹ç‡çš„ç‰ˆï¼‰...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # 1. å¤šæœŸé–“ç§»å‹•å¹³å‡ã¨ãã®ä¹–é›¢ç‡
        logger.info("1/7: å¤šæœŸé–“ç§»å‹•å¹³å‡ç³»...")
        ma_periods = [5, 10, 20, 25, 50, 75]
        for period in ma_periods:
            df[f'MA_{period}'] = df.groupby('Code')['Close'].rolling(period, min_periods=1).mean().reset_index(0, drop=True)
            df[f'Price_vs_MA{period}'] = (df['Close'] - df[f'MA_{period}']) / (df[f'MA_{period}'] + 1e-6)
            df[f'MA_Slope_{period}'] = df.groupby('Code')[f'MA_{period}'].pct_change(3)
        
        # 2. å¤šæœŸé–“ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        logger.info("2/7: å¤šæœŸé–“ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»...")
        vol_periods = [5, 10, 20, 30]
        for period in vol_periods:
            df[f'Volatility_{period}'] = df.groupby('Code')['Close'].rolling(period, min_periods=1).std().reset_index(0, drop=True)
            df[f'VolRank_{period}'] = df.groupby('Date')[f'Volatility_{period}'].rank(pct=True)
        
        # 3. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        logger.info("3/7: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™ç³»...")
        momentum_periods = [3, 5, 10, 20]
        for period in momentum_periods:
            df[f'Momentum_{period}'] = df.groupby('Code')['Close'].pct_change(period)
            df[f'ReturnSum_{period}'] = df.groupby('Code')['Returns'].rolling(period, min_periods=1).sum().reset_index(0, drop=True)
        
        # 4. RSIï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        logger.info("4/7: RSIç³»...")
        for period in [7, 14, 21]:
            df[f'RSI_{period}'] = self._calculate_rsi_fast(df, period)
        
        # 5. å‡ºæ¥é«˜æŒ‡æ¨™
        logger.info("5/7: å‡ºæ¥é«˜æŒ‡æ¨™ç³»...")
        vol_periods = [5, 10, 20]
        for period in vol_periods:
            df[f'VolMA_{period}'] = df.groupby('Code')['Volume'].rolling(period, min_periods=1).mean().reset_index(0, drop=True)
            df[f'VolRatio_{period}'] = df['Volume'] / (df[f'VolMA_{period}'] + 1e-6)
        
        # 6. å¸‚å ´æ§‹é€ æŒ‡æ¨™
        logger.info("6/7: å¸‚å ´æ§‹é€ æŒ‡æ¨™...")
        
        # æ—¥æ¬¡å¸‚å ´çµ±è¨ˆ
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std'],
            'Volume': ['mean', 'std'], 
            'Returns': ['mean', 'std']
        }).round(6)
        daily_market.columns = ['Mkt_Price_Mean', 'Mkt_Price_Std', 'Mkt_Vol_Mean', 'Mkt_Vol_Std', 'Mkt_Ret_Mean', 'Mkt_Ret_Std']
        daily_market = daily_market.reset_index()
        
        # å¸‚å ´å¹…æŒ‡æ¨™
        daily_breadth = df.groupby('Date')['Returns'].agg([
            ('Breadth', lambda x: (x > 0).sum() / len(x)),
            ('StrongUp', lambda x: (x > 0.02).sum() / len(x)),
            ('StrongDown', lambda x: (x < -0.02).sum() / len(x))
        ]).reset_index()
        
        # ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æ
        df['Sector'] = df['Code'].astype(str).str[:2]
        sector_stats = df.groupby(['Date', 'Sector'])['Returns'].mean().reset_index()
        sector_stats.columns = ['Date', 'Sector', 'SectorRet']
        
        # ãƒãƒ¼ã‚¸
        df = df.merge(daily_market, on='Date', how='left')
        df = df.merge(daily_breadth, on='Date', how='left')
        df = df.merge(sector_stats, on=['Date', 'Sector'], how='left')
        
        # 7. ç›¸å¯¾æŒ‡æ¨™
        logger.info("7/7: ç›¸å¯¾æŒ‡æ¨™...")
        df['RelativeToMarket'] = df['Returns'] - df['Mkt_Ret_Mean']
        df['RelativeToSector'] = df['Returns'] - df['SectorRet']
        df['PriceVsMarket'] = df['Close'] / (df['Mkt_Price_Mean'] + 1e-6)
        df['VolVsMarket'] = df['Volume'] / (df['Mkt_Vol_Mean'] + 1e-6)
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        
        logger.info(f"âœ… é«˜åº¦ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        return df
    
    def _calculate_rsi_fast(self, df, period):
        """é«˜é€ŸRSIè¨ˆç®—"""
        def rsi_calc(group):
            delta = group['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + 1e-6)
            return 100 - (100 / (1 + rs))
        
        return df.groupby('Code', group_keys=False).apply(rsi_calc).reset_index(0, drop=True)
    
    def get_feature_categories(self, df):
        """åŠ¹ç‡çš„ç‰¹å¾´é‡åˆ†é¡"""
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector'
        }
        
        all_features = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        categories = {
            'basic': [col for col in all_features if col in ['Returns']],
            'ma_system': [col for col in all_features if 'MA_' in col or 'Price_vs_MA' in col or 'MA_Slope' in col],
            'volatility': [col for col in all_features if 'Volatility' in col or 'VolRank' in col],
            'momentum': [col for col in all_features if 'Momentum' in col or 'ReturnSum' in col or 'RSI' in col],
            'volume': [col for col in all_features if 'Vol' in col and col != 'Volume'],
            'market': [col for col in all_features if col.startswith('Mkt_')],
            'breadth': [col for col in all_features if col in ['Breadth', 'StrongUp', 'StrongDown']],
            'relative': [col for col in all_features if 'Relative' in col or 'Vs' in col],
            'sector': [col for col in all_features if 'Sector' in col]
        }
        
        # åˆ†é¡çµæœè¡¨ç¤º
        for cat, features in categories.items():
            if features:
                logger.info(f"{cat:15s}: {len(features):3d}å€‹")
        
        logger.info(f"å…¨ç‰¹å¾´é‡: {len(all_features)}å€‹")
        return categories, all_features
    
    def rapid_feature_evaluation(self, X, y, features, eval_name):
        """é«˜é€Ÿç‰¹å¾´é‡è©•ä¾¡"""
        if not features:
            return 0.0
            
        X_subset = X[features]
        X_scaled = self.scaler.fit_transform(X_subset)
        
        # 3åˆ†å‰²ã§é«˜é€Ÿè©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            # LogisticRegressionã§é«˜é€Ÿè©•ä¾¡
            model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=500, random_state=42)
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            scores.append(accuracy_score(y_test, pred))
        
        avg_score = np.mean(scores)
        return avg_score
    
    def category_evaluation(self, df, categories, all_features):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥è©•ä¾¡"""
        logger.info("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥è©•ä¾¡...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[all_features]
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶, ç‰¹å¾´é‡: {len(all_features)}å€‹")
        
        category_results = {}
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®å˜ç‹¬è©•ä¾¡
        for category, features in categories.items():
            if features:
                logger.info(f"  {category} ({len(features)}ç‰¹å¾´é‡)...")
                score = self.rapid_feature_evaluation(X, y, features, category)
                category_results[category] = {
                    'score': score,
                    'features': features,
                    'count': len(features)
                }
                logger.info(f"    {category:15s}: {score:.1%}")
        
        return category_results, X, y
    
    def smart_feature_selection(self, X, y):
        """ã‚¹ãƒãƒ¼ãƒˆç‰¹å¾´é‡é¸æŠ"""
        logger.info("ğŸ§  ã‚¹ãƒãƒ¼ãƒˆç‰¹å¾´é‡é¸æŠ...")
        
        # 1. çµ±è¨ˆçš„é‡è¦åº¦
        logger.info("  1/4: çµ±è¨ˆçš„é‡è¦åº¦...")
        f_scores = f_classif(X, y)[0]
        f_ranking = list(zip(X.columns, f_scores))
        f_ranking.sort(key=lambda x: x[1], reverse=True)
        
        # 2. RandomForesté‡è¦åº¦
        logger.info("  2/4: RandomForesté‡è¦åº¦...")
        rf = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)
        rf.fit(X, y)
        rf_ranking = list(zip(X.columns, rf.feature_importances_))
        rf_ranking.sort(key=lambda x: x[1], reverse=True)
        
        # 3. ç›¸äº’æƒ…å ±é‡
        logger.info("  3/4: ç›¸äº’æƒ…å ±é‡...")
        mi_scores = mutual_info_classif(X, y, random_state=42)
        mi_ranking = list(zip(X.columns, mi_scores))
        mi_ranking.sort(key=lambda x: x[1], reverse=True)
        
        # 4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        logger.info("  4/4: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°...")
        ensemble_scores = {}
        
        # æ­£è¦åŒ–ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        all_rankings = [f_ranking, rf_ranking, mi_ranking]
        
        for ranking in all_rankings:
            scores = [score for name, score in ranking]
            if max(scores) > min(scores):
                min_s, max_s = min(scores), max(scores)
                for name, score in ranking:
                    norm_score = (score - min_s) / (max_s - min_s)
                    if name not in ensemble_scores:
                        ensemble_scores[name] = []
                    ensemble_scores[name].append(norm_score)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢
        final_ranking = []
        for name, scores in ensemble_scores.items():
            final_ranking.append((name, np.mean(scores)))
        
        final_ranking.sort(key=lambda x: x[1], reverse=True)
        
        logger.info("ä¸Šä½20ç‰¹å¾´é‡:")
        for i, (feature, score) in enumerate(final_ranking[:20]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {score:.4f}")
        
        return final_ranking
    
    def progressive_testing(self, X, y, feature_ranking):
        """æ®µéšçš„ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“ˆ æ®µéšçš„ãƒ†ã‚¹ãƒˆ...")
        
        # ãƒ†ã‚¹ãƒˆç‰¹å¾´é‡æ•°
        test_counts = [3, 5, 7, 10, 15, 20, 25, 30, 40, 50]
        max_features = min(len(feature_ranking), 50)
        test_counts = [n for n in test_counts if n <= max_features]
        
        results = {}
        models = {
            'LogisticRegression': LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42),
            'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)
        }
        
        for n_features in test_counts:
            logger.info(f"  {n_features}ç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆ...")
            
            # ä¸Šä½Nç‰¹å¾´é‡é¸æŠ
            selected_features = [name for name, score in feature_ranking[:n_features]]
            X_selected = X[selected_features]
            X_scaled = self.scaler.fit_transform(X_selected)
            
            model_results = {}
            
            # å„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
            for model_name, model in models.items():
                tscv = TimeSeriesSplit(n_splits=5)
                scores = []
                
                for train_idx, test_idx in tscv.split(X_scaled):
                    X_train = X_scaled[train_idx]
                    X_test = X_scaled[test_idx]
                    y_train = y.iloc[train_idx]
                    y_test = y.iloc[test_idx]
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_test)
                    scores.append(accuracy_score(y_test, pred))
                
                avg_score = np.mean(scores)
                std_score = np.std(scores)
                model_results[model_name] = {'avg': avg_score, 'std': std_score}
            
            # æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«
            best_model = max(model_results.keys(), key=lambda k: model_results[k]['avg'])
            best_score = model_results[best_model]['avg']
            best_std = model_results[best_model]['std']
            
            results[n_features] = {
                'best_model': best_model,
                'best_score': best_score,
                'best_std': best_std,
                'all_results': model_results,
                'features': selected_features
            }
            
            logger.info(f"    {n_features:2d}ç‰¹å¾´é‡: {best_score:.1%}Â±{best_std:.1%} ({best_model})")
        
        # æœ€é«˜æ€§èƒ½ç‰¹å®š
        best_n = max(results.keys(), key=lambda k: results[k]['best_score'])
        best_result = results[best_n]
        
        logger.info(f"\nğŸ† æœ€é«˜æ€§èƒ½: {best_n}ç‰¹å¾´é‡, {best_result['best_score']:.1%}Â±{best_result['best_std']:.1%}")
        logger.info(f"æœ€é©ãƒ¢ãƒ‡ãƒ«: {best_result['best_model']}")
        
        return results, best_result
    
    def category_combination_test(self, X, y, category_results):
        """ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”„ ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ...")
        
        # æ€§èƒ½ã®è‰¯ã„ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
        sorted_categories = sorted(category_results.items(), key=lambda x: x[1]['score'], reverse=True)
        top_categories = [cat for cat, result in sorted_categories[:6]]
        
        logger.info(f"ä¸Šä½ã‚«ãƒ†ã‚´ãƒª: {top_categories}")
        
        combo_results = {}
        
        # 2ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›
        from itertools import combinations
        for combo in combinations(top_categories, 2):
            combo_features = []
            for cat in combo:
                combo_features.extend(category_results[cat]['features'])
            
            # ç‰¹å¾´é‡æ•°åˆ¶é™
            if len(combo_features) > 40:
                continue
                
            combo_name = '+'.join(combo)
            score = self.rapid_feature_evaluation(X, y, combo_features, combo_name)
            
            combo_results[combo_name] = {
                'score': score,
                'features': combo_features,
                'categories': combo,
                'count': len(combo_features)
            }
            
            logger.info(f"  {combo_name:30s}: {score:.1%} ({len(combo_features)}ç‰¹å¾´é‡)")
        
        # æœ€é«˜çµ„ã¿åˆã‚ã›
        if combo_results:
            best_combo = max(combo_results.keys(), key=lambda k: combo_results[k]['score'])
            logger.info(f"\nğŸ† æœ€é«˜çµ„ã¿åˆã‚ã›: {best_combo} ({combo_results[best_combo]['score']:.1%})")
        
        return combo_results
    
    def final_validation(self, X, y, best_features, best_model_name):
        """æœ€çµ‚æ¤œè¨¼"""
        logger.info("ğŸ¯ æœ€çµ‚æ¤œè¨¼...")
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(best_features)}, ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
        
        X_final = X[best_features]
        X_scaled = self.scaler.fit_transform(X_final)
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        if best_model_name == 'LogisticRegression':
            model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=2000, random_state=42)
        else:
            model = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)
        
        # 5åˆ†å‰²ã§æœ€çµ‚è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            logger.info(f"Fold {fold+1}: {accuracy:.1%}")
        
        final_accuracy = np.mean(scores)
        final_std = np.std(scores)
        
        logger.info(f"\nğŸ¯ æœ€çµ‚çµæœ: {final_accuracy:.1%} Â± {final_std:.1%}")
        
        return final_accuracy, final_std, scores

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ åŠ¹ç‡çš„ç©¶æ¥µç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ç‰ˆ")
    logger.info("ğŸ¯ ç›®æ¨™: æœ€å¤§ç²¾åº¦é”æˆ")
    
    selector = EfficientUltimateFeatureSelector()
    
    try:
        # 1. å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = selector.load_full_data()
        if df is None:
            return
        
        # 2. é«˜åº¦ç‰¹å¾´é‡ä½œæˆ
        df = selector.create_advanced_features(df)
        
        # 3. ç‰¹å¾´é‡åˆ†é¡
        categories, all_features = selector.get_feature_categories(df)
        
        # 4. ã‚«ãƒ†ã‚´ãƒªåˆ¥è©•ä¾¡
        category_results, X, y = selector.category_evaluation(df, categories, all_features)
        
        # 5. ã‚¹ãƒãƒ¼ãƒˆç‰¹å¾´é‡é¸æŠ
        feature_ranking = selector.smart_feature_selection(X, y)
        
        # 6. æ®µéšçš„ãƒ†ã‚¹ãƒˆ
        progressive_results, best_progressive = selector.progressive_testing(X, y, feature_ranking)
        
        # 7. ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ
        combo_results = selector.category_combination_test(X, y, category_results)
        
        # 8. æœ€çµ‚æ¤œè¨¼
        final_accuracy, final_std, fold_scores = selector.final_validation(
            X, y, best_progressive['features'], best_progressive['best_model']
        )
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ åŠ¹ç‡çš„ç©¶æ¥µç‰¹å¾´é‡é¸æŠçµæœ")
        logger.info("="*80)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(df):,}ä»¶ (å…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼)")
        logger.info(f"ä½œæˆç‰¹å¾´é‡: {len(all_features)}å€‹")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
        logger.info("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥æ€§èƒ½:")
        sorted_cats = sorted(category_results.items(), key=lambda x: x[1]['score'], reverse=True)
        for cat, result in sorted_cats:
            logger.info(f"  {cat:15s}: {result['score']:.1%} ({result['count']}ç‰¹å¾´é‡)")
        
        # æœ€é«˜ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›
        if combo_results:
            best_combo = max(combo_results.keys(), key=lambda k: combo_results[k]['score'])
            logger.info(f"\nğŸ† æœ€é«˜ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›: {best_combo}")
            logger.info(f"çµ„ã¿åˆã‚ã›ç²¾åº¦: {combo_results[best_combo]['score']:.1%}")
        
        # æ®µéšçš„æœ€é«˜çµæœ
        logger.info(f"\nğŸ“ˆ æ®µéšçš„æœ€é©åŒ–çµæœ:")
        logger.info(f"æœ€é©ç‰¹å¾´é‡æ•°: {len(best_progressive['features'])}")
        logger.info(f"æ®µéšçš„æœ€é«˜ç²¾åº¦: {best_progressive['best_score']:.1%} Â± {best_progressive['best_std']:.1%}")
        
        # æœ€çµ‚æ¤œè¨¼çµæœ
        logger.info(f"\nğŸ¯ æœ€çµ‚æ¤œè¨¼çµæœ: {final_accuracy:.1%} Â± {final_std:.1%}")
        logger.info(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {best_progressive['best_model']}")
        
        # æœ€é©ç‰¹å¾´é‡
        logger.info(f"\næœ€é©ç‰¹å¾´é‡ (ä¸Šä½20å€‹):")
        for i, feature in enumerate(best_progressive['features'][:20], 1):
            logger.info(f"  {i:2d}. {feature}")
        if len(best_progressive['features']) > 20:
            logger.info(f"  ... ä»–{len(best_progressive['features'])-20}å€‹")
        
        # å…¨ä½“ã®æœ€é«˜ç²¾åº¦
        all_scores = [final_accuracy, best_progressive['best_score']]
        if combo_results:
            all_scores.append(max(combo_results[k]['score'] for k in combo_results))
        
        max_achieved = max(all_scores)
        logger.info(f"\nğŸ† é”æˆæœ€é«˜ç²¾åº¦: {max_achieved:.1%}")
        logger.info(f"âš ï¸ ã“ã®çµæœã¯394,102ä»¶ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å³å¯†æ¤œè¨¼ã§ã™")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()