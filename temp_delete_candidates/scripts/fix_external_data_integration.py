#!/usr/bin/env python3
"""
å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆä¿®æ­£
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

def fix_external_data():
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®ä¿®æ­£ã¨çµ±åˆ"""
    logger.info("ğŸ”§ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ä¿®æ­£ã¨çµ±åˆ")
    
    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    external_file = Path("data/external/external_macro_data.parquet")
    if not external_file.exists():
        logger.error("âŒ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    # ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ã‚«ãƒ©ãƒ ã‚’ä¿®æ­£
    external_data = pd.read_parquet(external_file)
    
    # ã‚«ãƒ©ãƒ åã‚’ä¿®æ­£ï¼ˆã‚¿ãƒ—ãƒ«ã‹ã‚‰æ–‡å­—åˆ—ã«ï¼‰
    if isinstance(external_data.columns[0], tuple):
        external_data.columns = [col[0] for col in external_data.columns]
    
    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_data):,}ä»¶")
    logger.info(f"ã‚«ãƒ©ãƒ : {list(external_data.columns)}")
    
    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
    external_data['Date'] = pd.to_datetime(external_data['Date'])
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    processed_dir = Path("data/processed")
    existing_files = list(processed_dir.glob("*.parquet"))
    
    if not existing_files:
        logger.error("âŒ æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
        
    existing_data = pd.read_parquet(existing_files[0])
    existing_data['Date'] = pd.to_datetime(existing_data['Date'])
    logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data):,}ä»¶")
    
    # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã§çµ±åˆ
    logger.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
    integrated_data = existing_data.merge(external_data, on='Date', how='left')
    
    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®å‰åŸ‹ã‚ï¼ˆå¹³æ—¥ã®æ ªå¼ãƒ‡ãƒ¼ã‚¿ã«é€±æœ«ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’é©ç”¨ï¼‰
    external_cols = [col for col in external_data.columns if col != 'Date']
    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {len(external_cols)}å€‹")
    
    # å‰åŸ‹ã‚å‡¦ç†
    integrated_data[external_cols] = integrated_data[external_cols].fillna(method='ffill')
    
    # æ¬ æå€¤çµ±è¨ˆ
    missing_stats = integrated_data[external_cols].isnull().sum()
    logger.info(f"å‰åŸ‹ã‚å¾Œã®æ¬ æå€¤:")
    for col, missing_count in missing_stats[missing_stats > 0].items():
        logger.info(f"  {col}: {missing_count:,}ä»¶ ({missing_count/len(integrated_data)*100:.1f}%)")
    
    # æ®‹ã‚Šã®æ¬ æå€¤ã¯0ã§åŸ‹ã‚ã‚‹
    integrated_data[external_cols] = integrated_data[external_cols].fillna(0)
    
    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_data):,}ä»¶")
    logger.info(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(integrated_data.columns)}")
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
    integrated_file = processed_dir / "integrated_with_external.parquet"
    integrated_data.to_parquet(integrated_file, index=False)
    logger.info(f"ğŸ’¾ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {integrated_file}")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    logger.info(f"\nğŸ“‹ çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ï¼‰:")
    external_sample = integrated_data[['Date'] + external_cols].tail()
    print(external_sample.to_string(index=False))
    
    return integrated_data

def validate_integration():
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    logger.info("âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")
    
    integrated_file = Path("data/processed/integrated_with_external.parquet")
    if not integrated_file.exists():
        logger.error("âŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    data = pd.read_parquet(integrated_file)
    logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(data):,}ä»¶, {len(data.columns)}ã‚«ãƒ©ãƒ ")
    
    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã®ç¢ºèª
    external_pattern_cols = [col for col in data.columns if any(pattern in col for pattern in ['us_10y', 'sp500', 'usd_jpy', 'nikkei', 'vix'])]
    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {len(external_pattern_cols)}å€‹")
    
    for col in external_pattern_cols:
        non_null_count = data[col].notna().sum()
        logger.info(f"  {col}: {non_null_count:,}/{len(data):,}ä»¶ ({non_null_count/len(data)*100:.1f}%)")
    
    # çµ±è¨ˆæƒ…å ±
    if external_pattern_cols:
        logger.info(f"\nğŸ“Š å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        stats = data[external_pattern_cols].describe()
        print(stats.round(4).to_string())
    
    # Binary_Directionã®å­˜åœ¨ç¢ºèª
    if 'Binary_Direction' in data.columns:
        valid_targets = data['Binary_Direction'].notna().sum()
        logger.info(f"\nğŸ¯ äºˆæ¸¬å¯¾è±¡: {valid_targets:,}ä»¶")
        logger.info(f"âœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ–°ç‰¹å¾´é‡ã§ã®ç²¾åº¦è©•ä¾¡ãŒå¯èƒ½")
    else:
        logger.warning("âš ï¸ Binary_DirectionãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ ")
    
    try:
        # 1. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ä¿®æ­£ã¨çµ±åˆ
        integrated_data = fix_external_data()
        
        if integrated_data is not None:
            # 2. çµ±åˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            validate_integration()
            
            logger.info("\n" + "="*80)
            logger.info("âœ… å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆä¿®æ­£å®Œäº†")
            logger.info("="*80)
            logger.info("ğŸš€ æ¬¡ã¯æ–°ç‰¹å¾´é‡ã§ã®ç²¾åº¦è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        else:
            logger.error("âŒ çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()