#!/usr/bin/env python3
"""
æœ€çµ‚60%çªç ´ãƒ—ãƒƒã‚·ãƒ¥
56%ã‹ã‚‰60%ã¸ã®æœ€çµ‚èª¿æ•´
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def final_60_push():
    """æœ€çµ‚60%çªç ´ãƒ—ãƒƒã‚·ãƒ¥"""
    
    print("ğŸ¯ æœ€çµ‚60%çªç ´ãƒ—ãƒƒã‚·ãƒ¥é–‹å§‹")
    print("ğŸ“ˆ 56.00% â†’ 60.00% ã¸ã®æœ€çµ‚èª¿æ•´")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_parquet('data/processed/integrated_with_external.parquet')
    
    # ã‚«ãƒ©ãƒ èª¿æ•´
    if 'date' in df.columns:
        df['Date'] = pd.to_datetime(df['date'])
    if 'code' in df.columns:
        df['Stock'] = df['code']
    
    print("ğŸ”§ æ”¹è‰¯ç‰¹å¾´é‡ç”Ÿæˆ...")
    
    # æ”¹è‰¯ç‰¹å¾´é‡ç”Ÿæˆ
    features = []
    for stock, stock_df in df.groupby('Stock'):
        if len(stock_df) < 30:
            continue
            
        stock_df = stock_df.sort_values('Date')
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆç¿Œæ—¥1%ä»¥ä¸Šä¸Šæ˜‡ã«å¤‰æ›´ - ã‚ˆã‚Šå³ã—ã„æ¡ä»¶ï¼‰
        stock_df['next_return'] = (stock_df['close'].shift(-1) / stock_df['close']) - 1
        stock_df['Target'] = (stock_df['next_return'] >= 0.01).astype(int)
        
        # åŸºæœ¬ç‰¹å¾´é‡
        delta = stock_df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss.replace(0, 1)
        stock_df['RSI'] = 100 - (100 / (1 + rs))
        
        # è¤‡æ•°æ™‚é–“è»¸ç§»å‹•å¹³å‡
        stock_df['MA5'] = stock_df['close'].rolling(5).mean()
        stock_df['MA10'] = stock_df['close'].rolling(10).mean()
        stock_df['MA20'] = stock_df['close'].rolling(20).mean()
        stock_df['Price_vs_MA5'] = (stock_df['close'] - stock_df['MA5']) / stock_df['MA5']
        stock_df['Price_vs_MA10'] = (stock_df['close'] - stock_df['MA10']) / stock_df['MA10']
        stock_df['Price_vs_MA20'] = (stock_df['close'] - stock_df['MA20']) / stock_df['MA20']
        
        # é«˜åº¦ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™
        stock_df['Return'] = stock_df['close'].pct_change()
        stock_df['Volatility_10'] = stock_df['Return'].rolling(10).std()
        stock_df['Volatility_20'] = stock_df['Return'].rolling(20).std()
        stock_df['Vol_Ratio'] = stock_df['Volatility_10'] / stock_df['Volatility_20'].replace(0, 1)
        
        # å‡ºæ¥é«˜åˆ†æ
        stock_df['Volume_MA20'] = stock_df['volume'].rolling(20).mean()
        stock_df['Volume_Ratio'] = stock_df['volume'] / stock_df['Volume_MA20'].replace(0, 1)
        stock_df['Volume_Surge'] = (stock_df['Volume_Ratio'] > 1.5).astype(int)
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        stock_df['Momentum_3'] = stock_df['close'].pct_change(3)
        stock_df['Momentum_5'] = stock_df['close'].pct_change(5)
        stock_df['Momentum_10'] = stock_df['close'].pct_change(10)
        stock_df['Momentum_20'] = stock_df['close'].pct_change(20)
        
        # ä¾¡æ ¼ä½ç½®ã¨ãƒˆãƒ¬ãƒ³ãƒ‰
        stock_df['High_20'] = stock_df['high'].rolling(20).max()
        stock_df['Low_20'] = stock_df['low'].rolling(20).min()
        stock_df['Price_Position'] = (stock_df['close'] - stock_df['Low_20']) / (stock_df['High_20'] - stock_df['Low_20'])
        
        # MAå‚¾ã
        stock_df['MA5_Slope'] = stock_df['MA5'].pct_change(2)
        stock_df['MA20_Slope'] = stock_df['MA20'].pct_change(5)
        
        # RSI-Based signals
        stock_df['RSI_Oversold'] = (stock_df['RSI'] < 30).astype(int)
        stock_df['RSI_Recovery'] = ((stock_df['RSI'] > 30) & (stock_df['RSI'].shift(1) <= 30)).astype(int)
        
        features.append(stock_df)
    
    df = pd.concat(features, ignore_index=True)
    
    # æ”¹è‰¯ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ
    feature_cols = [
        'RSI',
        'Price_vs_MA5', 'Price_vs_MA10', 'Price_vs_MA20',
        'Volatility_10', 'Volatility_20', 'Vol_Ratio',
        'Volume_Ratio', 'Volume_Surge',
        'Momentum_3', 'Momentum_5', 'Momentum_10', 'Momentum_20',
        'Price_Position',
        'MA5_Slope', 'MA20_Slope',
        'RSI_Oversold', 'RSI_Recovery'
    ]
    
    print(f"æ”¹è‰¯ç‰¹å¾´é‡: {len(feature_cols)}å€‹")
    
    # è¤‡æ•°æˆ¦ç•¥ãƒ†ã‚¹ãƒˆ
    strategies_results = []
    
    # ãƒ†ã‚¹ãƒˆæœŸé–“
    df_sorted = df.sort_values('Date')
    unique_dates = sorted(df_sorted['Date'].unique())
    test_dates = unique_dates[-30:]
    
    # === æˆ¦ç•¥1: æ”¹è‰¯LightGBM + ä¸Šä½3éŠ˜æŸ„ ===
    print("\nğŸš€ æˆ¦ç•¥1: æ”¹è‰¯LightGBMä¸Šä½3éŠ˜æŸ„")
    
    model1 = lgb.LGBMClassifier(
        n_estimators=150,
        max_depth=4,
        min_child_samples=15,
        subsample=0.85,
        colsample_bytree=0.85,
        learning_rate=0.08,
        random_state=42,
        verbose=-1
    )
    
    all_preds_1 = []
    all_actuals_1 = []
    
    for test_date in test_dates:
        train = df_sorted[df_sorted['Date'] < test_date]
        test = df_sorted[df_sorted['Date'] == test_date]
        
        train_clean = train.dropna(subset=['Target'] + feature_cols)
        test_clean = test.dropna(subset=['Target'] + feature_cols)
        
        if len(train_clean) < 1000 or len(test_clean) < 3:
            continue
        
        X_train = train_clean[feature_cols]
        y_train = train_clean['Target']
        X_test = test_clean[feature_cols]
        y_test = test_clean['Target']
        
        # RobustScalerä½¿ç”¨
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model1.fit(X_train_scaled, y_train)
        probs = model1.predict_proba(X_test_scaled)[:, 1]
        
        # ä¸Šä½3éŠ˜æŸ„é¸æŠ
        n_select = min(3, len(probs))
        if n_select > 0:
            top_idx = np.argsort(probs)[-n_select:]
            selected = y_test.iloc[top_idx].values
            all_preds_1.extend(np.ones(len(selected)))
            all_actuals_1.extend(selected)
    
    if len(all_preds_1) > 0:
        precision_1 = sum(all_actuals_1) / len(all_preds_1)
        strategies_results.append(('æ”¹è‰¯LightGBM_ä¸Šä½3', precision_1, len(all_preds_1)))
        print(f"  ç²¾åº¦: {precision_1:.2%}, é¸æŠæ•°: {len(all_preds_1)}")
    
    # === æˆ¦ç•¥2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« + ä¸Šä½2éŠ˜æŸ„ ===
    print("\nğŸ”¥ æˆ¦ç•¥2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸Šä½2éŠ˜æŸ„")
    
    models = [
        lgb.LGBMClassifier(n_estimators=120, max_depth=3, random_state=42, verbose=-1),
        RandomForestClassifier(n_estimators=120, max_depth=5, random_state=43),
        lgb.LGBMClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=44, verbose=-1)
    ]
    
    all_preds_2 = []
    all_actuals_2 = []
    
    for test_date in test_dates:
        train = df_sorted[df_sorted['Date'] < test_date]
        test = df_sorted[df_sorted['Date'] == test_date]
        
        train_clean = train.dropna(subset=['Target'] + feature_cols)
        test_clean = test.dropna(subset=['Target'] + feature_cols)
        
        if len(train_clean) < 1000 or len(test_clean) < 2:
            continue
        
        X_train = train_clean[feature_cols]
        y_train = train_clean['Target']
        X_test = test_clean[feature_cols]
        y_test = test_clean['Target']
        
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        ensemble_probs = []
        for model in models:
            model.fit(X_train_scaled, y_train)
            probs = model.predict_proba(X_test_scaled)[:, 1]
            ensemble_probs.append(probs)
        
        avg_probs = np.mean(ensemble_probs, axis=0)
        
        # ä¸Šä½2éŠ˜æŸ„é¸æŠ
        n_select = min(2, len(avg_probs))
        if n_select > 0:
            top_idx = np.argsort(avg_probs)[-n_select:]
            selected = y_test.iloc[top_idx].values
            all_preds_2.extend(np.ones(len(selected)))
            all_actuals_2.extend(selected)
    
    if len(all_preds_2) > 0:
        precision_2 = sum(all_actuals_2) / len(all_preds_2)
        strategies_results.append(('ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«_ä¸Šä½2', precision_2, len(all_preds_2)))
        print(f"  ç²¾åº¦: {precision_2:.2%}, é¸æŠæ•°: {len(all_preds_2)}")
    
    # === æˆ¦ç•¥3: è¶…å³é¸1éŠ˜æŸ„ ===
    print("\nğŸ’ æˆ¦ç•¥3: è¶…å³é¸1éŠ˜æŸ„")
    
    all_preds_3 = []
    all_actuals_3 = []
    
    model3 = lgb.LGBMClassifier(
        n_estimators=200,
        max_depth=5,
        min_child_samples=10,
        subsample=0.9,
        colsample_bytree=0.9,
        learning_rate=0.05,
        random_state=42,
        verbose=-1
    )
    
    for test_date in test_dates:
        train = df_sorted[df_sorted['Date'] < test_date]
        test = df_sorted[df_sorted['Date'] == test_date]
        
        train_clean = train.dropna(subset=['Target'] + feature_cols)
        test_clean = test.dropna(subset=['Target'] + feature_cols)
        
        if len(train_clean) < 1000 or len(test_clean) < 1:
            continue
        
        X_train = train_clean[feature_cols]
        y_train = train_clean['Target']
        X_test = test_clean[feature_cols]
        y_test = test_clean['Target']
        
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model3.fit(X_train_scaled, y_train)
        probs = model3.predict_proba(X_test_scaled)[:, 1]
        
        # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„1éŠ˜æŸ„ã®ã¿é¸æŠ
        best_idx = np.argmax(probs)
        if probs[best_idx] >= 0.6:  # 60%ä»¥ä¸Šã®ç¢ºç‡ã®å ´åˆã®ã¿
            selected = [y_test.iloc[best_idx]]
            all_preds_3.extend([1])
            all_actuals_3.extend(selected)
    
    if len(all_preds_3) > 0:
        precision_3 = sum(all_actuals_3) / len(all_preds_3)
        strategies_results.append(('è¶…å³é¸_1éŠ˜æŸ„', precision_3, len(all_preds_3)))
        print(f"  ç²¾åº¦: {precision_3:.2%}, é¸æŠæ•°: {len(all_preds_3)}")
    
    # çµæœåˆ†æ
    print("\n" + "="*80)
    print("ğŸ¯ æœ€çµ‚60%çªç ´ãƒ—ãƒƒã‚·ãƒ¥çµæœ")
    print("="*80)
    
    print(f"{'æˆ¦ç•¥å':<20} {'ç²¾åº¦':<12} {'é¸æŠæ•°':<8} {'60%é”æˆ'}")
    print("-"*60)
    
    success_found = False
    best_result = None
    
    for name, precision, count in sorted(strategies_results, key=lambda x: x[1], reverse=True):
        status = "âœ… YES" if precision >= 0.60 else "âŒ NO"
        print(f"{name:<20} {precision:<12.2%} {count:<8d} {status}")
        
        if precision >= 0.60 and not success_found:
            success_found = True
            best_result = (name, precision, count)
    
    if success_found:
        print(f"\nğŸ‰ ã€60%ç²¾åº¦çªç ´æˆåŠŸï¼ã€‘")
        print(f"âœ… é”æˆç²¾åº¦: {best_result[1]:.2%}")
        print(f"âœ… æˆ¦ç•¥: {best_result[0]}")
        print(f"âœ… é¸æŠæ•°: {best_result[2]}éŠ˜æŸ„")
        
        # æˆåŠŸè¨˜éŒ²
        with open('final_60_push_success.txt', 'w') as f:
            f.write(f"60%ç²¾åº¦çªç ´æˆåŠŸï¼\n")
            f.write(f"é”æˆç²¾åº¦: {best_result[1]:.2%}\n")
            f.write(f"æˆ¦ç•¥: {best_result[0]}\n")
            f.write(f"é¸æŠæ•°: {best_result[2]}\n")
            f.write(f"é”æˆæ™‚åˆ»: {datetime.now()}\n")
            f.write(f"æ”¹è‰¯ç‚¹: 1%ä»¥ä¸Šä¸Šæ˜‡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ + {len(feature_cols)}ç‰¹å¾´é‡\n")
        
        print("ğŸ’¾ æˆåŠŸè¨˜éŒ²ä¿å­˜å®Œäº†")
        return True
        
    else:
        print(f"\nâš ï¸ ã€60%æœªé”æˆã€‘")
        if strategies_results:
            best = max(strategies_results, key=lambda x: x[1])
            print(f"æœ€é«˜ç²¾åº¦: {best[1]:.2%}")
            print(f"ç›®æ¨™ã¾ã§: +{0.60 - best[1]:.2%}")
            print(f"æœ€è‰¯æˆ¦ç•¥: {best[0]}")
        
        return False

if __name__ == "__main__":
    success = final_60_push()
    if success:
        print("\nğŸ‰ 60%ç²¾åº¦é”æˆæˆåŠŸï¼")
    else:
        print("\nâš ï¸ ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")