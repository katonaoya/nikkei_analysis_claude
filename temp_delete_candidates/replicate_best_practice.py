#!/usr/bin/env python3
"""
57.93%ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å†ç¾
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨˜è¼‰ã®æˆåŠŸæ‰‹æ³•ã‚’æ­£ç¢ºã«å†ç¾
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def replicate_best_practice():
    """57.93%ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å†ç¾"""
    
    print("ğŸ¯ 57.93%ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å†ç¾é–‹å§‹")
    print("ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨˜è¼‰ã®æˆåŠŸæ‰‹æ³•ã‚’æ­£ç¢ºã«å†ç¾")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_parquet('data/processed/integrated_with_external.parquet')
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}ä»¶")
    
    # ã‚«ãƒ©ãƒ èª¿æ•´
    if 'date' in df.columns:
        df['Date'] = pd.to_datetime(df['date'])
    if 'code' in df.columns:
        df['Stock'] = df['code']
    
    print("ğŸ”§ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ç‰¹å¾´é‡ç”Ÿæˆ...")
    print("ä½¿ç”¨ç‰¹å¾´é‡: RSI, Price_vs_MA5, Price_vs_MA20, Volatility, Volume_Ratio, Momentum_5, Momentum_20, Price_Position")
    
    # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ç‰¹å¾´é‡ç”Ÿæˆï¼ˆ8å€‹ï¼‰
    features = []
    for stock, stock_df in df.groupby('Stock'):
        if len(stock_df) < 30:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éŠ˜æŸ„ã®ã¿
            continue
            
        stock_df = stock_df.sort_values('Date')
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆç¿Œæ—¥ã®çµ‚å€¤ãŒå½“æ—¥ã‚ˆã‚Šé«˜ã„ï¼‰
        stock_df['Target'] = (stock_df['close'].shift(-1) > stock_df['close']).astype(int)
        
        # 1. RSI (14æ—¥)
        delta = stock_df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss.replace(0, 1)
        stock_df['RSI'] = 100 - (100 / (1 + rs))
        
        # 2. Price_vs_MA5 (5æ—¥ç§»å‹•å¹³å‡ä¹–é›¢ç‡)
        stock_df['MA5'] = stock_df['close'].rolling(5).mean()
        stock_df['Price_vs_MA5'] = (stock_df['close'] - stock_df['MA5']) / stock_df['MA5']
        
        # 3. Price_vs_MA20 (20æ—¥ç§»å‹•å¹³å‡ä¹–é›¢ç‡)
        stock_df['MA20'] = stock_df['close'].rolling(20).mean()
        stock_df['Price_vs_MA20'] = (stock_df['close'] - stock_df['MA20']) / stock_df['MA20']
        
        # 4. Volatility (20æ—¥ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£)
        stock_df['Return'] = stock_df['close'].pct_change()
        stock_df['Volatility'] = stock_df['Return'].rolling(20).std()
        
        # 5. Volume_Ratio (å‡ºæ¥é«˜æ¯”ç‡)
        stock_df['Volume_MA20'] = stock_df['volume'].rolling(20).mean()
        stock_df['Volume_Ratio'] = stock_df['volume'] / stock_df['Volume_MA20'].replace(0, 1)
        
        # 6. Momentum_5 (5æ—¥ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ )
        stock_df['Momentum_5'] = stock_df['close'].pct_change(5)
        
        # 7. Momentum_20 (20æ—¥ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ )
        stock_df['Momentum_20'] = stock_df['close'].pct_change(20)
        
        # 8. Price_Position (ä¾¡æ ¼å¸¯ã§ã®ä½ç½®)
        stock_df['High_20'] = stock_df['high'].rolling(20).max()
        stock_df['Low_20'] = stock_df['low'].rolling(20).min()
        stock_df['Price_Position'] = (stock_df['close'] - stock_df['Low_20']) / (stock_df['High_20'] - stock_df['Low_20'])
        
        features.append(stock_df)
    
    df = pd.concat(features, ignore_index=True)
    
    # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ç‰¹å¾´é‡ï¼ˆ8å€‹ï¼‰
    feature_cols = [
        'RSI',
        'Price_vs_MA5', 
        'Price_vs_MA20',
        'Volatility',
        'Volume_Ratio',
        'Momentum_5',
        'Momentum_20',
        'Price_Position'
    ]
    
    print(f"ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(feature_cols)}å€‹")
    
    # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãƒ¢ãƒ‡ãƒ«è¨­å®š
    model = lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=3,
        min_child_samples=20,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    )
    
    print("ğŸš€ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æˆ¦ç•¥å®Ÿè¡Œ: ä¸Šä½5éŠ˜æŸ„é¸æŠ")
    
    # æ™‚ç³»åˆ—ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆ30æ—¥é–“ï¼‰
    df_sorted = df.sort_values('Date')
    unique_dates = sorted(df_sorted['Date'].unique())
    test_dates = unique_dates[-30:]  # ç›´è¿‘30æ—¥é–“
    
    print(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {len(test_dates)}æ—¥é–“")
    
    all_predictions = []
    all_actuals = []
    daily_results = []
    
    for i, test_date in enumerate(test_dates):
        if i % 10 == 0:
            print(f"  é€²æ—: {i+1}/{len(test_dates)} ({(i+1)/len(test_dates)*100:.0f}%)")
        
        # 1. å½“æ—¥ã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        train_data = df_sorted[df_sorted['Date'] < test_date]
        test_data = df_sorted[df_sorted['Date'] == test_date]
        
        train_clean = train_data.dropna(subset=['Target'] + feature_cols)
        test_clean = test_data.dropna(subset=['Target'] + feature_cols)
        
        if len(train_clean) < 1000 or len(test_clean) < 5:
            continue
        
        X_train = train_clean[feature_cols]
        y_train = train_clean['Target']
        X_test = test_clean[feature_cols]
        y_test = test_clean['Target']
        
        # 2. ç‰¹å¾´é‡æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model.fit(X_train_scaled, y_train)
        
        # 4. äºˆæ¸¬ç¢ºç‡å–å¾—
        pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        # 5. ä¸Šä½5éŠ˜æŸ„é¸æŠï¼ˆãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰
        n_select = min(5, len(pred_proba))
        if n_select > 0:
            top_indices = np.argsort(pred_proba)[-n_select:]
            selected_actuals = y_test.iloc[top_indices].values
            selected_probs = pred_proba[top_indices]
            
            # çµæœè¨˜éŒ²
            all_predictions.extend(np.ones(len(selected_actuals)))
            all_actuals.extend(selected_actuals)
            
            # æ—¥åˆ¥çµæœ
            daily_precision = sum(selected_actuals) / len(selected_actuals) if len(selected_actuals) > 0 else 0
            daily_results.append({
                'date': test_date.strftime('%Y-%m-%d'),
                'selected': len(selected_actuals),
                'correct': sum(selected_actuals),
                'precision': daily_precision,
                'avg_prob': np.mean(selected_probs)
            })
    
    # æœ€çµ‚çµæœè¨ˆç®—
    if len(all_predictions) > 0:
        precision = sum([a for a, p in zip(all_actuals, all_predictions) if a == 1 and p == 1]) / len(all_predictions)
        
        print("\n" + "="*80)
        print("ğŸ¯ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å†ç¾çµæœ")
        print("="*80)
        print(f"ğŸ“Š æˆ¦ç•¥å: LightGBM + ä¸Šä½5éŠ˜æŸ„é¸æŠ")
        print(f"ğŸ“ˆ 1æ—¥å¹³å‡é¸æŠæ•°: {len(all_predictions)/len(test_dates):.1f}éŠ˜æŸ„")
        print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆæœŸé–“: ç›´è¿‘30æ—¥é–“")
        print(f"ğŸ“ ç·é¸æŠéŠ˜æŸ„æ•°: {len(all_predictions)}")
        print(f"âœ… æ­£è§£æ•°: {sum(all_actuals)}")
        print(f"ğŸ–ï¸ **ç²¾åº¦: {precision:.2%}**")
        
        # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æ¯”è¼ƒ
        target_precision = 0.5793  # 57.93%
        if precision >= target_precision:
            print(f"ğŸ‰ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é”æˆï¼ ({target_precision:.2%}ä»¥ä¸Š)")
        else:
            print(f"âš ï¸ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æœªé” (ç›®æ¨™: {target_precision:.2%})")
        
        # 60%é”æˆç¢ºèª
        if precision >= 0.60:
            print(f"ğŸš€ ã€60%ç²¾åº¦çªç ´æˆåŠŸï¼ã€‘")
            print(f"âœ… ç›®æ¨™ã‚¯ãƒªã‚¢: {precision:.2%} â‰¥ 60.00%")
            
            # æˆåŠŸè¨˜éŒ²
            with open('best_practice_60_success.txt', 'w') as f:
                f.write(f"60%ç²¾åº¦çªç ´æˆåŠŸï¼\n")
                f.write(f"é”æˆç²¾åº¦: {precision:.2%}\n")
                f.write(f"æˆ¦ç•¥: ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å†ç¾\n")
                f.write(f"é¸æŠæ•°: {len(all_predictions)}\n")
                f.write(f"é”æˆæ™‚åˆ»: {datetime.now()}\n")
                f.write(f"ä½¿ç”¨ç‰¹å¾´é‡: {', '.join(feature_cols)}\n")
            
            print("ğŸ’¾ æˆåŠŸè¨˜éŒ²ä¿å­˜å®Œäº†")
            return True
        else:
            print(f"âš ï¸ 60%æœªé”æˆ (ç›®æ¨™ã¾ã§: +{0.60 - precision:.2%})")
        
        # è©³ç´°åˆ†æ
        print(f"\nğŸ“Š è©³ç´°åˆ†æ:")
        successful_days = len([r for r in daily_results if r['precision'] > 0.5])
        print(f"æˆåŠŸæ—¥æ•°: {successful_days}/{len(daily_results)} ({successful_days/len(daily_results)*100:.1f}%)")
        avg_daily_precision = np.mean([r['precision'] for r in daily_results])
        print(f"æ—¥åˆ¥å¹³å‡ç²¾åº¦: {avg_daily_precision:.2%}")
        
        return precision >= 0.60
        
    else:
        print("âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: æœ‰åŠ¹ãªäºˆæ¸¬ãªã—")
        return False

if __name__ == "__main__":
    success = replicate_best_practice()
    if success:
        print("\nğŸ‰ 60%ç²¾åº¦é”æˆæˆåŠŸï¼")
    else:
        print("\nâš ï¸ ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")