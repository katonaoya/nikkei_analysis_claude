#!/usr/bin/env python3
"""
ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆã®å¾¹åº•çš„ãªåŸå› èª¿æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’åˆ†æã—ã€æ ¹æœ¬åŸå› ã‚’ç‰¹å®šã™ã‚‹
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from pathlib import Path
import logging
import json

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensivePriceInvestigator:
    """ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆã®åŒ…æ‹¬çš„èª¿æŸ»ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.reports_dir = Path("./production_reports")
        self.data_dir = Path("./data")
        self.results_dir = Path("./investigation_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # èª¿æŸ»å¯¾è±¡éŠ˜æŸ„ï¼ˆå•é¡Œã®å¤šã‹ã£ãŸéŠ˜æŸ„ï¼‰
        self.target_stocks = {
            "9984": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯G",
            "6758": "ã‚½ãƒ‹ãƒ¼G", 
            "7974": "ä»»å¤©å ‚",
            "4478": "ãƒ•ãƒªãƒ¼",
            "8035": "æ±äº¬ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ³",
            "6098": "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆHD",
            "7203": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š",
            "4519": "ä¸­å¤–è£½è–¬"
        }
        
    def load_all_data_sources(self) -> dict:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ” å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        data_sources = {}
        
        # 1. Enhanced J-Quantsãƒ‡ãƒ¼ã‚¿
        enhanced_files = list(self.data_dir.rglob("enhanced_jquants*.parquet"))
        if enhanced_files:
            latest_file = max(enhanced_files, key=lambda x: x.stat().st_mtime)
            df = pd.read_parquet(latest_file)
            df['Date'] = pd.to_datetime(df['Date'])
            df['Code'] = df['Code'].astype(str)
            data_sources['enhanced_jquants'] = {
                'data': df,
                'file': latest_file.name,
                'records': len(df),
                'date_range': f"{df['Date'].min().date()} ~ {df['Date'].max().date()}",
                'stocks': df['Code'].nunique()
            }
            logger.info(f"âœ… Enhanced J-Quants: {len(df):,}ä»¶ ({df['Date'].min().date()} ~ {df['Date'].max().date()})")
        
        # 2. Nikkei225 Fullãƒ‡ãƒ¼ã‚¿
        nikkei_files = list(self.data_dir.rglob("nikkei225_full*.parquet"))
        if nikkei_files:
            latest_file = max(nikkei_files, key=lambda x: x.stat().st_mtime)
            df = pd.read_parquet(latest_file)
            df['Date'] = pd.to_datetime(df['Date'])
            df['Code'] = df['Code'].astype(str)
            data_sources['nikkei225_full'] = {
                'data': df,
                'file': latest_file.name,
                'records': len(df),
                'date_range': f"{df['Date'].min().date()} ~ {df['Date'].max().date()}",
                'stocks': df['Code'].nunique()
            }
            logger.info(f"âœ… Nikkei225 Full: {len(df):,}ä»¶ ({df['Date'].min().date()} ~ {df['Date'].max().date()})")
        
        # 3. ãã®ä»–ã®parquetãƒ•ã‚¡ã‚¤ãƒ«
        other_parquets = list(self.data_dir.rglob("*.parquet"))
        for file in other_parquets:
            if 'enhanced_jquants' not in file.name and 'nikkei225_full' not in file.name:
                try:
                    df = pd.read_parquet(file)
                    if 'Date' in df.columns and 'Code' in df.columns:
                        df['Date'] = pd.to_datetime(df['Date'])
                        df['Code'] = df['Code'].astype(str)
                        key = file.stem
                        data_sources[key] = {
                            'data': df,
                            'file': file.name,
                            'records': len(df),
                            'date_range': f"{df['Date'].min().date()} ~ {df['Date'].max().date()}",
                            'stocks': df['Code'].nunique()
                        }
                        logger.info(f"âœ… {key}: {len(df):,}ä»¶")
                except Exception as e:
                    logger.warning(f"âš ï¸ {file.name} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return data_sources
    
    def analyze_single_stock_across_sources(self, code: str, target_date: str) -> dict:
        """å˜ä¸€éŠ˜æŸ„ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã§åˆ†æ"""
        logger.info(f"ğŸ” {self.target_stocks.get(code, code)} ({code}) - {target_date} è©³ç´°åˆ†æ")
        
        data_sources = self.load_all_data_sources()
        target_dt = pd.to_datetime(target_date)
        
        results = {
            'code': code,
            'company': self.target_stocks.get(code, 'ä¸æ˜'),
            'target_date': target_date,
            'data_sources': []
        }
        
        for source_name, source_info in data_sources.items():
            df = source_info['data']
            
            # è©²å½“éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            stock_data = df[df['Code'] == code]
            
            if stock_data.empty:
                results['data_sources'].append({
                    'source': source_name,
                    'status': 'no_stock_data',
                    'file': source_info['file']
                })
                continue
            
            # ç›®æ¨™æ—¥ä»˜è¿‘è¾ºã®ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
            date_variants = [
                target_dt,
                target_dt - timedelta(days=1),
                target_dt + timedelta(days=1),
                target_dt - timedelta(days=2),
                target_dt + timedelta(days=2)
            ]
            
            found_data = []
            for check_date in date_variants:
                day_data = stock_data[stock_data['Date'].dt.date == check_date.date()]
                if not day_data.empty:
                    row = day_data.iloc[-1]
                    found_data.append({
                        'date': row['Date'],
                        'open': row.get('Open', 'N/A'),
                        'high': row.get('High', 'N/A'),
                        'low': row.get('Low', 'N/A'),
                        'close': row.get('Close', 'N/A'),
                        'volume': row.get('Volume', 'N/A'),
                        'days_from_target': (check_date.date() - target_dt.date()).days
                    })
            
            if found_data:
                results['data_sources'].append({
                    'source': source_name,
                    'status': 'found',
                    'file': source_info['file'],
                    'price_data': found_data,
                    'total_records': len(stock_data),
                    'date_range': f"{stock_data['Date'].min().date()} ~ {stock_data['Date'].max().date()}"
                })
            else:
                # æœ€è¿‘æ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                if not stock_data.empty:
                    nearest = stock_data.iloc[(stock_data['Date'] - target_dt).abs().argsort()[:1]]
                    if not nearest.empty:
                        row = nearest.iloc[0]
                        results['data_sources'].append({
                            'source': source_name,
                            'status': 'nearest_only',
                            'file': source_info['file'],
                            'nearest_data': {
                                'date': row['Date'],
                                'close': row.get('Close', 'N/A'),
                                'days_diff': abs((row['Date'] - target_dt).days)
                            },
                            'total_records': len(stock_data)
                        })
        
        return results
    
    def extract_report_price_with_context(self, report_file: Path) -> dict:
        """ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ä¾¡æ ¼ã¨ãã®æ–‡è„ˆã‚’è©³ç´°æŠ½å‡º"""
        try:
            content = report_file.read_text(encoding='utf-8')
            
            # ãƒ¬ãƒãƒ¼ãƒˆæ—¥ä»˜æŠ½å‡º
            date_match = re.search(r'(\d{8})', report_file.name)
            if date_match:
                report_date = datetime.strptime(date_match.group(1), "%Y%m%d").date()
            else:
                return {'error': 'date_not_found'}
            
            # ç”Ÿæˆæ™‚åˆ»æŠ½å‡º
            generation_time_match = re.search(r'ç”Ÿæˆæ™‚åˆ».*?(\d{4}-\d{2}-\d{2} \d{2}:\d{2})', content)
            generation_time = generation_time_match.group(1) if generation_time_match else None
            
            # å¯¾è±¡æ—¥æŠ½å‡º
            target_date_match = re.search(r'äºˆæ¸¬å¯¾è±¡æ—¥.*?(\d{4})å¹´(\d{2})æœˆ(\d{2})æ—¥', content)
            target_date = None
            if target_date_match:
                year, month, day = target_date_match.groups()
                target_date = datetime(int(year), int(month), int(day)).date()
            
            # éŠ˜æŸ„ä¾¡æ ¼æŠ½å‡ºï¼ˆè©³ç´°ç‰ˆï¼‰
            stock_prices = []
            
            # TOP3æ¨å¥¨éŠ˜æŸ„ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            top3_pattern = r'### (\d+)\.\s*ã€.*?ã€‘\s*(.+?)\s*\((\d{4})\).*?\n(.*?)\n.*?ç¾åœ¨ä¾¡æ ¼.*?(\d{1,3}(?:,\d{3})*(?:\.\d+)?)å††.*?\n.*?ä¸Šæ˜‡ç¢ºç‡.*?(\d+\.\d+)%'
            
            for match in re.finditer(top3_pattern, content, re.MULTILINE | re.DOTALL):
                rank, company, code, context, price_str, probability = match.groups()
                
                # æŠ€è¡“æŒ‡æ¨™æŠ½å‡º
                tech_section = content[match.end():match.end()+500]
                tech_indicators = re.findall(r'- (.+)', tech_section)
                
                stock_prices.append({
                    'rank': int(rank),
                    'company': company.strip(),
                    'code': code,
                    'price': float(price_str.replace(',', '')),
                    'probability': float(probability),
                    'context': context.strip(),
                    'technical_indicators': tech_indicators[:5]  # æœ€åˆã®5å€‹
                })
            
            return {
                'report_file': report_file.name,
                'report_date': report_date,
                'target_date': target_date,
                'generation_time': generation_time,
                'stock_prices': stock_prices,
                'full_content_length': len(content)
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def deep_dive_price_discrepancies(self) -> dict:
        """ä¾¡æ ¼å·®ç•°ã®æ·±æ˜ã‚Šåˆ†æ"""
        logger.info("ğŸš€ ä¾¡æ ¼å·®ç•°ã®æ·±æ˜ã‚Šåˆ†æé–‹å§‹...")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã§è©³ç´°åˆ†æ
        sample_reports = [
            "20250801_prediction_report.md",
            "20250802_prediction_report.md", 
            "20250804_prediction_report.md"
        ]
        
        investigation_results = []
        
        for report_name in sample_reports:
            report_file = self.reports_dir / report_name
            if not report_file.exists():
                continue
            
            logger.info(f"ğŸ“‹ {report_name} è©³ç´°åˆ†æä¸­...")
            
            # ãƒ¬ãƒãƒ¼ãƒˆæƒ…å ±æŠ½å‡º
            report_info = self.extract_report_price_with_context(report_file)
            if 'error' in report_info:
                continue
            
            # å„éŠ˜æŸ„ã‚’è©³ç´°åˆ†æ
            stock_analyses = []
            
            for stock_price in report_info['stock_prices']:
                code = stock_price['code']
                
                if code in self.target_stocks:
                    # è¤‡æ•°æ—¥ä»˜ã§ã®åˆ†æ
                    analysis_dates = []
                    
                    if report_info['target_date']:
                        analysis_dates.append(report_info['target_date'].strftime('%Y-%m-%d'))
                    if report_info['report_date']:
                        analysis_dates.append(report_info['report_date'].strftime('%Y-%m-%d'))
                    
                    # å‰å¾Œã®æ—¥ä»˜ã‚‚è¿½åŠ 
                    base_date = report_info['report_date'] if report_info['report_date'] else report_info['target_date']
                    if base_date:
                        for offset in [-2, -1, 0, 1, 2]:
                            check_date = base_date + timedelta(days=offset)
                            analysis_dates.append(check_date.strftime('%Y-%m-%d'))
                    
                    # é‡è¤‡é™¤å»
                    analysis_dates = list(set(analysis_dates))
                    
                    date_analyses = []
                    for date_str in analysis_dates[:3]:  # æœ€å¤§3æ—¥åˆ†
                        data_analysis = self.analyze_single_stock_across_sources(code, date_str)
                        date_analyses.append(data_analysis)
                    
                    stock_analyses.append({
                        'report_stock_info': stock_price,
                        'multi_date_analysis': date_analyses
                    })
            
            investigation_results.append({
                'report_info': report_info,
                'stock_analyses': stock_analyses
            })
        
        return {
            'investigation_date': datetime.now().isoformat(),
            'analyzed_reports': len(investigation_results),
            'results': investigation_results
        }
    
    def identify_root_cause(self, investigation_data: dict) -> dict:
        """æ ¹æœ¬åŸå› ã®ç‰¹å®š"""
        logger.info("ğŸ”¬ æ ¹æœ¬åŸå› ç‰¹å®šåˆ†æä¸­...")
        
        patterns = {
            'price_scale_issues': [],
            'date_mismatches': [],
            'data_source_inconsistencies': [],
            'potential_mock_data': [],
            'adjustment_factor_issues': []
        }
        
        for result in investigation_data['results']:
            report_info = result['report_info']
            
            for stock_analysis in result['stock_analyses']:
                report_price = stock_analysis['report_stock_info']['price']
                code = stock_analysis['report_stock_info']['code']
                company = stock_analysis['report_stock_info']['company']
                
                # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã§ã®ä¾¡æ ¼æ¯”è¼ƒ
                for date_analysis in stock_analysis['multi_date_analysis']:
                    for source_data in date_analysis['data_sources']:
                        if source_data['status'] == 'found':
                            for price_data in source_data['price_data']:
                                actual_close = price_data.get('close')
                                if actual_close and actual_close != 'N/A':
                                    try:
                                        actual_close = float(actual_close)
                                        diff_pct = ((report_price - actual_close) / actual_close) * 100
                                        
                                        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                                        if abs(diff_pct) > 50:
                                            patterns['price_scale_issues'].append({
                                                'company': company,
                                                'code': code,
                                                'report_price': report_price,
                                                'actual_price': actual_close,
                                                'diff_pct': diff_pct,
                                                'source': source_data['source'],
                                                'date': price_data['date']
                                            })
                                        
                                        # ç‰¹å®šã®å€æ•°é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
                                        ratios = [0.1, 0.5, 2.0, 10.0, 100.0, 1000.0]
                                        for ratio in ratios:
                                            if abs(report_price / actual_close - ratio) < 0.05:
                                                patterns['adjustment_factor_issues'].append({
                                                    'company': company,
                                                    'code': code,
                                                    'ratio': ratio,
                                                    'report_price': report_price,
                                                    'actual_price': actual_close
                                                })
                                    except:
                                        pass
        
        # æ ¹æœ¬åŸå› ã®æ¨å®š
        root_causes = []
        
        if len(patterns['price_scale_issues']) > 5:
            root_causes.append({
                'cause': 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é–“ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¸æ•´åˆ',
                'evidence_count': len(patterns['price_scale_issues']),
                'confidence': 'é«˜',
                'description': 'ãƒ¬ãƒãƒ¼ãƒˆã¨å®Ÿãƒ‡ãƒ¼ã‚¿ã§ä¾¡æ ¼ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§å¹…ã«ç•°ãªã‚‹'
            })
        
        if len(patterns['adjustment_factor_issues']) > 3:
            root_causes.append({
                'cause': 'æ ªä¾¡èª¿æ•´ä¿‚æ•°ã®å•é¡Œ',
                'evidence_count': len(patterns['adjustment_factor_issues']),
                'confidence': 'ä¸­',
                'description': 'æ ªå¼åˆ†å‰²ç­‰ã®èª¿æ•´ãŒæ­£ã—ãå‡¦ç†ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§'
            })
        
        return {
            'patterns': patterns,
            'root_causes': root_causes,
            'recommendations': self.generate_recommendations(patterns, root_causes)
        }
    
    def generate_recommendations(self, patterns: dict, root_causes: list) -> list:
        """ä¿®æ­£æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        if any('ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é–“ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¸æ•´åˆ' in cause['cause'] for cause in root_causes):
            recommendations.append({
                'priority': 'æœ€é«˜',
                'action': 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±ä¸€',
                'description': 'å…¨ã¦ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å˜ä¸€ã®ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ã«çµ±ä¸€ã™ã‚‹',
                'implementation': '1. æœ€ã‚‚æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ç‰¹å®š 2. å…¨ã‚·ã‚¹ãƒ†ãƒ ã§çµ±ä¸€ä½¿ç”¨'
            })
        
        recommendations.append({
            'priority': 'é«˜',
            'action': 'ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰',
            'description': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¸‚å ´ä¾¡æ ¼ã¨ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯',
            'implementation': '1. å¤–éƒ¨ä¾¡æ ¼APIé€£æº 2. è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½'
        })
        
        recommendations.append({
            'priority': 'ä¸­',
            'action': 'ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå†å®Ÿè¡Œ',
            'description': 'æ­£ç¢ºãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å†å®Ÿè¡Œ',
            'implementation': '1. ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ã§ã®å®Œå…¨å†è¨ˆç®— 2. çµæœæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä½œæˆ'
        })
        
        return recommendations
    
    def run_comprehensive_investigation(self):
        """åŒ…æ‹¬çš„èª¿æŸ»ã®å®Ÿè¡Œ"""
        logger.info("ğŸ” åŒ…æ‹¬çš„ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿èª¿æŸ»é–‹å§‹...")
        
        # æ·±æ˜ã‚Šåˆ†æå®Ÿè¡Œ
        investigation_data = self.deep_dive_price_discrepancies()
        
        # æ ¹æœ¬åŸå› ç‰¹å®š
        root_cause_analysis = self.identify_root_cause(investigation_data)
        
        # çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è©³ç´°çµæœä¿å­˜
        investigation_file = self.results_dir / f"price_investigation_{timestamp}.json"
        with open(investigation_file, 'w', encoding='utf-8') as f:
            json.dump({
                'investigation_data': investigation_data,
                'root_cause_analysis': root_cause_analysis
            }, f, ensure_ascii=False, indent=2, default=str)
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        self.print_investigation_summary(root_cause_analysis)
        
        logger.info(f"ğŸ’¾ è©³ç´°èª¿æŸ»çµæœä¿å­˜: {investigation_file}")
        
        return root_cause_analysis
    
    def print_investigation_summary(self, analysis: dict):
        """èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›"""
        print("\n" + "="*80)
        print("ğŸ”¬ ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ - æ ¹æœ¬åŸå› èª¿æŸ»çµæœ")
        print("="*80)
        
        print(f"\nğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³:")
        patterns = analysis['patterns']
        for pattern_name, issues in patterns.items():
            if issues:
                print(f"   {pattern_name}: {len(issues)}ä»¶")
        
        print(f"\nğŸ¯ ç‰¹å®šã•ã‚ŒãŸæ ¹æœ¬åŸå› :")
        for i, cause in enumerate(analysis['root_causes'], 1):
            print(f"   {i}. {cause['cause']}")
            print(f"      è¨¼æ‹ æ•°: {cause['evidence_count']}ä»¶")
            print(f"      ä¿¡é ¼åº¦: {cause['confidence']}")
            print(f"      èª¬æ˜: {cause['description']}")
        
        print(f"\nğŸ› ï¸ ä¿®æ­£æ¨å¥¨äº‹é …:")
        for i, rec in enumerate(analysis['recommendations'], 1):
            print(f"   {i}. [{rec['priority']}] {rec['action']}")
            print(f"      {rec['description']}")
            print(f"      å®Ÿè£…: {rec['implementation']}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    investigator = ComprehensivePriceInvestigator()
    investigator.run_comprehensive_investigation()

if __name__ == "__main__":
    main()