#!/usr/bin/env python3
"""
é«˜é€Ÿä¿¡é ¼æ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
83.33%ç²¾åº¦ã®å®Ÿé‹ç”¨ä¿¡é ¼æ€§ã‚’è¿…é€Ÿã«æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import lightgbm as lgb
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

from yahoo_market_data import YahooMarketData
from loguru import logger

def quick_reliability_test():
    """é«˜é€Ÿä¿¡é ¼æ€§æ¤œè¨¼"""
    
    logger.info("ğŸ” 83.33%ç²¾åº¦ é«˜é€Ÿä¿¡é ¼æ€§æ¤œè¨¼é–‹å§‹")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = pd.read_parquet('data/processed/integrated_with_external.parquet')
        
        if 'date' in df.columns:
            df['Date'] = pd.to_datetime(df['date'])
        if 'code' in df.columns:
            df['Stock'] = df['code'].astype(str)
        
        # é«˜å“è³ªéŠ˜æŸ„é¸æŠ
        stock_counts = df['Stock'].value_counts()
        quality_stocks = stock_counts[stock_counts >= 300].head(80).index.tolist()
        df = df[df['Stock'].isin(quality_stocks)].copy()
        
        # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆè»½é‡ç‰ˆï¼‰
        market_data = YahooMarketData()
        data_dict = market_data.get_all_market_data(period="1y")  # è»½é‡åŒ–
        
        if data_dict:
            market_features = market_data.calculate_market_features(data_dict)
            if not market_features.empty:
                # æ—¥ä»˜çµ±ä¸€
                df['Date'] = pd.to_datetime(df['Date']).dt.date
                market_features['Date'] = pd.to_datetime(market_features['Date'], utc=True).dt.date
                
                df = df.merge(market_features, on='Date', how='left')
                
                # ãƒãƒ¼ã‚±ãƒƒãƒˆç‰¹å¾´é‡ã®ã¿ä½¿ç”¨
                market_feature_cols = [col for col in market_features.columns 
                                     if col != 'Date' and not col.endswith('_volume')]
                
                # æ¬ æå€¤å‡¦ç†
                for col in market_feature_cols:
                    if col in df.columns:
                        df[col] = df[col].fillna(method='ffill').fillna(0)
                
                logger.success(f"âœ… ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆ: {len(market_feature_cols)}ç‰¹å¾´é‡")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ
        df = df.sort_values(['Stock', 'Date'])
        df['next_high'] = df.groupby('Stock')['high'].shift(-1)
        df['Target'] = (df['next_high'] > df['close'] * 1.01).astype(int)
        
        # ãƒ†ã‚¹ãƒˆæœŸé–“ï¼ˆæœ€æ–°15æ—¥ã§é«˜é€ŸåŒ–ï¼‰
        df_sorted = df.sort_values(['Stock', 'Date'])
        unique_dates = sorted(df_sorted['Date'].unique())
        test_dates = unique_dates[-15:]  # é«˜é€ŸåŒ–
        
        logger.info(f"é«˜é€Ÿæ¤œè¨¼æœŸé–“: {len(test_dates)}æ—¥")
        
        # è¤‡æ•°æœŸé–“ã§ã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
        stability_results = {}
        
        test_periods = [5, 8, 10]  # è»½é‡åŒ–
        
        for period in test_periods:
            logger.info(f"ğŸ“Š {period}æ—¥é–“ã§ã®å®‰å®šæ€§æ¤œè¨¼")
            
            period_test_dates = test_dates[-period:]
            predictions = []
            actuals = []
            
            for test_date in period_test_dates:
                train = df_sorted[df_sorted['Date'] < test_date]
                test = df_sorted[df_sorted['Date'] == test_date]
                
                train_clean = train.dropna(subset=['Target'] + market_feature_cols[:15])  # è»½é‡åŒ–
                test_clean = test.dropna(subset=['Target'] + market_feature_cols[:15])
                
                if len(train_clean) < 500 or len(test_clean) < 1:
                    continue
                
                # ç‰¹å¾´é‡é¸æŠ
                available_features = [col for col in market_feature_cols[:15] 
                                    if col in train_clean.columns]
                
                X_train = train_clean[available_features]
                y_train = train_clean['Target']
                X_test = test_clean[available_features]
                y_test = test_clean['Target']
                
                # ç‰¹å¾´é‡é¸æŠï¼ˆä¸Šä½8å€‹ï¼‰
                selector = SelectKBest(score_func=f_classif, k=min(8, len(available_features)))
                X_train_selected = selector.fit_transform(X_train, y_train)
                X_test_selected = selector.transform(X_test)
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train_selected)
                X_test_scaled = scaler.transform(X_test_selected)
                
                # LightGBMãƒ¢ãƒ‡ãƒ«ï¼ˆé«˜ç²¾åº¦è¨­å®šï¼‰
                model = lgb.LGBMClassifier(
                    n_estimators=120,
                    max_depth=4,
                    min_child_samples=10,
                    subsample=0.85,
                    colsample_bytree=0.85,
                    learning_rate=0.08,
                    random_state=42,
                    verbose=-1
                )
                
                model.fit(X_train_scaled, y_train)
                probs = model.predict_proba(X_test_scaled)[:, 1]
                
                # ä¸Šä½3éŠ˜æŸ„é¸æŠï¼ˆ83.33%å®Ÿç¾æˆ¦ç•¥ï¼‰
                if len(probs) >= 3:
                    top_indices = np.argsort(probs)[-3:]
                    selected_actuals = y_test.iloc[top_indices].values
                    predictions.extend([1] * 3)
                    actuals.extend(selected_actuals)
            
            if predictions:
                precision = sum(actuals) / len(actuals)
                stability_results[f'{period}æ—¥é–“'] = {
                    'precision': precision,
                    'count': len(actuals),
                    'success_rate': sum(actuals)
                }
                logger.info(f"  {period}æ—¥é–“: {precision:.2%} ({sum(actuals)}/{len(actuals)})")
        
        # å¸‚å ´ç’°å¢ƒåˆ¥æ¤œè¨¼ï¼ˆVIXæ°´æº–ã§ã®åˆ†æï¼‰
        logger.info("ğŸ“Š å¸‚å ´ç’°å¢ƒåˆ¥ä¿¡é ¼æ€§æ¤œè¨¼")
        
        # VIXæ°´æº–ã§å¸‚å ´ç’°å¢ƒåˆ†é¡
        if 'vix_close' in df.columns:
            df['VIX_Level'] = pd.cut(df['vix_close'], 
                                   bins=[0, 15, 25, 100], 
                                   labels=['Low_VIX', 'Medium_VIX', 'High_VIX'])
            
            environment_results = {}
            
            for env in ['Low_VIX', 'Medium_VIX', 'High_VIX']:
                env_dates = df[df['VIX_Level'] == env]['Date'].unique()
                env_test_dates = [d for d in test_dates[-8:] if d in env_dates][:5]  # è»½é‡åŒ–
                
                if len(env_test_dates) < 2:
                    continue
                
                env_predictions = []
                env_actuals = []
                
                for test_date in env_test_dates:
                    train = df_sorted[df_sorted['Date'] < test_date]
                    test = df_sorted[df_sorted['Date'] == test_date]
                    
                    train_clean = train.dropna(subset=['Target'] + market_feature_cols[:10])
                    test_clean = test.dropna(subset=['Target'] + market_feature_cols[:10])
                    
                    if len(train_clean) < 300 or len(test_clean) < 1:
                        continue
                    
                    available_features = [col for col in market_feature_cols[:10] 
                                        if col in train_clean.columns]
                    
                    X_train = train_clean[available_features]
                    y_train = train_clean['Target']
                    X_test = test_clean[available_features]
                    y_test = test_clean['Target']
                    
                    # è»½é‡ãƒ¢ãƒ‡ãƒ«
                    model = lgb.LGBMClassifier(
                        n_estimators=80,
                        max_depth=3,
                        learning_rate=0.1,
                        random_state=42,
                        verbose=-1
                    )
                    
                    scaler = RobustScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    
                    model.fit(X_train_scaled, y_train)
                    probs = model.predict_proba(X_test_scaled)[:, 1]
                    
                    # ä¸Šä½2éŠ˜æŸ„é¸æŠ
                    if len(probs) >= 2:
                        top_indices = np.argsort(probs)[-2:]
                        selected_actuals = y_test.iloc[top_indices].values
                        env_predictions.extend([1] * 2)
                        env_actuals.extend(selected_actuals)
                
                if env_predictions:
                    env_precision = sum(env_actuals) / len(env_actuals)
                    environment_results[env] = {
                        'precision': env_precision,
                        'count': len(env_actuals),
                        'success_rate': sum(env_actuals)
                    }
                    logger.info(f"  {env}: {env_precision:.2%} ({sum(env_actuals)}/{len(env_actuals)})")
        
        # çµ±è¨ˆçš„ä¿¡é ¼åŒºé–“ï¼ˆãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼‰
        logger.info("ğŸ“Š çµ±è¨ˆçš„ä¿¡é ¼æ€§æ¤œè¨¼")
        
        # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿
        all_predictions = []
        all_actuals = []
        
        for test_date in test_dates[-10:]:  # è»½é‡åŒ–
            train = df_sorted[df_sorted['Date'] < test_date]
            test = df_sorted[df_sorted['Date'] == test_date]
            
            train_clean = train.dropna(subset=['Target'] + market_feature_cols[:12])
            test_clean = test.dropna(subset=['Target'] + market_feature_cols[:12])
            
            if len(train_clean) < 500 or len(test_clean) < 1:
                continue
            
            available_features = [col for col in market_feature_cols[:12] 
                                if col in train_clean.columns]
            
            X_train = train_clean[available_features]
            y_train = train_clean['Target']
            X_test = test_clean[available_features]
            y_test = test_clean['Target']
            
            # æœ€é©ãƒ¢ãƒ‡ãƒ«
            model = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=4,
                min_child_samples=8,
                subsample=0.9,
                colsample_bytree=0.8,
                learning_rate=0.08,
                random_state=42,
                verbose=-1
            )
            
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            model.fit(X_train_scaled, y_train)
            probs = model.predict_proba(X_test_scaled)[:, 1]
            
            # ä¸Šä½3éŠ˜æŸ„é¸æŠ
            if len(probs) >= 3:
                top_indices = np.argsort(probs)[-3:]
                selected_actuals = y_test.iloc[top_indices].values
                all_predictions.extend([1] * 3)
                all_actuals.extend(selected_actuals)
        
        # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ä¿¡é ¼åŒºé–“
        if all_actuals:
            bootstrap_precisions = []
            n_bootstrap = 500  # è»½é‡åŒ–
            
            for _ in range(n_bootstrap):
                bootstrap_sample = np.random.choice(all_actuals, size=len(all_actuals), replace=True)
                bootstrap_precision = np.mean(bootstrap_sample)
                bootstrap_precisions.append(bootstrap_precision)
            
            bootstrap_precisions = np.array(bootstrap_precisions)
            mean_precision = np.mean(bootstrap_precisions)
            ci_lower = np.percentile(bootstrap_precisions, 2.5)
            ci_upper = np.percentile(bootstrap_precisions, 97.5)
            
            logger.info(f"çµ±è¨ˆçš„ä¿¡é ¼åŒºé–“: {ci_lower:.2%} - {ci_upper:.2%}")
        
        # === æœ€çµ‚çµæœè¡¨ç¤º ===
        print("\n" + "="*60)
        print("ğŸ” 83.33%ç²¾åº¦ é«˜é€Ÿä¿¡é ¼æ€§æ¤œè¨¼çµæœ")
        print("="*60)
        
        print(f"\nã€1. æ™‚ç³»åˆ—å®‰å®šæ€§ã€‘")
        for period, result in stability_results.items():
            stability_status = "ğŸŸ¢ å®‰å®š" if result['precision'] >= 0.75 else "ğŸŸ¡ æ³¨æ„" if result['precision'] >= 0.65 else "ğŸ”´ ä¸å®‰å®š"
            print(f"  {period}: {result['precision']:.2%} ({result['count']}å›) {stability_status}")
        
        if environment_results:
            print(f"\nã€2. å¸‚å ´ç’°å¢ƒåˆ¥ä¿¡é ¼æ€§ã€‘")
            for env, result in environment_results.items():
                env_status = "ğŸŸ¢ é«˜ä¿¡é ¼" if result['precision'] >= 0.70 else "ğŸŸ¡ ä¸­ä¿¡é ¼" if result['precision'] >= 0.60 else "ğŸ”´ ä½ä¿¡é ¼"
                print(f"  {env}: {result['precision']:.2%} ({result['count']}å›) {env_status}")
        
        if all_actuals:
            overall_precision = sum(all_actuals) / len(all_actuals)
            print(f"\nã€3. çµ±è¨ˆçš„ä¿¡é ¼æ€§ã€‘")
            print(f"  å®Ÿæ¸¬ç²¾åº¦: {overall_precision:.2%} ({sum(all_actuals)}/{len(all_actuals)})")
            print(f"  95%ä¿¡é ¼åŒºé–“: {ci_lower:.2%} - {ci_upper:.2%}")
            
            # ä¿¡é ¼åŒºé–“ã®å¹…
            ci_width = ci_upper - ci_lower
            ci_status = "ğŸŸ¢ é«˜ç²¾åº¦" if ci_width <= 0.15 else "ğŸŸ¡ ä¸­ç²¾åº¦" if ci_width <= 0.25 else "ğŸ”´ ä½ç²¾åº¦"
            print(f"  ä¿¡é ¼åŒºé–“å¹…: Â±{ci_width/2:.2%} {ci_status}")
        
        # === æœ€çµ‚åˆ¤å®š ===
        print(f"\nã€4. å®Ÿé‹ç”¨ä¿¡é ¼æ€§åˆ¤å®šã€‘")
        
        reliable_periods = sum(1 for r in stability_results.values() if r['precision'] >= 0.70)
        total_periods = len(stability_results)
        
        if reliable_periods >= total_periods * 0.8:
            stability_verdict = "ğŸŸ¢ é«˜å®‰å®š"
        elif reliable_periods >= total_periods * 0.6:
            stability_verdict = "ğŸŸ¡ ä¸­å®‰å®š"
        else:
            stability_verdict = "ğŸ”´ ä¸å®‰å®š"
        
        print(f"  æ™‚ç³»åˆ—å®‰å®šæ€§: {reliable_periods}/{total_periods}æœŸé–“ã§70%+ {stability_verdict}")
        
        if environment_results:
            reliable_envs = sum(1 for r in environment_results.values() if r['precision'] >= 0.65)
            total_envs = len(environment_results)
            
            if reliable_envs >= total_envs * 0.75:
                env_verdict = "ğŸŸ¢ ç’°å¢ƒé ‘å¥"
            elif reliable_envs >= total_envs * 0.5:
                env_verdict = "ğŸŸ¡ ç’°å¢ƒä¾å­˜"
            else:
                env_verdict = "ğŸ”´ ç’°å¢ƒè„†å¼±"
            
            print(f"  ç’°å¢ƒé ‘å¥æ€§: {reliable_envs}/{total_envs}ç’°å¢ƒã§65%+ {env_verdict}")
        
        if all_actuals:
            if ci_lower >= 0.70:
                statistical_verdict = "ğŸŸ¢ çµ±è¨ˆçš„ä¿¡é ¼"
            elif ci_lower >= 0.60:
                statistical_verdict = "ğŸŸ¡ çµ±è¨ˆçš„æ³¨æ„"
            else:
                statistical_verdict = "ğŸ”´ çµ±è¨ˆçš„ä¸å®‰"
            
            print(f"  çµ±è¨ˆçš„ä¿¡é ¼æ€§: ä¸‹é™{ci_lower:.2%} {statistical_verdict}")
        
        # ç·åˆåˆ¤å®š
        print(f"\nã€5. ç·åˆå®Ÿé‹ç”¨æ¨å¥¨åº¦ã€‘")
        
        if (reliable_periods >= total_periods * 0.8 and 
            (not environment_results or reliable_envs >= total_envs * 0.75) and 
            (not all_actuals or ci_lower >= 0.65)):
            
            final_verdict = "ğŸŸ¢ å®Ÿé‹ç”¨æ¨å¥¨"
            recommendation = "é«˜ã„ä¿¡é ¼æ€§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚å®Ÿé‹ç”¨å¯èƒ½ã§ã™ã€‚"
            
        elif (reliable_periods >= total_periods * 0.6 and 
              (not all_actuals or ci_lower >= 0.55)):
            
            final_verdict = "ğŸŸ¡ æ¡ä»¶ä»˜ãæ¨å¥¨"
            recommendation = "åŸºæœ¬çš„ã«ä¿¡é ¼ã§ãã¾ã™ãŒã€å¸‚å ´ç’°å¢ƒã®å¤‰åŒ–ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
            
        else:
            final_verdict = "ğŸ”´ å®Ÿé‹ç”¨æ³¨æ„"
            recommendation = "ã•ã‚‰ãªã‚‹æ”¹å–„ã¨æ¤œè¨¼ãŒå¿…è¦ã§ã™ã€‚"
        
        print(f"  ç·åˆåˆ¤å®š: {final_verdict}")
        print(f"  æ¨å¥¨äº‹é …: {recommendation}")
        
        print("="*60)
        
        return final_verdict.startswith("ğŸŸ¢") or final_verdict.startswith("ğŸŸ¡")
        
    except Exception as e:
        logger.error(f"âŒ ä¿¡é ¼æ€§æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    reliable = quick_reliability_test()
    
    if reliable:
        print("\nâœ… 83.33%ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ ã®ä¿¡é ¼æ€§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸï¼")
    else:
        print("\nâš ï¸ ã•ã‚‰ãªã‚‹æ”¹å–„ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚")