"""
ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œç‰ˆJ-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
APIåˆ¶é™ã‚’å›é¿ã—ãªãŒã‚‰å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import os
import time
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime, date, timedelta
from pathlib import Path
import logging
import random

import pandas as pd
import requests
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# APIè¨­å®š
JQUANTS_BASE_URL = "https://api.jquants.com/v1"


class RateLimitedJQuantsFetcher:
    """APIåˆ¶é™ã‚’å›é¿ã—ãªãŒã‚‰å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        self.last_token_request: Optional[datetime] = None
        self.api_call_count = 0
        self.last_reset_time = datetime.now()
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        logger.info("ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œç‰ˆJ-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
    
    def _wait_for_rate_limit(self, min_interval: float = 2.0):
        """APIåˆ¶é™å›é¿ã®ãŸã‚ã®å¾…æ©Ÿ"""
        if self.last_token_request:
            elapsed = (datetime.now() - self.last_token_request).total_seconds()
            if elapsed < min_interval:
                wait_time = min_interval - elapsed + random.uniform(0.5, 1.5)  # ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ è¿½åŠ 
                logger.info(f"APIåˆ¶é™å›é¿ã®ãŸã‚ {wait_time:.1f}ç§’å¾…æ©Ÿ...")
                time.sleep(wait_time)
    
    def _get_id_token(self) -> str:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œç‰ˆIDãƒˆãƒ¼ã‚¯ãƒ³å–å¾—"""
        # ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if (self.id_token and self.token_expires_at and 
            datetime.now() < self.token_expires_at - timedelta(minutes=5)):  # 5åˆ†å‰ã«æ›´æ–°
            return self.id_token
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        self._wait_for_rate_limit(min_interval=3.0)  # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯3ç§’é–“éš”
        
        logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ä¸­...")
        
        try:
            # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            auth_payload = {
                "mailaddress": self.mail_address,
                "password": self.password
            }
            
            self.last_token_request = datetime.now()
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_user",
                data=json.dumps(auth_payload),
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Šé•·æ™‚é–“å¾…æ©Ÿã—ã¾ã™...")
                time.sleep(60)  # 1åˆ†å¾…æ©Ÿ
                return self._get_id_token()  # å†è©¦è¡Œ
                
            resp.raise_for_status()
            refresh_token = resp.json().get("refreshToken")
            
            if not refresh_token:
                raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            logger.info("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            
            # IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆå°‘ã—å¾…æ©Ÿï¼‰
            time.sleep(1.0)
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Šé•·æ™‚é–“å¾…æ©Ÿã—ã¾ã™...")
                time.sleep(60)
                return self._get_id_token()  # å†è©¦è¡Œ
                
            resp.raise_for_status()
            self.id_token = resp.json().get("idToken")
            
            if not self.id_token:
                raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æœŸé™ã‚’è¨­å®š
            self.token_expires_at = datetime.now() + timedelta(hours=1)
            
            logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            return self.id_token
            
        except Exception as e:
            if "429" in str(e):
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã€é•·æ™‚é–“å¾…æ©Ÿå¾Œå†è©¦è¡Œ...")
                time.sleep(120)  # 2åˆ†å¾…æ©Ÿ
                return self._get_id_token()
            logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def get_daily_quotes_safe(
        self, 
        code: str,
        from_date: str,
        to_date: str,
        max_retries: int = 3
    ) -> pd.DataFrame:
        """
        å®‰å…¨ãªæ—¥æ¬¡æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
        """
        for attempt in range(max_retries):
            try:
                headers = {"Authorization": f"Bearer {self._get_id_token()}"}
                results: List[Dict] = []
                pagination_key: Optional[str] = None
                
                # APIã‚³ãƒ¼ãƒ«å‰ã®å¾…æ©Ÿ
                time.sleep(random.uniform(0.5, 1.0))  # ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
                
                while True:
                    params = {
                        "code": code,
                        "from": from_date,
                        "to": to_date
                    }
                    if pagination_key:
                        params["pagination_key"] = pagination_key
                    
                    resp = requests.get(
                        f"{JQUANTS_BASE_URL}/prices/daily_quotes",
                        headers=headers,
                        params=params,
                        timeout=120
                    )
                    
                    if resp.status_code == 429:
                        logger.warning(f"éŠ˜æŸ„ {code}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€60ç§’å¾…æ©Ÿ...")
                        time.sleep(60)
                        continue
                    
                    if resp.status_code == 400:
                        logger.warning(f"éŠ˜æŸ„ {code}: ç„¡åŠ¹ãªã‚³ãƒ¼ãƒ‰ï¼ˆ400ã‚¨ãƒ©ãƒ¼ï¼‰")
                        return pd.DataFrame()
                    
                    resp.raise_for_status()
                    data = resp.json()
                    
                    items = data.get("daily_quotes", [])
                    if items:
                        results.extend(items)
                    
                    pagination_key = data.get("pagination_key")
                    if not pagination_key:
                        break
                    
                    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³é–“ã®å¾…æ©Ÿ
                    time.sleep(0.2)
                
                if results:
                    return pd.DataFrame(results)
                else:
                    logger.info(f"éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return pd.DataFrame()
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 5  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    logger.warning(f"éŠ˜æŸ„ {code} å–å¾—å¤±æ•— (è©¦è¡Œ{attempt+1}/{max_retries}): {str(e)}")
                    logger.info(f"  {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(wait_time)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚»ãƒƒãƒˆ
                    self.id_token = None
                    continue
                else:
                    logger.error(f"éŠ˜æŸ„ {code} æœ€çµ‚å–å¾—å¤±æ•—: {str(e)}")
                    return pd.DataFrame()
        
        return pd.DataFrame()
    
    def get_working_stock_codes(self, max_stocks: int = 255) -> List[str]:
        """
        å®Ÿéš›ã«å–å¾—å¯èƒ½ãªéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å–å¾—
        """
        # æ—¢ã«å‹•ä½œç¢ºèªæ¸ˆã¿ã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‹ã‚‰é–‹å§‹
        working_codes = [
            "29140", "40050", "40630", "45020", "45070", "45190", "45230", "45430",
            "45680", "49010", "60980", "61780", "62730", "63670", "65010", "67580",
            "68570", "68610", "69020", "69050", "69200", "69540", "69710", "72030",
            "72670", "78320", "79740", "80010", "80310", "80350", "83060", "83090",
            "84110", "88020", "90200", "94320", "99830", "99840"
        ]
        
        logger.info(f"å‹•ä½œç¢ºèªæ¸ˆã¿éŠ˜æŸ„: {len(working_codes)}éŠ˜æŸ„")
        
        if len(working_codes) >= max_stocks:
            return working_codes[:max_stocks]
        
        # ä¸è¶³åˆ†ã¯ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‹ã‚‰è£œå®Œ
        try:
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            time.sleep(1)
            
            resp = requests.get(
                f"{JQUANTS_BASE_URL}/listed/info",
                headers=headers,
                timeout=60
            )
            resp.raise_for_status()
            data = resp.json()
            
            items = data.get("info", [])
            if items:
                listed_df = pd.DataFrame(items)
                # æ—¢çŸ¥éŠ˜æŸ„ä»¥å¤–ã‚’è¿½åŠ 
                additional_codes = []
                for code in listed_df['Code'].tolist():
                    if code not in working_codes and len(working_codes) + len(additional_codes) < max_stocks:
                        additional_codes.append(code)
                
                working_codes.extend(additional_codes)
                logger.info(f"è¿½åŠ éŠ˜æŸ„: {len(additional_codes)}éŠ˜æŸ„")
                
        except Exception as e:
            logger.warning(f"ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—å¤±æ•—: {str(e)}")
        
        final_codes = working_codes[:max_stocks]
        logger.info(f"æœ€çµ‚éŠ˜æŸ„æ•°: {len(final_codes)}éŠ˜æŸ„")
        return final_codes
    
    def fetch_large_scale_data_safe(
        self,
        target_stocks: int = 100,  # ã¾ãšã¯100éŠ˜æŸ„ã§è©¦ã™
        from_date: str = "2015-01-01",
        to_date: str = "2025-08-31",
        save_intermediate: bool = True
    ) -> pd.DataFrame:
        """
        ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿ã—ãªãŒã‚‰å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å–å¾—
        """
        logger.info("=== å®‰å…¨ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        logger.info(f"å¯¾è±¡éŠ˜æŸ„æ•°: {target_stocks}éŠ˜æŸ„")
        logger.info(f"æœŸé–“: {from_date} ï½ {to_date}")
        
        # Step 1: å‹•ä½œã™ã‚‹éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—
        stock_codes = self.get_working_stock_codes(max_stocks=target_stocks)
        logger.info(f"å–å¾—å¯¾è±¡éŠ˜æŸ„æ•°: {len(stock_codes)}éŠ˜æŸ„")
        
        # Step 2: ãƒ‡ãƒ¼ã‚¿å–å¾—
        all_stock_data = []
        failed_stocks = []
        
        # ä¸­é–“ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if save_intermediate:
            intermediate_dir = Path("data/intermediate_data")
            intermediate_dir.mkdir(parents=True, exist_ok=True)
        
        for idx, code in enumerate(stock_codes, 1):
            try:
                logger.info(f"éŠ˜æŸ„ {code} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... ({idx}/{len(stock_codes)}) - {idx/len(stock_codes)*100:.1f}%å®Œäº†")
                
                stock_data = self.get_daily_quotes_safe(
                    code=code,
                    from_date=from_date,
                    to_date=to_date
                )
                
                if not stock_data.empty:
                    all_stock_data.append(stock_data)
                    logger.info(f"  âœ… éŠ˜æŸ„ {code}: {len(stock_data)}ä»¶å–å¾—")
                    
                    # ä¸­é–“ä¿å­˜ï¼ˆ20éŠ˜æŸ„ã”ã¨ï¼‰
                    if save_intermediate and idx % 20 == 0:
                        intermediate_df = pd.concat(all_stock_data, ignore_index=True)
                        intermediate_file = intermediate_dir / f"safe_intermediate_{idx}stocks_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl"
                        intermediate_df.to_pickle(intermediate_file)
                        logger.info(f"  ğŸ’¾ ä¸­é–“ä¿å­˜: {intermediate_file} ({len(intermediate_df):,}ä»¶)")
                
                else:
                    failed_stocks.append(code)
                    logger.info(f"  âš ï¸ éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                
                # éŠ˜æŸ„é–“ã®å¾…æ©Ÿï¼ˆé‡è¦ï¼‰
                wait_time = random.uniform(2.0, 4.0)  # 2-4ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
                if idx % 10 == 0:
                    wait_time = random.uniform(5.0, 8.0)  # 10éŠ˜æŸ„ã”ã¨ã«é•·ã„å¾…æ©Ÿ
                    logger.info(f"  â¸ï¸  APIåˆ¶é™å¯¾å¿œã§{wait_time:.1f}ç§’å¾…æ©Ÿ...")
                
                time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"  âŒ éŠ˜æŸ„ {code} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                failed_stocks.append(code)
                continue
        
        # Step 3: ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        if not all_stock_data:
            raise RuntimeError("å…¨ã¦ã®éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        logger.info("=== ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­ ===")
        combined_df = pd.concat(all_stock_data, ignore_index=True)
        
        logger.info("=== å®‰å…¨ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ===")
        logger.info(f"æˆåŠŸéŠ˜æŸ„: {len(all_stock_data)}éŠ˜æŸ„")
        logger.info(f"å¤±æ•—éŠ˜æŸ„: {len(failed_stocks)}éŠ˜æŸ„")
        if failed_stocks:
            logger.info(f"å¤±æ•—éŠ˜æŸ„ãƒªã‚¹ãƒˆ: {failed_stocks[:20]}...")  # æœ€åˆã®20å€‹è¡¨ç¤º
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(combined_df):,}ä»¶")
        logger.info(f"æœŸé–“: {combined_df['Date'].min()} ï½ {combined_df['Date'].max()}")
        
        # Step 4: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        processed_df = self._process_large_scale_data(combined_df)
        
        # Step 5: æœ€çµ‚ä¿å­˜
        output_dir = Path("data/large_scale_jquants_data")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"safe_large_scale_{len(all_stock_data)}stocks_10years_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        processed_df.to_pickle(output_file)
        
        logger.info(f"ğŸ‰ å®‰å…¨ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {output_file}")
        
        return processed_df
    
    def _process_large_scale_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†"""
        logger.info("å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹...")
        
        # åˆ—åæ¨™æº–åŒ–
        column_mapping = {
            'Date': 'date',
            'Code': 'symbol', 
            'Close': 'close_price',
            'Open': 'open_price',
            'High': 'high_price',
            'Low': 'low_price',
            'Volume': 'volume',
            'AdjustmentFactor': 'adjustment_factor',
            'AdjustmentOpen': 'adj_open',
            'AdjustmentHigh': 'adj_high', 
            'AdjustmentLow': 'adj_low',
            'AdjustmentClose': 'adj_close',
            'AdjustmentVolume': 'adj_volume'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df[new_col] = df[old_col]
        
        # å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèª
        required_cols = ['date', 'symbol', 'close_price']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_cols}")
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        df['date'] = pd.to_datetime(df['date'])
        df['close_price'] = pd.to_numeric(df['close_price'], errors='coerce')
        if 'volume' in df.columns:
            df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        
        # é‡è¤‡é™¤å»ãƒ»ã‚½ãƒ¼ãƒˆ
        df = df.drop_duplicates(subset=['date', 'symbol'])
        df = df.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
        logger.info("ãƒªã‚¿ãƒ¼ãƒ³ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—ä¸­...")
        df['daily_return'] = df.groupby('symbol')['close_price'].pct_change(fill_method=None)
        df['next_day_return'] = df.groupby('symbol')['close_price'].pct_change(fill_method=None).shift(-1)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ (ç¿Œæ—¥+1%ä»¥ä¸Š)
        df['target'] = (df['next_day_return'] >= 0.01).astype(int)
        
        # ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        df = df.dropna(subset=['close_price', 'next_day_return', 'target'])
        
        logger.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        logger.info(f"å¯¾è±¡éŠ˜æŸ„æ•°: {df['symbol'].nunique()}éŠ˜æŸ„")
        logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {df['target'].mean():.1%} (ä¸Šæ˜‡)")
        logger.info(f"æœŸé–“: {df['date'].min().date()} ï½ {df['date'].max().date()}")
        
        return df


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        fetcher = RateLimitedJQuantsFetcher()
        
        # ã¾ãšã¯100éŠ˜æŸ„ãƒ»10å¹´åˆ†ã§å®‰å…¨ã«å–å¾—
        large_scale_data = fetcher.fetch_large_scale_data_safe(
            target_stocks=100,        # 100éŠ˜æŸ„ã‹ã‚‰é–‹å§‹
            from_date="2015-01-01",   # 10å¹´å‰
            to_date="2025-08-31"      # ç¾åœ¨ã¾ã§
        )
        
        print("\n=== ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œç‰ˆå¤§è¦æ¨¡J-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ ===")
        print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(large_scale_data):,}ä»¶")
        print(f"éŠ˜æŸ„æ•°: {large_scale_data['symbol'].nunique()}éŠ˜æŸ„") 
        print(f"æœŸé–“: {large_scale_data['date'].min().date()} ï½ {large_scale_data['date'].max().date()}")
        print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {large_scale_data['target'].mean():.1%}")
        print("âœ… 100éŠ˜æŸ„ãƒ»10å¹´åˆ†ã®100%å®Ÿãƒ‡ãƒ¼ã‚¿ã§å–å¾—å®Œäº†")
        
        return large_scale_data
        
    except Exception as e:
        logger.error(f"å®‰å…¨ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {str(e)}")
        print("âŒ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
        raise


if __name__ == "__main__":
    main()