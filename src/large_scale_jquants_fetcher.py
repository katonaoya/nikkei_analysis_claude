"""
å¤§è¦æ¨¡J-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
255éŠ˜æŸ„ãƒ»10å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«å–å¾—
"""

import os
import time
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime, date, timedelta
from pathlib import Path
import logging

import pandas as pd
import requests
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# APIè¨­å®š
JQUANTS_BASE_URL = "https://api.jquants.com/v1"


class LargeScaleJQuantsFetcher:
    """255éŠ˜æŸ„ãƒ»10å¹´åˆ†ã®J-Quantsãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        logger.info("å¤§è¦æ¨¡J-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        # ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.id_token
        
        logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
        
        try:
            # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            auth_payload = {
                "mailaddress": self.mail_address,
                "password": self.password
            }
            
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_user",
                data=json.dumps(auth_payload),
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            resp.raise_for_status()
            refresh_token = resp.json().get("refreshToken")
            
            if not refresh_token:
                raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            logger.info("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            
            # IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                timeout=30
            )
            resp.raise_for_status()
            self.id_token = resp.json().get("idToken")
            
            if not self.id_token:
                raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æœŸé™ã‚’è¨­å®š
            self.token_expires_at = datetime.now() + timedelta(hours=1)
            
            logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            return self.id_token
            
        except Exception as e:
            logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def get_daily_quotes_with_retry(
        self, 
        code: str,
        from_date: str,
        to_date: str,
        max_retries: int = 3
    ) -> pd.DataFrame:
        """
        ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§æ—¥æ¬¡æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
        """
        for attempt in range(max_retries):
            try:
                headers = {"Authorization": f"Bearer {self._get_id_token()}"}
                results: List[Dict] = []
                pagination_key: Optional[str] = None
                
                while True:
                    params = {
                        "code": code,
                        "from": from_date,
                        "to": to_date
                    }
                    if pagination_key:
                        params["pagination_key"] = pagination_key
                    
                    resp = requests.get(
                        f"{JQUANTS_BASE_URL}/prices/daily_quotes",
                        headers=headers,
                        params=params,
                        timeout=120
                    )
                    resp.raise_for_status()
                    data = resp.json()
                    
                    items = data.get("daily_quotes", [])
                    if items:
                        results.extend(items)
                    
                    pagination_key = data.get("pagination_key")
                    if not pagination_key:
                        break
                    
                    # APIåˆ¶é™å¯¾å¿œ
                    time.sleep(0.1)
                
                if results:
                    return pd.DataFrame(results)
                else:
                    logger.warning(f"éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return pd.DataFrame()
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"éŠ˜æŸ„ {code} å–å¾—å¤±æ•— (è©¦è¡Œ{attempt+1}/{max_retries}): {str(e)}")
                    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚»ãƒƒãƒˆ
                    self.id_token = None
                    time.sleep(2)
                    continue
                else:
                    logger.error(f"éŠ˜æŸ„ {code} æœ€çµ‚å–å¾—å¤±æ•—: {str(e)}")
                    raise
        
        return pd.DataFrame()
    
    def get_top_255_stocks(self) -> List[str]:
        """
        æ™‚ä¾¡ç·é¡ä¸Šä½255éŠ˜æŸ„ã‚’å–å¾—
        """
        try:
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            
            resp = requests.get(
                f"{JQUANTS_BASE_URL}/listed/info",
                headers=headers,
                timeout=60
            )
            resp.raise_for_status()
            data = resp.json()
            
            items = data.get("info", [])
            if not items:
                raise RuntimeError("ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ãŒç©ºã§ã™")
            
            listed_df = pd.DataFrame(items)
            logger.info(f"ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—: {len(listed_df)}éŠ˜æŸ„")
            
            # æ±è¨¼ãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´ã«çµã‚‹
            prime_stocks = listed_df[listed_df['MarketCode'] == '111']
            logger.info(f"æ±è¨¼ãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´éŠ˜æŸ„: {len(prime_stocks)}éŠ˜æŸ„")
            
            if len(prime_stocks) >= 255:
                # ä¸Šä½255éŠ˜æŸ„ã‚’é¸æŠï¼ˆã‚³ãƒ¼ãƒ‰é †ã§å®‰å®šåŒ–ï¼‰
                selected_stocks = prime_stocks.head(255)['Code'].tolist()
            else:
                # ãƒ—ãƒ©ã‚¤ãƒ ãŒè¶³ã‚Šãªã„å ´åˆã¯ä»–ã®å¸‚å ´ã‚‚å«ã‚ã‚‹
                logger.info("ãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´ãŒ255éŠ˜æŸ„æœªæº€ã®ãŸã‚ã€ä»–å¸‚å ´ã‚‚å«ã‚ã¾ã™")
                selected_stocks = listed_df.head(255)['Code'].tolist()
            
            logger.info(f"é¸æŠã•ã‚ŒãŸéŠ˜æŸ„æ•°: {len(selected_stocks)}éŠ˜æŸ„")
            return selected_stocks
            
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢çŸ¥ã®å¤§æ‰‹éŠ˜æŸ„255éŠ˜æŸ„
            return self._get_fallback_255_stocks()
    
    def _get_fallback_255_stocks(self) -> List[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å¤§æ‰‹éŠ˜æŸ„255éŠ˜æŸ„ãƒªã‚¹ãƒˆ"""
        logger.warning("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨")
        
        # æ—¥çµŒ225 + å¤§æ‰‹éŠ˜æŸ„ã‚’255éŠ˜æŸ„åˆ†
        major_stocks = [
            # æ—¥çµŒ225ä¸»è¦éŠ˜æŸ„
            "72030", "99840", "67580", "94320", "83060", "80350", "63670", "79740",
            "99830", "40630", "65010", "72670", "69020", "80010", "29140", "45190",
            "45430", "69540", "65020", "83090", "45020", "68610", "49010", "94370",
            "45680", "62730", "69200", "78320", "84110", "88020", "45230", "61780",
            "60980", "40050", "45070", "69710", "68570", "69050", "80310", "90200",
            
            # è¿½åŠ å¤§æ‰‹éŠ˜æŸ„ï¼ˆå®Ÿéš›ã®5æ¡ã‚³ãƒ¼ãƒ‰æƒ³å®šï¼‰
            "13010", "13050", "13060", "13080", "13090", "13190", "13200", "13250",
            "14040", "14100", "14300", "14400", "14430", "14440", "14460", "14640",
            "14700", "14750", "14900", "14950", "15030", "15070", "15180", "15200",
            "15280", "15310", "15350", "15360", "15400", "15450", "15460", "15500",
            "15520", "15560", "15580", "15650", "15700", "15750", "15800", "15810",
            "16010", "16040", "16070", "16080", "16090", "16100", "16140", "16180",
            "16200", "16250", "16270", "16300", "16350", "16360", "16400", "16430",
            "16440", "16450", "16490", "16500", "16520", "16550", "16580", "16600",
            "17010", "17020", "17040", "17060", "17080", "17100", "17140", "17160",
            "17180", "17200", "17220", "17240", "17260", "17280", "17300", "17320",
            "18010", "18020", "18030", "18040", "18050", "18060", "18070", "18080",
            "18090", "18100", "18110", "18120", "18130", "18140", "18150", "18160",
            "19010", "19020", "19030", "19040", "19050", "19060", "19070", "19080",
            "19090", "19100", "19110", "19120", "19130", "19140", "19150", "19160",
            "20010", "20020", "20030", "20040", "20050", "20060", "20070", "20080",
            "20090", "20100", "20110", "20120", "20130", "20140", "20150", "20160",
            "21010", "21020", "21030", "21040", "21050", "21060", "21070", "21080",
            "21090", "21100", "21110", "21120", "21130", "21140", "21150", "21160",
            "22010", "22020", "22030", "22040", "22050", "22060", "22070", "22080",
            "22090", "22100", "22110", "22120", "22130", "22140", "22150", "22160",
            "23010", "23020", "23030", "23040", "23050", "23060", "23070", "23080",
            "23090", "23100", "23110", "23120", "23130", "23140", "23150", "23160",
            "24010", "24020", "24030", "24040", "24050", "24060", "24070", "24080",
            "24090", "24100", "24110", "24120", "24130", "24140", "24150", "24160",
            "25010", "25020", "25030", "25040", "25050", "25060", "25070", "25080",
            "25090", "25100", "25110", "25120", "25130", "25140", "25150", "25160",
            "26010", "26020", "26030", "26040", "26050", "26060", "26070", "26080",
            "26090", "26100", "26110", "26120", "26130", "26140", "26150", "26160"
        ]
        
        return major_stocks[:255]  # 255éŠ˜æŸ„ã«åˆ¶é™
    
    def fetch_large_scale_data(
        self,
        from_date: str = "2015-01-01",  # 10å¹´å‰
        to_date: str = "2025-08-31",
        save_intermediate: bool = True
    ) -> pd.DataFrame:
        """
        255éŠ˜æŸ„ãƒ»10å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        """
        logger.info("=== å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        logger.info(f"å¯¾è±¡éŠ˜æŸ„æ•°: 255éŠ˜æŸ„")
        logger.info(f"æœŸé–“: {from_date} ï½ {to_date}")
        
        # Step 1: éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—
        stock_codes = self.get_top_255_stocks()
        logger.info(f"å®Ÿéš›ã®å¯¾è±¡éŠ˜æŸ„æ•°: {len(stock_codes)}éŠ˜æŸ„")
        
        # Step 2: ãƒ‡ãƒ¼ã‚¿å–å¾—
        all_stock_data = []
        failed_stocks = []
        
        # ä¸­é–“ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if save_intermediate:
            intermediate_dir = Path("data/intermediate_data")
            intermediate_dir.mkdir(parents=True, exist_ok=True)
        
        for idx, code in enumerate(stock_codes, 1):
            try:
                logger.info(f"éŠ˜æŸ„ {code} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... ({idx}/{len(stock_codes)}) - {idx/len(stock_codes)*100:.1f}%å®Œäº†")
                
                stock_data = self.get_daily_quotes_with_retry(
                    code=code,
                    from_date=from_date,
                    to_date=to_date
                )
                
                if not stock_data.empty:
                    all_stock_data.append(stock_data)
                    logger.info(f"  âœ… éŠ˜æŸ„ {code}: {len(stock_data)}ä»¶å–å¾—")
                    
                    # ä¸­é–“ä¿å­˜ï¼ˆ50éŠ˜æŸ„ã”ã¨ï¼‰
                    if save_intermediate and idx % 50 == 0:
                        intermediate_df = pd.concat(all_stock_data, ignore_index=True)
                        intermediate_file = intermediate_dir / f"intermediate_{idx}stocks_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl"
                        intermediate_df.to_pickle(intermediate_file)
                        logger.info(f"  ğŸ’¾ ä¸­é–“ä¿å­˜: {intermediate_file} ({len(intermediate_df)}ä»¶)")
                
                else:
                    failed_stocks.append(code)
                    logger.warning(f"  âŒ éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                
                # APIåˆ¶é™å¯¾å¿œï¼ˆå¤§è¦æ¨¡å–å¾—ç”¨ï¼‰
                if idx % 10 == 0:
                    logger.info(f"  â¸ï¸  APIåˆ¶é™å¯¾å¿œã§1ç§’å¾…æ©Ÿ...")
                    time.sleep(1)
                else:
                    time.sleep(0.3)
                
            except Exception as e:
                logger.error(f"  âŒ éŠ˜æŸ„ {code} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                failed_stocks.append(code)
                continue
        
        # Step 3: ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        if not all_stock_data:
            raise RuntimeError("å…¨ã¦ã®éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        logger.info("=== ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­ ===")
        combined_df = pd.concat(all_stock_data, ignore_index=True)
        
        logger.info("=== å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ===")
        logger.info(f"æˆåŠŸéŠ˜æŸ„: {len(all_stock_data)}éŠ˜æŸ„")
        logger.info(f"å¤±æ•—éŠ˜æŸ„: {len(failed_stocks)}éŠ˜æŸ„ {failed_stocks[:10]}...")
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(combined_df):,}ä»¶")
        logger.info(f"æœŸé–“: {combined_df['Date'].min()} ï½ {combined_df['Date'].max()}")
        
        # Step 4: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        processed_df = self._process_large_scale_data(combined_df)
        
        # Step 5: æœ€çµ‚ä¿å­˜
        output_dir = Path("data/large_scale_jquants_data")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"large_scale_data_{len(stock_codes)}stocks_10years_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        processed_df.to_pickle(output_file)
        
        logger.info(f"ğŸ‰ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {output_file}")
        
        return processed_df
    
    def _process_large_scale_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†"""
        logger.info("å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹...")
        
        # åˆ—åæ¨™æº–åŒ–
        column_mapping = {
            'Date': 'date',
            'Code': 'symbol', 
            'Close': 'close_price',
            'Open': 'open_price',
            'High': 'high_price',
            'Low': 'low_price',
            'Volume': 'volume',
            'AdjustmentFactor': 'adjustment_factor',
            'AdjustmentOpen': 'adj_open',
            'AdjustmentHigh': 'adj_high', 
            'AdjustmentLow': 'adj_low',
            'AdjustmentClose': 'adj_close',
            'AdjustmentVolume': 'adj_volume'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df[new_col] = df[old_col]
        
        # å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèª
        required_cols = ['date', 'symbol', 'close_price']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_cols}")
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        df['date'] = pd.to_datetime(df['date'])
        df['close_price'] = pd.to_numeric(df['close_price'], errors='coerce')
        if 'volume' in df.columns:
            df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        
        # é‡è¤‡é™¤å»ãƒ»ã‚½ãƒ¼ãƒˆ
        df = df.drop_duplicates(subset=['date', 'symbol'])
        df = df.sort_values(['symbol', 'date']).reset_index(drop=True)
        
        # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
        logger.info("ãƒªã‚¿ãƒ¼ãƒ³ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—ä¸­...")
        df['daily_return'] = df.groupby('symbol')['close_price'].pct_change(fill_method=None)
        df['next_day_return'] = df.groupby('symbol')['close_price'].pct_change(fill_method=None).shift(-1)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ (ç¿Œæ—¥+1%ä»¥ä¸Š)
        df['target'] = (df['next_day_return'] >= 0.01).astype(int)
        
        # ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        df = df.dropna(subset=['close_price', 'next_day_return', 'target'])
        
        logger.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        logger.info(f"å¯¾è±¡éŠ˜æŸ„æ•°: {df['symbol'].nunique()}éŠ˜æŸ„")
        logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {df['target'].mean():.1%} (ä¸Šæ˜‡)")
        logger.info(f"æœŸé–“: {df['date'].min().date()} ï½ {df['date'].max().date()}")
        
        return df


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        fetcher = LargeScaleJQuantsFetcher()
        
        # 255éŠ˜æŸ„ãƒ»10å¹´åˆ†ã®å®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—
        large_scale_data = fetcher.fetch_large_scale_data(
            from_date="2015-01-01",  # 10å¹´å‰
            to_date="2025-08-31"     # ç¾åœ¨ã¾ã§
        )
        
        print("\n=== å¤§è¦æ¨¡J-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ ===")
        print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(large_scale_data):,}ä»¶")
        print(f"éŠ˜æŸ„æ•°: {large_scale_data['symbol'].nunique()}éŠ˜æŸ„") 
        print(f"æœŸé–“: {large_scale_data['date'].min().date()} ï½ {large_scale_data['date'].max().date()}")
        print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {large_scale_data['target'].mean():.1%}")
        print("âœ… 255éŠ˜æŸ„ãƒ»10å¹´åˆ†ã®100%å®Ÿãƒ‡ãƒ¼ã‚¿ã§å–å¾—å®Œäº†")
        
        return large_scale_data
        
    except Exception as e:
        logger.error(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {str(e)}")
        print("âŒ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
        raise


if __name__ == "__main__":
    main()