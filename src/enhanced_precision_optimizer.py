"""
Enhanced Precision Optimizer - 175éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦å‘ä¸Š
ç›®æ¨™: Precision â‰¥ 0.75é”æˆã®ãŸã‚ã®é«˜åº¦ãªæœ€é©åŒ–
"""

import os
import pickle
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import logging

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import IsolationForest
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.calibration import CalibratedClassifierCV
import lightgbm as lgb
import catboost as cb
import xgboost as xgb
import optuna

warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EnhancedPrecisionOptimizer:
    """é«˜åº¦ãªç²¾åº¦æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, target_precision: float = 0.75):
        """
        åˆæœŸåŒ–
        
        Args:
            target_precision: ç›®æ¨™ç²¾åº¦
        """
        self.target_precision = target_precision
        self.scaler = RobustScaler()
        self.feature_selector = None
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.best_models = {}
        self.best_params = {}
        self.feature_importance = {}
        
        logger.info(f"Enhanced Precision OptimizeråˆæœŸåŒ–å®Œäº† (ç›®æ¨™ç²¾åº¦: {target_precision:.1%})")
    
    def load_data(self) -> pd.DataFrame:
        """175éŠ˜æŸ„ã®10å¹´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("175éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        data_dir = Path("data/nikkei225_full_data")
        
        # æœ€æ–°ã®pklãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        pkl_files = list(data_dir.glob("nikkei225_full_10years_*.pkl"))
        if not pkl_files:
            raise FileNotFoundError("175éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        latest_file = max(pkl_files, key=lambda f: f.stat().st_mtime)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        
        df = pd.read_pickle(latest_file)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        logger.info(f"éŠ˜æŸ„æ•°: {df['Code'].nunique()}éŠ˜æŸ„")
        logger.info(f"æœŸé–“: {df['Date'].min()} ï½ {df['Date'].max()}")
        
        return df
    
    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° - 300+ç‰¹å¾´é‡"""
        logger.info("é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        
        # åŸºæœ¬çš„ãªå‰å‡¦ç†
        df = df.sort_values(['Code', 'Date']).reset_index(drop=True)
        df['close_price'] = pd.to_numeric(df['Close'], errors='coerce')
        df['volume'] = pd.to_numeric(df['Volume'], errors='coerce')
        df['high_price'] = pd.to_numeric(df['High'], errors='coerce')
        df['low_price'] = pd.to_numeric(df['Low'], errors='coerce')
        df['open_price'] = pd.to_numeric(df['Open'], errors='coerce')
        
        # èª¿æ•´æ¸ˆã¿ä¾¡æ ¼ã‚’ä½¿ç”¨
        for col in ['AdjustmentClose', 'AdjustmentHigh', 'AdjustmentLow', 'AdjustmentOpen', 'AdjustmentVolume']:
            if col in df.columns:
                df[col.lower().replace('adjustment', 'adj_')] = pd.to_numeric(df[col], errors='coerce')
        
        # åŸºæœ¬ãƒªã‚¿ãƒ¼ãƒ³
        df['daily_return'] = df.groupby('Code')['close_price'].pct_change(fill_method=None)
        df['next_day_return'] = df.groupby('Code')['close_price'].pct_change(fill_method=None).shift(-1)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆã‚ˆã‚Šå³ã—ã„é–¾å€¤ã§é«˜ç²¾åº¦ã‚’ç‹™ã†ï¼‰
        df['target'] = (df['next_day_return'] >= 0.015).astype(int)  # 1.5%ä»¥ä¸Šã®ä¸Šæ˜‡
        
        logger.info("é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—ä¸­...")
        
        # è¤‡æ•°æœŸé–“ã§ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        windows = [3, 5, 7, 10, 14, 20, 25, 30, 50, 100]
        
        for window in windows:
            # ç§»å‹•å¹³å‡ç³»
            df[f'sma_{window}'] = df.groupby('Code')['close_price'].transform(
                lambda x: x.rolling(window).mean()
            )
            df[f'ema_{window}'] = df.groupby('Code')['close_price'].transform(
                lambda x: x.ewm(span=window).mean()
            )
            
            # ä¾¡æ ¼æ¯”ç‡
            df[f'price_to_sma_{window}'] = df['close_price'] / df[f'sma_{window}']
            df[f'price_to_ema_{window}'] = df['close_price'] / df[f'ema_{window}']
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            df[f'volatility_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).std()
            )
            
            # é«˜å€¤ãƒ»å®‰å€¤ã‹ã‚‰ã®è·é›¢
            df[f'high_{window}'] = df.groupby('Code')['high_price'].transform(
                lambda x: x.rolling(window).max()
            )
            df[f'low_{window}'] = df.groupby('Code')['low_price'].transform(
                lambda x: x.rolling(window).min()
            )
            df[f'price_position_{window}'] = (df['close_price'] - df[f'low_{window}']) / (
                df[f'high_{window}'] - df[f'low_{window}'] + 1e-8
            )
            
            # ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
            df[f'return_mean_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).mean()
            )
            df[f'return_std_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).std()
            )
            df[f'return_skew_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).skew()
            )
            df[f'return_kurt_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).kurt()
            )
            
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™
            if 'volume' in df.columns:
                df[f'volume_ma_{window}'] = df.groupby('Code')['volume'].transform(
                    lambda x: x.rolling(window).mean()
                )
                df[f'volume_ratio_{window}'] = df['volume'] / (df[f'volume_ma_{window}'] + 1)
        
        # RSIï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        for period in [7, 14, 21, 30]:
            df[f'rsi_{period}'] = df.groupby('Code').apply(
                lambda x: self._calculate_rsi(x['close_price'], period)
            ).values
        
        # MACDç³»æŒ‡æ¨™
        logger.info("MACDæŒ‡æ¨™è¨ˆç®—ä¸­...")
        
        def calculate_group_macd(group):
            ema12 = group['close_price'].ewm(span=12).mean()
            ema26 = group['close_price'].ewm(span=26).mean()
            macd = ema12 - ema26
            signal = macd.ewm(span=9).mean()
            return pd.DataFrame({'macd': macd, 'macd_signal': signal})
        
        macd_results = df.groupby('Code').apply(calculate_group_macd)
        macd_results.index = macd_results.index.droplevel(0)
        df = df.join(macd_results)
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆè¤‡æ•°æœŸé–“ãƒ»æ¨™æº–åå·®ï¼‰
        for window in [10, 20, 30]:
            for std_dev in [1.5, 2.0, 2.5]:
                mean = df.groupby('Code')['close_price'].transform(lambda x: x.rolling(window).mean())
                std = df.groupby('Code')['close_price'].transform(lambda x: x.rolling(window).std())
                df[f'bb_upper_{window}_{std_dev}'] = mean + (std_dev * std)
                df[f'bb_lower_{window}_{std_dev}'] = mean - (std_dev * std)
                df[f'bb_position_{window}_{std_dev}'] = (df['close_price'] - df[f'bb_lower_{window}_{std_dev}']) / (
                    df[f'bb_upper_{window}_{std_dev}'] - df[f'bb_lower_{window}_{std_dev}'] + 1e-8
                )
        
        # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
        logger.info("ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹æŒ‡æ¨™è¨ˆç®—ä¸­...")
        for k_period in [14, 21]:
            for d_period in [3, 5]:
                def calculate_stoch(group):
                    low_min = group['low_price'].rolling(k_period).min()
                    high_max = group['high_price'].rolling(k_period).max()
                    k_percent = 100 * ((group['close_price'] - low_min) / (high_max - low_min + 1e-8))
                    d_percent = k_percent.rolling(d_period).mean()
                    return pd.DataFrame({
                        f'stoch_k_{k_period}_{d_period}': k_percent,
                        f'stoch_d_{k_period}_{d_period}': d_percent
                    })
                
                stoch_results = df.groupby('Code').apply(calculate_stoch)
                stoch_results.index = stoch_results.index.droplevel(0)
                df = df.join(stoch_results)
        
        # ä¾¡æ ¼å¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
        for lag in range(1, 11):
            df[f'return_lag_{lag}'] = df.groupby('Code')['daily_return'].shift(lag)
            df[f'price_lag_{lag}'] = df.groupby('Code')['close_price'].shift(lag)
        
        # ã‚»ã‚¯ã‚¿ãƒ¼ãƒ»å¸‚å ´å…¨ä½“ã¨ã®ç›¸é–¢
        logger.info("å¸‚å ´ç›¸é–¢æŒ‡æ¨™è¨ˆç®—ä¸­...")
        market_return = df.groupby('Date')['daily_return'].mean()
        df['market_return'] = df['Date'].map(market_return)
        
        def calculate_beta(group):
            return group['daily_return'].rolling(60).corr(group['market_return'])
        
        beta_results = df.groupby('Code').apply(calculate_beta)
        beta_results.index = beta_results.index.droplevel(0)
        df['beta_vs_market'] = beta_results
        
        # æµå‹•æ€§æŒ‡æ¨™
        if 'volume' in df.columns:
            df['liquidity_score'] = df['volume'] * df['close_price']
            df['liquidity_percentile'] = df.groupby('Date')['liquidity_score'].rank(pct=True)
        
        # æ™‚ç³»åˆ—çµ±è¨ˆçš„ç‰¹å¾´é‡
        for window in [5, 10, 20]:
            # è‡ªå·±ç›¸é–¢
            df[f'autocorr_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).apply(lambda y: y.autocorr(lag=1) if len(y.dropna()) > 1 else 0)
            )
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
            df[f'trend_strength_{window}'] = df.groupby('Code')['close_price'].transform(
                lambda x: x.rolling(window).apply(
                    lambda y: np.corrcoef(np.arange(len(y)), y)[0, 1] if len(y) == window else np.nan
                )
            )
        
        # ç•°å¸¸å€¤æ¤œå‡ºç‰¹å¾´é‡
        df['price_zscore'] = df.groupby('Code')['close_price'].transform(
            lambda x: (x - x.rolling(60).mean()) / (x.rolling(60).std() + 1e-8)
        )
        df['volume_zscore'] = df.groupby('Code')['volume'].transform(
            lambda x: (x - x.rolling(60).mean()) / (x.rolling(60).std() + 1e-8)
        ) if 'volume' in df.columns else 0
        
        # ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™
        for window in [10, 20, 30]:
            # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¿‘ä¼¼
            df[f'fractal_dim_{window}'] = df.groupby('Code')['close_price'].transform(
                lambda x: x.rolling(window).apply(self._estimate_fractal_dimension)
            )
        
        logger.info(f"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {len([col for col in df.columns if col not in ['Code', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'target', 'next_day_return']])}å€‹ã®ç‰¹å¾´é‡")
        
        return df
    
    def _calculate_rsi(self, prices: pd.Series, window: int) -> pd.Series:
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
        rs = gain / (loss + 1e-8)
        return 100 - (100 / (1 + rs))
    
    def _calculate_macd(self, prices: pd.Series) -> Tuple[pd.Series, pd.Series]:
        """MACDè¨ˆç®—"""
        ema12 = prices.ewm(span=12).mean()
        ema26 = prices.ewm(span=26).mean()
        macd = ema12 - ema26
        signal = macd.ewm(span=9).mean()
        return macd, signal
    
    def _calculate_stochastic(self, group: pd.DataFrame, k_period: int, d_period: int) -> Tuple[pd.Series, pd.Series]:
        """ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹è¨ˆç®—"""
        low_min = group['low_price'].rolling(k_period).min()
        high_max = group['high_price'].rolling(k_period).max()
        k_percent = 100 * ((group['close_price'] - low_min) / (high_max - low_min + 1e-8))
        d_percent = k_percent.rolling(d_period).mean()
        return k_percent, d_percent
    
    def _estimate_fractal_dimension(self, series: pd.Series) -> float:
        """ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®ç°¡æ˜“æ¨å®š"""
        if len(series) < 3:
            return np.nan
        
        # Higuchiæ³•ã«ã‚ˆã‚‹ç°¡æ˜“å®Ÿè£…
        try:
            n = len(series)
            k_max = min(10, n // 2)
            lm = []
            
            for k in range(1, k_max + 1):
                l_k = 0
                for m in range(1, k + 1):
                    indices = np.arange(m - 1, n, k)
                    if len(indices) > 1:
                        norm_sum = np.sum(np.abs(np.diff(series.iloc[indices])))
                        l_k += norm_sum * (n - 1) / ((len(indices) - 1) * k)
                
                if l_k > 0:
                    lm.append(l_k / k)
            
            if len(lm) < 2:
                return np.nan
                
            # ç·šå½¢å›å¸°ã§ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒæ¨å®š
            x = np.log(range(1, len(lm) + 1))
            y = np.log(lm)
            slope = np.polyfit(x, y, 1)[0]
            return 2 - slope
        except:
            return np.nan
    
    def optimize_hyperparameters(self, X_train: pd.DataFrame, y_train: pd.Series, 
                                X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """Optunaä½¿ç”¨ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        logger.info("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹...")
        
        def objective(trial):
            # LightGBMç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            lgb_params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('lgb_num_leaves', 30, 300),
                'feature_fraction': trial.suggest_float('lgb_feature_fraction', 0.4, 1.0),
                'bagging_fraction': trial.suggest_float('lgb_bagging_fraction', 0.4, 1.0),
                'bagging_freq': trial.suggest_int('lgb_bagging_freq', 1, 7),
                'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3),
                'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('lgb_reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('lgb_reg_lambda', 0, 10),
                'verbosity': -1,
                'random_state': 42
            }
            
            # CatBoostç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            cb_params = {
                'loss_function': 'Logloss',
                'iterations': trial.suggest_int('cb_iterations', 100, 1000),
                'learning_rate': trial.suggest_float('cb_learning_rate', 0.01, 0.3),
                'depth': trial.suggest_int('cb_depth', 4, 10),
                'l2_leaf_reg': trial.suggest_float('cb_l2_leaf_reg', 1, 10),
                'border_count': trial.suggest_int('cb_border_count', 32, 255),
                'random_seed': 42,
                'verbose': False
            }
            
            # XGBoostç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            xgb_params = {
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),
                'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.3),
                'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 1000),
                'subsample': trial.suggest_float('xgb_subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('xgb_reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('xgb_reg_lambda', 1, 10),
                'random_state': 42
            }
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            lgb_model = lgb.LGBMClassifier(**lgb_params)
            cb_model = cb.CatBoostClassifier(**cb_params)
            xgb_model = xgb.XGBClassifier(**xgb_params)
            
            lgb_model.fit(X_train, y_train)
            cb_model.fit(X_train, y_train)
            xgb_model.fit(X_train, y_train)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            lgb_proba = lgb_model.predict_proba(X_val)[:, 1]
            cb_proba = cb_model.predict_proba(X_val)[:, 1]
            xgb_proba = xgb_model.predict_proba(X_val)[:, 1]
            
            # é‡ã¿æœ€é©åŒ–
            w1 = trial.suggest_float('weight_lgb', 0.2, 0.6)
            w2 = trial.suggest_float('weight_cb', 0.2, 0.6)
            w3 = 1 - w1 - w2
            
            if w3 < 0:
                w3 = 0
                w1 = w1 / (w1 + w2)
                w2 = w2 / (w1 + w2)
            
            ensemble_proba = w1 * lgb_proba + w2 * cb_proba + w3 * xgb_proba
            
            # é–¾å€¤æœ€é©åŒ–
            threshold = trial.suggest_float('threshold', 0.7, 0.95)
            predictions = (ensemble_proba >= threshold).astype(int)
            
            # ç²¾åº¦è¨ˆç®—ï¼ˆäºˆæ¸¬ãŒ0å€‹ã®å ´åˆã¯ä½ã„ç²¾åº¦ã‚’è¿”ã™ï¼‰
            if predictions.sum() == 0:
                return 0.0
            
            precision = precision_score(y_val, predictions, zero_division=0)
            return precision
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        study = optuna.create_study(direction='maximize', 
                                   sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=100, timeout=3600)  # 1æ™‚é–“ã®åˆ¶é™
        
        logger.info(f"æœ€é©åŒ–å®Œäº†: Best precision = {study.best_value:.4f}")
        return study.best_params
    
    def train_final_model(self, df: pd.DataFrame) -> Dict[str, float]:
        """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡"""
        logger.info("æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        feature_columns = [col for col in df.columns if col not in 
                          ['Code', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume', 
                           'target', 'next_day_return', 'close_price', 'volume', 
                           'high_price', 'low_price', 'open_price']]
        
        X = df[feature_columns].fillna(0)
        y = df['target']
        dates = df['Date']
        
        # å¤–ã‚Œå€¤é™¤å»
        logger.info("å¤–ã‚Œå€¤æ¤œå‡ºãƒ»é™¤å»...")
        outliers = self.anomaly_detector.fit_predict(X) == -1
        X = X[~outliers]
        y = y[~outliers]
        dates = dates[~outliers]
        
        logger.info(f"å¤–ã‚Œå€¤é™¤å»å¾Œ: {len(X):,}ãƒ¬ã‚³ãƒ¼ãƒ‰ (é™¤å»: {outliers.sum():,}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
        
        # ç‰¹å¾´é‡é¸æŠ
        logger.info("ç‰¹å¾´é‡é¸æŠ...")
        selector = SelectFromModel(
            lgb.LGBMClassifier(n_estimators=100, random_state=42),
            threshold='median'
        )
        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()]
        
        logger.info(f"é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {len(selected_features)}/{len(feature_columns)}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_scaled = self.scaler.fit_transform(X_selected)
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=8, gap=5)
        results = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled)):
            logger.info(f"Fold {fold + 1}/8 è¨“ç·´ä¸­...")
            
            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
            best_params = self.optimize_hyperparameters(
                pd.DataFrame(X_train), y_train,
                pd.DataFrame(X_val), y_val
            )
            
            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            lgb_params = {k.replace('lgb_', ''): v for k, v in best_params.items() if k.startswith('lgb_')}
            lgb_params.update({'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1, 'random_state': 42})
            
            cb_params = {k.replace('cb_', ''): v for k, v in best_params.items() if k.startswith('cb_')}
            cb_params.update({'loss_function': 'Logloss', 'random_seed': 42, 'verbose': False})
            
            xgb_params = {k.replace('xgb_', ''): v for k, v in best_params.items() if k.startswith('xgb_')}
            xgb_params.update({'objective': 'binary:logistic', 'eval_metric': 'logloss', 'random_state': 42})
            
            lgb_model = lgb.LGBMClassifier(**lgb_params)
            cb_model = cb.CatBoostClassifier(**cb_params)
            xgb_model = xgb.XGBClassifier(**xgb_params)
            
            # ç¢ºç‡æ ¡æ­£
            lgb_calibrated = CalibratedClassifierCV(lgb_model, method='isotonic', cv=3)
            cb_calibrated = CalibratedClassifierCV(cb_model, method='isotonic', cv=3)
            xgb_calibrated = CalibratedClassifierCV(xgb_model, method='isotonic', cv=3)
            
            lgb_calibrated.fit(X_train, y_train)
            cb_calibrated.fit(X_train, y_train)
            xgb_calibrated.fit(X_train, y_train)
            
            # äºˆæ¸¬
            lgb_proba = lgb_calibrated.predict_proba(X_val)[:, 1]
            cb_proba = cb_calibrated.predict_proba(X_val)[:, 1]
            xgb_proba = xgb_calibrated.predict_proba(X_val)[:, 1]
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            w1 = best_params.get('weight_lgb', 0.4)
            w2 = best_params.get('weight_cb', 0.4)
            w3 = 1 - w1 - w2
            
            ensemble_proba = w1 * lgb_proba + w2 * cb_proba + w3 * xgb_proba
            
            # é–¾å€¤æœ€é©åŒ–
            threshold = best_params.get('threshold', 0.85)
            predictions = (ensemble_proba >= threshold).astype(int)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            if predictions.sum() > 0:
                precision = precision_score(y_val, predictions, zero_division=0)
                recall = recall_score(y_val, predictions, zero_division=0)
                f1 = f1_score(y_val, predictions, zero_division=0)
                accuracy = accuracy_score(y_val, predictions)
                
                results.append({
                    'fold': fold + 1,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'accuracy': accuracy,
                    'predictions_count': predictions.sum(),
                    'total_count': len(predictions)
                })
                
                logger.info(f"Fold {fold + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
            else:
                logger.warning(f"Fold {fold + 1} - äºˆæ¸¬æ•°ãŒ0ã®ãŸã‚è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        # çµæœé›†è¨ˆ
        if results:
            final_results = {
                'mean_precision': np.mean([r['precision'] for r in results]),
                'std_precision': np.std([r['precision'] for r in results]),
                'mean_recall': np.mean([r['recall'] for r in results]),
                'mean_f1': np.mean([r['f1'] for r in results]),
                'mean_accuracy': np.mean([r['accuracy'] for r in results]),
                'total_predictions': sum([r['predictions_count'] for r in results]),
                'total_samples': sum([r['total_count'] for r in results])
            }
            
            logger.info("=== æœ€çµ‚çµæœ ===")
            logger.info(f"å¹³å‡ç²¾åº¦: {final_results['mean_precision']:.4f} Â± {final_results['std_precision']:.4f}")
            logger.info(f"å¹³å‡å†ç¾ç‡: {final_results['mean_recall']:.4f}")
            logger.info(f"å¹³å‡F1ã‚¹ã‚³ã‚¢: {final_results['mean_f1']:.4f}")
            logger.info(f"ç·äºˆæ¸¬æ•°: {final_results['total_predictions']}/{final_results['total_samples']}")
            
            return final_results
        else:
            logger.error("å…¨ã¦ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§äºˆæ¸¬ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return {'mean_precision': 0.0, 'error': 'No predictions generated'}
    
    def run_enhanced_optimization(self) -> Dict[str, float]:
        """æ‹¡å¼µæœ€é©åŒ–å®Ÿè¡Œ"""
        logger.info("=== Enhanced Precision Optimizationé–‹å§‹ ===")
        logger.info(f"ç›®æ¨™ç²¾åº¦: {self.target_precision:.1%}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = self.load_data()
        
        # é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        df_enhanced = self.create_advanced_features(df)
        
        # NaNé™¤å»
        df_clean = df_enhanced.dropna(subset=['target', 'next_day_return']).copy()
        
        logger.info(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(df_clean):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {df_clean['target'].mean():.1%} (1.5%ä»¥ä¸Šä¸Šæ˜‡)")
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        results = self.train_final_model(df_clean)
        
        # çµæœä¿å­˜
        results_dir = Path("results/enhanced_precision")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        with open(results_dir / "enhanced_results.pkl", 'wb') as f:
            pickle.dump({
                'results': results,
                'target_precision': self.target_precision,
                'feature_count': len(df_enhanced.columns) - 8,  # åŸºæœ¬åˆ—ã‚’é™¤ã
                'data_size': len(df_clean)
            }, f)
        
        logger.info(f"çµæœä¿å­˜: {results_dir / 'enhanced_results.pkl'}")
        
        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        optimizer = EnhancedPrecisionOptimizer(target_precision=0.75)
        
        # æ‹¡å¼µæœ€é©åŒ–å®Ÿè¡Œ
        results = optimizer.run_enhanced_optimization()
        
        print("\n=== Enhanced Precision Optimizationçµæœ ===")
        if 'error' not in results:
            print(f"å¹³å‡ç²¾åº¦: {results['mean_precision']:.4f}")
            print(f"æ¨™æº–åå·®: {results.get('std_precision', 0):.4f}")
            print(f"å¹³å‡å†ç¾ç‡: {results.get('mean_recall', 0):.4f}")
            print(f"å¹³å‡F1ã‚¹ã‚³ã‚¢: {results.get('mean_f1', 0):.4f}")
            print(f"ç·äºˆæ¸¬æ•°: {results.get('total_predictions', 0)}")
            
            if results['mean_precision'] >= 0.75:
                print("ğŸ‰ ç›®æ¨™ç²¾åº¦ 0.75é”æˆï¼")
            else:
                print(f"âŒ ç›®æ¨™ç²¾åº¦æœªé”æˆ (å·®åˆ†: {0.75 - results['mean_precision']:.4f})")
        else:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {results['error']}")
        
        return results
        
    except Exception as e:
        logger.error(f"Enhanced Precision Optimizationå¤±æ•—: {str(e)}")
        raise


if __name__ == "__main__":
    main()