"""
Final Precision Booster - 175éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã§ã®åŠ¹ç‡çš„ç²¾åº¦å‘ä¸Š
ç›®æ¨™: Precision â‰¥ 0.75é”æˆã®ãŸã‚ã®æœ€é©åŒ–ï¼ˆè¨ˆç®—åŠ¹ç‡é‡è¦–ï¼‰
"""

import os
import pickle
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import logging

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import IsolationForest
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.calibration import CalibratedClassifierCV
import lightgbm as lgb
import catboost as cb
import xgboost as xgb

warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FinalPrecisionBooster:
    """åŠ¹ç‡çš„ãªæœ€çµ‚ç²¾åº¦å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, target_precision: float = 0.75):
        self.target_precision = target_precision
        self.scaler = RobustScaler()
        logger.info(f"Final Precision BoosteråˆæœŸåŒ–å®Œäº† (ç›®æ¨™ç²¾åº¦: {target_precision:.1%})")
    
    def load_data(self) -> pd.DataFrame:
        """175éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("175éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        
        data_dir = Path("data/nikkei225_full_data")
        pkl_files = list(data_dir.glob("nikkei225_full_10years_*.pkl"))
        latest_file = max(pkl_files, key=lambda f: f.stat().st_mtime)
        
        df = pd.read_pickle(latest_file)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰, {df['Code'].nunique()}éŠ˜æŸ„")
        
        return df
    
    def create_optimized_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŠ¹ç‡çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        logger.info("åŠ¹ç‡çš„ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        
        # åŸºæœ¬å‰å‡¦ç†
        df = df.sort_values(['Code', 'Date']).reset_index(drop=True)
        
        # æ•°å€¤ã‚«ãƒ©ãƒ æº–å‚™
        for col in ['Close', 'High', 'Low', 'Open', 'Volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # èª¿æ•´æ¸ˆã¿ä¾¡æ ¼ä½¿ç”¨
        adj_cols = ['AdjustmentClose', 'AdjustmentHigh', 'AdjustmentLow', 'AdjustmentOpen']
        for col in adj_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # åŸºæœ¬ãƒªã‚¿ãƒ¼ãƒ³
        df['daily_return'] = df.groupby('Code')['Close'].pct_change(fill_method=None)
        df['next_day_return'] = df.groupby('Code')['Close'].pct_change(fill_method=None).shift(-1)
        
        # ã‚ˆã‚Šå³ã—ã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆé«˜ç²¾åº¦ç‹™ã„ï¼‰
        df['target'] = (df['next_day_return'] >= 0.02).astype(int)  # 2%ä»¥ä¸Š
        
        logger.info("æ ¸å¿ƒçš„ç‰¹å¾´é‡è¨ˆç®—...")
        
        # é‡è¦ãªç‰¹å¾´é‡ã®ã¿ã«çµã£ã¦é«˜é€Ÿè¨ˆç®—
        windows = [5, 10, 20, 50]
        
        for window in windows:
            # ç§»å‹•å¹³å‡ãƒ»ä¾¡æ ¼æ¯”ç‡
            sma = df.groupby('Code')['Close'].transform(lambda x: x.rolling(window).mean())
            df[f'price_to_sma_{window}'] = df['Close'] / sma
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            df[f'volatility_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).std()
            )
            
            # é«˜å€¤ãƒ»å®‰å€¤æ¯”ç‡
            high_max = df.groupby('Code')['High'].transform(lambda x: x.rolling(window).max())
            low_min = df.groupby('Code')['Low'].transform(lambda x: x.rolling(window).min())
            df[f'price_position_{window}'] = (df['Close'] - low_min) / (high_max - low_min + 1e-8)
            
            # ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
            df[f'return_mean_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).mean()
            )
            df[f'return_std_{window}'] = df.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).std()
            )
            
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™
            if window <= 20:  # è¨ˆç®—è² è·å‰Šæ¸›
                vol_ma = df.groupby('Code')['Volume'].transform(lambda x: x.rolling(window).mean())
                df[f'volume_ratio_{window}'] = df['Volume'] / (vol_ma + 1)
        
        # RSI (14æ—¥ã®ã¿)
        def calc_rsi(prices, window=14):
            delta = prices.diff()
            gain = delta.where(delta > 0, 0).rolling(window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
            rs = gain / (loss + 1e-8)
            return 100 - (100 / (1 + rs))
        
        df['rsi_14'] = df.groupby('Code')['Close'].transform(calc_rsi)
        
        # MACD (ç°¡ç•¥ç‰ˆ)
        def calc_macd(group):
            ema12 = group['Close'].ewm(span=12).mean()
            ema26 = group['Close'].ewm(span=26).mean()
            macd = ema12 - ema26
            signal = macd.ewm(span=9).mean()
            return pd.DataFrame({
                'macd': macd,
                'macd_signal': signal,
                'macd_histogram': macd - signal
            })
        
        logger.info("MACDè¨ˆç®—...")
        macd_df = df.groupby('Code').apply(calc_macd)
        macd_df.index = macd_df.index.droplevel(0)
        df = df.join(macd_df)
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ (20æ—¥ã®ã¿)
        sma_20 = df.groupby('Code')['Close'].transform(lambda x: x.rolling(20).mean())
        std_20 = df.groupby('Code')['Close'].transform(lambda x: x.rolling(20).std())
        df['bb_upper'] = sma_20 + (2 * std_20)
        df['bb_lower'] = sma_20 - (2 * std_20)
        df['bb_position'] = (df['Close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-8)
        
        # ãƒ©ã‚°ç‰¹å¾´é‡ (5å€‹ã¾ã§)
        for lag in range(1, 6):
            df[f'return_lag_{lag}'] = df.groupby('Code')['daily_return'].shift(lag)
            df[f'price_lag_{lag}'] = df.groupby('Code')['Close'].shift(lag)
        
        # å¸‚å ´ã¨ã®ç›¸é–¢ (ç°¡ç•¥ç‰ˆ)
        market_return = df.groupby('Date')['daily_return'].mean()
        df['market_return'] = df['Date'].map(market_return)
        df['relative_return'] = df['daily_return'] - df['market_return']
        
        # ç•°å¸¸å€¤ç‰¹å¾´é‡
        df['price_zscore'] = df.groupby('Code')['Close'].transform(
            lambda x: (x - x.rolling(30).mean()) / (x.rolling(30).std() + 1e-8)
        )
        
        # æµå‹•æ€§
        df['liquidity'] = df['Volume'] * df['Close']
        df['liquidity_rank'] = df.groupby('Date')['liquidity'].rank(pct=True)
        
        logger.info(f"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: ç´„{len([c for c in df.columns if c not in ['Code', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'target', 'next_day_return']])}å€‹")
        
        return df
    
    def optimize_ensemble_weights(self, lgb_proba: np.ndarray, cb_proba: np.ndarray, 
                                xgb_proba: np.ndarray, y_true: np.ndarray) -> Tuple[float, float, float]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–"""
        best_precision = 0
        best_weights = (0.33, 0.33, 0.34)
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©é‡ã¿æ¢ç´¢
        for w1 in np.arange(0.2, 0.7, 0.1):
            for w2 in np.arange(0.2, 0.8 - w1, 0.1):
                w3 = 1 - w1 - w2
                if w3 < 0.1:
                    continue
                
                ensemble_proba = w1 * lgb_proba + w2 * cb_proba + w3 * xgb_proba
                
                # è¤‡æ•°é–¾å€¤ã§è©•ä¾¡
                for threshold in np.arange(0.7, 0.95, 0.05):
                    preds = (ensemble_proba >= threshold).astype(int)
                    if preds.sum() > 0:
                        precision = precision_score(y_true, preds, zero_division=0)
                        if precision > best_precision:
                            best_precision = precision
                            best_weights = (w1, w2, w3)
        
        return best_weights
    
    def train_advanced_models(self, X_train: np.ndarray, y_train: np.ndarray, 
                            X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:
        """é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        
        # LightGBM (é«˜ç²¾åº¦è¨­å®š)
        lgb_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 128,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'learning_rate': 0.05,
            'min_child_samples': 20,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbosity': -1,
            'random_state': 42,
            'n_estimators': 500
        }
        
        # CatBoost (é«˜ç²¾åº¦è¨­å®š)
        cb_params = {
            'loss_function': 'Logloss',
            'iterations': 500,
            'learning_rate': 0.05,
            'depth': 8,
            'l2_leaf_reg': 3,
            'bootstrap_type': 'Bernoulli',
            'subsample': 0.8,
            'random_seed': 42,
            'verbose': False
        }
        
        # XGBoost (é«˜ç²¾åº¦è¨­å®š)
        xgb_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'max_depth': 8,
            'learning_rate': 0.05,
            'n_estimators': 500,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 1,
            'random_state': 42
        }
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        lgb_model = lgb.LGBMClassifier(**lgb_params)
        cb_model = cb.CatBoostClassifier(**cb_params)
        xgb_model = xgb.XGBClassifier(**xgb_params)
        
        # ç¢ºç‡æ ¡æ­£
        lgb_calibrated = CalibratedClassifierCV(lgb_model, method='isotonic', cv=3)
        cb_calibrated = CalibratedClassifierCV(cb_model, method='isotonic', cv=3)
        xgb_calibrated = CalibratedClassifierCV(xgb_model, method='isotonic', cv=3)
        
        lgb_calibrated.fit(X_train, y_train)
        cb_calibrated.fit(X_train, y_train)
        xgb_calibrated.fit(X_train, y_train)
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
        lgb_proba = lgb_calibrated.predict_proba(X_val)[:, 1]
        cb_proba = cb_calibrated.predict_proba(X_val)[:, 1]
        xgb_proba = xgb_calibrated.predict_proba(X_val)[:, 1]
        
        # æœ€é©é‡ã¿è¨ˆç®—
        best_weights = self.optimize_ensemble_weights(lgb_proba, cb_proba, xgb_proba, y_val)
        
        return {
            'models': {
                'lgb': lgb_calibrated,
                'cb': cb_calibrated, 
                'xgb': xgb_calibrated
            },
            'weights': best_weights,
            'probabilities': {
                'lgb': lgb_proba,
                'cb': cb_proba,
                'xgb': xgb_proba
            }
        }
    
    def find_optimal_threshold(self, ensemble_proba: np.ndarray, y_true: np.ndarray) -> float:
        """æœ€é©é–¾å€¤æ¢ç´¢"""
        best_precision = 0
        best_threshold = 0.85
        
        for threshold in np.arange(0.5, 0.99, 0.02):
            predictions = (ensemble_proba >= threshold).astype(int)
            if predictions.sum() > 0:
                precision = precision_score(y_true, predictions, zero_division=0)
                if precision > best_precision:
                    best_precision = precision
                    best_threshold = threshold
        
        return best_threshold
    
    def run_final_optimization(self) -> Dict[str, float]:
        """æœ€çµ‚æœ€é©åŒ–å®Ÿè¡Œ"""
        logger.info("=== Final Precision Boosté–‹å§‹ ===")
        logger.info(f"ç›®æ¨™ç²¾åº¦: {self.target_precision:.1%}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df = self.load_data()
        df = self.create_optimized_features(df)
        
        # ç‰¹å¾´é‡æº–å‚™
        feature_cols = [col for col in df.columns if col not in 
                       ['Code', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume', 
                        'target', 'next_day_return'] and not col.startswith('Adjustment')]
        
        X = df[feature_cols].fillna(0)
        y = df['target']
        
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X):,}")
        logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {y.mean():.1%}")
        
        # å¤–ã‚Œå€¤é™¤å»
        outlier_detector = IsolationForest(contamination=0.05, random_state=42)
        outliers = outlier_detector.fit_predict(X) == -1
        X, y = X[~outliers], y[~outliers]
        
        logger.info(f"å¤–ã‚Œå€¤é™¤å»å¾Œ: {len(X):,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
        # ç‰¹å¾´é‡é¸æŠ (ä¸Šä½70%)
        selector = SelectKBest(f_classif, k=int(len(feature_cols) * 0.7))
        X_selected = selector.fit_transform(X, y)
        selected_features = np.array(feature_cols)[selector.get_support()]
        
        logger.info(f"é¸æŠç‰¹å¾´é‡: {len(selected_features)}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_scaled = self.scaler.fit_transform(X_selected)
        
        # æ™‚ç³»åˆ—åˆ†å‰²è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=5, gap=10)
        results = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled)):
            logger.info(f"Fold {fold + 1}/5 å®Ÿè¡Œä¸­...")
            
            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # é«˜åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            model_results = self.train_advanced_models(X_train, y_train, X_val, y_val)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            w1, w2, w3 = model_results['weights']
            ensemble_proba = (w1 * model_results['probabilities']['lgb'] + 
                            w2 * model_results['probabilities']['cb'] + 
                            w3 * model_results['probabilities']['xgb'])
            
            # æœ€é©é–¾å€¤
            threshold = self.find_optimal_threshold(ensemble_proba, y_val)
            predictions = (ensemble_proba >= threshold).astype(int)
            
            # è©•ä¾¡
            if predictions.sum() > 0:
                precision = precision_score(y_val, predictions)
                recall = recall_score(y_val, predictions)
                f1 = f1_score(y_val, predictions)
                
                results.append({
                    'fold': fold + 1,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'threshold': threshold,
                    'predictions': predictions.sum(),
                    'total': len(predictions),
                    'weights': (w1, w2, w3)
                })
                
                logger.info(f"Fold {fold + 1} - Precision: {precision:.4f}, Threshold: {threshold:.3f}, Predictions: {predictions.sum()}")
        
        # æœ€çµ‚çµæœ
        if results:
            final_results = {
                'mean_precision': np.mean([r['precision'] for r in results]),
                'std_precision': np.std([r['precision'] for r in results]),
                'mean_recall': np.mean([r['recall'] for r in results]),
                'mean_f1': np.mean([r['f1'] for r in results]),
                'mean_threshold': np.mean([r['threshold'] for r in results]),
                'total_predictions': sum([r['predictions'] for r in results]),
                'total_samples': sum([r['total'] for r in results]),
                'feature_count': len(selected_features),
                'success_rate': len([r for r in results if r['precision'] >= self.target_precision]) / len(results)
            }
            
            logger.info("=== æœ€çµ‚çµæœ ===")
            logger.info(f"å¹³å‡ç²¾åº¦: {final_results['mean_precision']:.4f} Â± {final_results['std_precision']:.4f}")
            logger.info(f"ç›®æ¨™é”æˆãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰: {final_results['success_rate']:.1%}")
            logger.info(f"å¹³å‡é–¾å€¤: {final_results['mean_threshold']:.3f}")
            logger.info(f"é¸æŠç‰¹å¾´é‡æ•°: {final_results['feature_count']}")
            
            return final_results
        else:
            return {'error': 'No valid results generated'}


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        booster = FinalPrecisionBooster(target_precision=0.75)
        results = booster.run_final_optimization()
        
        print("\n=== Final Precision Boostçµæœ ===")
        if 'error' not in results:
            print(f"å¹³å‡ç²¾åº¦: {results['mean_precision']:.4f}")
            print(f"æ¨™æº–åå·®: {results['std_precision']:.4f}")
            print(f"ç›®æ¨™é”æˆç‡: {results['success_rate']:.1%}")
            print(f"å¹³å‡é–¾å€¤: {results['mean_threshold']:.3f}")
            print(f"ç‰¹å¾´é‡æ•°: {results['feature_count']}")
            
            if results['mean_precision'] >= 0.75:
                print("ğŸ‰ ç›®æ¨™ç²¾åº¦0.75é”æˆï¼")
            else:
                shortage = 0.75 - results['mean_precision']
                print(f"âŒ ç›®æ¨™ç²¾åº¦æœªé”æˆ (ä¸è¶³: {shortage:.4f})")
                
                # ã•ã‚‰ãªã‚‹æ”¹å–„ææ¡ˆ
                if shortage < 0.05:
                    print("ğŸ’¡ ã‚ãšã‹ãªæ”¹å–„ã§é”æˆå¯èƒ½ã€‚é–¾å€¤èª¿æ•´ã‚„ç‰¹å¾´é‡è¿½åŠ ã‚’æ¨å¥¨")
                elif shortage < 0.1:
                    print("ğŸ’¡ ä¸­ç¨‹åº¦ã®æ”¹å–„ãŒå¿…è¦ã€‚ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿èª¿æ•´ã‚’æ¨å¥¨")
                else:
                    print("ğŸ’¡ å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã€‚ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®šç¾©è¦‹ç›´ã—ã‚’æ¨å¥¨")
        else:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {results['error']}")
        
        return results
        
    except Exception as e:
        logger.error(f"Final Precision Boostå¤±æ•—: {str(e)}")
        raise


if __name__ == "__main__":
    main()