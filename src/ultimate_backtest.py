"""
ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ: 10å¹´é–“å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®é«˜ç²¾åº¦AIæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼
38éŠ˜æŸ„ Ã— 10å¹´é–“ï¼ˆ92,755ä»¶ï¼‰ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæœ€çµ‚æ¤œè¨¼
ç›®æ¨™: Precision â‰¥ 0.75
"""

import os
import warnings
import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score
import lightgbm as lgb
import catboost as cb
import optuna

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Optunaã®ãƒ­ã‚°ã‚’æŠ‘åˆ¶
optuna.logging.set_verbosity(optuna.logging.WARNING)


class UltimateBacktester:
    """ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ã‚¿ãƒ¼: 10å¹´é–“å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®é«˜ç²¾åº¦æ¤œè¨¼"""
    
    def __init__(self, data_file_path: str):
        """
        åˆæœŸåŒ–
        
        Args:
            data_file_path: 10å¹´é–“å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.data_file_path = data_file_path
        self.data: Optional[pd.DataFrame] = None
        self.features: Optional[pd.DataFrame] = None
        self.target: Optional[pd.Series] = None
        self.feature_names: List[str] = []
        
        logger.info("ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file_path}")
    
    def load_data(self) -> None:
        """10å¹´é–“å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("10å¹´é–“å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        if not os.path.exists(self.data_file_path):
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.data_file_path}")
        
        self.data = pd.read_pickle(self.data_file_path)
        
        logger.info("10å¹´é–“å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
        logger.info(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(self.data):,}ä»¶")
        logger.info(f"  éŠ˜æŸ„æ•°: {self.data['Code'].nunique()}éŠ˜æŸ„")
        logger.info(f"  æœŸé–“: {self.data['Date'].min()} ï½ {self.data['Date'].max()}")
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        self.data['Date'] = pd.to_datetime(self.data['Date'])
        self.data = self.data.sort_values(['Code', 'Date']).reset_index(drop=True)
        
        logger.info("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
    
    def create_ultimate_features(self) -> None:
        """ç©¶æ¥µã®åŒ…æ‹¬ç‰¹å¾´é‡ä½œæˆï¼ˆ200+ç‰¹å¾´é‡ï¼‰"""
        logger.info("ç©¶æ¥µåŒ…æ‹¬ç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        required_cols = ['Date', 'Code', 'Close', 'Open', 'High', 'Low', 'Volume']
        for col in required_cols:
            if col not in self.data.columns:
                raise ValueError(f"å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {col}")
        
        # æ•°å€¤å‹ã«å¤‰æ›
        price_cols = ['Close', 'Open', 'High', 'Low']
        for col in price_cols:
            self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
        
        self.data['Volume'] = pd.to_numeric(self.data['Volume'], errors='coerce').fillna(0)
        
        features_list = []
        
        # éŠ˜æŸ„ã”ã¨ã«ç‰¹å¾´é‡ä½œæˆ
        for code in self.data['Code'].unique():
            code_data = self.data[self.data['Code'] == code].copy().sort_values('Date')
            
            if len(code_data) < 100:  # æœ€ä½100æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                continue
            
            # 1. åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
            code_data['close_price'] = code_data['Close']
            code_data['open_price'] = code_data['Open']
            code_data['high_price'] = code_data['High']
            code_data['low_price'] = code_data['Low']
            code_data['volume'] = code_data['Volume']
            
            # 2. ãƒªã‚¿ãƒ¼ãƒ³ç³»ç‰¹å¾´é‡ï¼ˆå¤šæœŸé–“ï¼‰
            for period in [1, 2, 3, 5, 10, 15, 20, 30]:
                code_data[f'return_{period}d'] = code_data['Close'].pct_change(period, fill_method=None)
            
            # 3. ç§»å‹•å¹³å‡ç³»ç‰¹å¾´é‡
            for window in [5, 10, 15, 20, 25, 30, 40, 50, 60, 75, 100]:
                code_data[f'sma_{window}'] = code_data['Close'].rolling(window=window, min_periods=1).mean()
                code_data[f'sma_ratio_{window}'] = code_data['Close'] / code_data[f'sma_{window}']
                
                # EMA
                code_data[f'ema_{window}'] = code_data['Close'].ewm(span=window, adjust=False).mean()
                code_data[f'ema_ratio_{window}'] = code_data['Close'] / code_data[f'ema_{window}']
            
            # 4. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»ç‰¹å¾´é‡
            for window in [5, 10, 20, 30, 60]:
                code_data[f'volatility_{window}'] = code_data['return_1d'].rolling(window=window).std()
                code_data[f'realized_vol_{window}'] = np.sqrt(
                    ((code_data['High'] / code_data['Low']).apply(np.log) ** 2).rolling(window=window).sum()
                )
            
            # 5. å‡ºæ¥é«˜ç³»ç‰¹å¾´é‡
            for window in [5, 10, 20, 30]:
                code_data[f'volume_sma_{window}'] = code_data['Volume'].rolling(window=window, min_periods=1).mean()
                code_data[f'volume_ratio_{window}'] = code_data['Volume'] / (code_data[f'volume_sma_{window}'] + 1)
                
                # å‡ºæ¥é«˜Ã—ä¾¡æ ¼æŒ‡æ¨™
                code_data[f'vwap_{window}'] = (
                    (code_data['Close'] * code_data['Volume']).rolling(window=window).sum() /
                    code_data['Volume'].rolling(window=window).sum()
                )
                code_data[f'vwap_ratio_{window}'] = code_data['Close'] / code_data[f'vwap_{window}']
            
            # 6. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
            # RSI
            for period in [9, 14, 21]:
                delta = code_data['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
                rs = gain / (loss + 1e-8)
                code_data[f'rsi_{period}'] = 100 - (100 / (1 + rs))
            
            # MACD
            exp1 = code_data['Close'].ewm(span=12, adjust=False).mean()
            exp2 = code_data['Close'].ewm(span=26, adjust=False).mean()
            code_data['macd'] = exp1 - exp2
            code_data['macd_signal'] = code_data['macd'].ewm(span=9, adjust=False).mean()
            code_data['macd_histogram'] = code_data['macd'] - code_data['macd_signal']
            
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
            for period in [20, 30]:
                sma = code_data['Close'].rolling(window=period).mean()
                std = code_data['Close'].rolling(window=period).std()
                code_data[f'bb_upper_{period}'] = sma + (std * 2)
                code_data[f'bb_lower_{period}'] = sma - (std * 2)
                code_data[f'bb_ratio_{period}'] = (code_data['Close'] - code_data[f'bb_lower_{period}']) / (
                    code_data[f'bb_upper_{period}'] - code_data[f'bb_lower_{period}'] + 1e-8
                )
            
            # 7. çµ±è¨ˆçš„ç‰¹å¾´é‡
            for window in [10, 20, 30]:
                # ååº¦ãƒ»å°–åº¦
                code_data[f'skew_{window}'] = code_data['return_1d'].rolling(window=window).skew()
                code_data[f'kurtosis_{window}'] = code_data['return_1d'].rolling(window=window).kurt()
                
                # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
                code_data[f'percentile_20_{window}'] = (
                    code_data['Close'].rolling(window=window).apply(lambda x: np.percentile(x, 20))
                )
                code_data[f'percentile_80_{window}'] = (
                    code_data['Close'].rolling(window=window).apply(lambda x: np.percentile(x, 80))
                )
                code_data[f'percentile_ratio_{window}'] = (
                    (code_data['Close'] - code_data[f'percentile_20_{window}']) /
                    (code_data[f'percentile_80_{window}'] - code_data[f'percentile_20_{window}'] + 1e-8)
                )
            
            # 8. ç›¸å¯¾å¼·åº¦æŒ‡æ¨™
            # ä¾¡æ ¼ãƒ©ãƒ³ã‚¯
            for window in [20, 50, 100]:
                code_data[f'rank_{window}'] = (
                    code_data['Close'].rolling(window=window).rank(pct=True)
                )
            
            # 9. å­£ç¯€æ€§ãƒ»æ™‚ç³»åˆ—ç‰¹å¾´é‡
            code_data['day_of_week'] = code_data['Date'].dt.dayofweek
            code_data['day_of_month'] = code_data['Date'].dt.day
            code_data['month'] = code_data['Date'].dt.month
            code_data['quarter'] = code_data['Date'].dt.quarter
            code_data['day_of_year'] = code_data['Date'].dt.dayofyear
            
            # 10. ãƒ©ã‚°ç‰¹å¾´é‡
            for lag in [1, 2, 3, 5]:
                code_data[f'close_lag_{lag}'] = code_data['Close'].shift(lag)
                code_data[f'volume_lag_{lag}'] = code_data['Volume'].shift(lag)
                code_data[f'return_lag_{lag}'] = code_data['return_1d'].shift(lag)
            
            # 11. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆ
            code_data['next_day_return'] = code_data['Close'].pct_change(fill_method=None).shift(-1)
            code_data['target'] = (code_data['next_day_return'] >= 0.01).astype(int)
            
            features_list.append(code_data)
        
        # å…¨ãƒ‡ãƒ¼ã‚¿çµåˆ
        self.data = pd.concat(features_list, ignore_index=True)
        
        # æ¬ æå€¤å‡¦ç†
        logger.info("æ¬ æå€¤å‡¦ç†ä¸­...")
        self.data = self.data.fillna(method='ffill').fillna(method='bfill')
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†é›¢
        exclude_cols = ['Date', 'Code', 'Close', 'Open', 'High', 'Low', 'Volume', 'next_day_return', 'target']
        self.feature_names = [col for col in self.data.columns if col not in exclude_cols and not col.startswith('sma_') and not col.startswith('ema_')]
        
        # æ•°å€¤ç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
        numeric_features = []
        for col in self.feature_names:
            if pd.api.types.is_numeric_dtype(self.data[col]):
                numeric_features.append(col)
        
        self.feature_names = numeric_features
        
        self.features = self.data[self.feature_names].copy()
        self.target = self.data['target'].copy()
        
        # ç„¡é™å€¤ã‚„ç•°å¸¸å€¤ã®å‡¦ç†
        self.features = self.features.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        # æœ€çµ‚çš„ãªã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
        valid_indices = ~(self.target.isna() | (self.target < 0))
        self.features = self.features[valid_indices].reset_index(drop=True)
        self.target = self.target[valid_indices].reset_index(drop=True)
        self.data = self.data[valid_indices].reset_index(drop=True)
        
        logger.info(f"ç©¶æ¥µåŒ…æ‹¬ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(self.feature_names)}å€‹")
        logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿: {len(self.features):,}ä»¶")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {len(self.feature_names)}å€‹")
        logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {self.target.mean():.1%} (ä¸Šæ˜‡)")
    
    def optimize_lightgbm(self, X_train, y_train, X_val, y_val, n_trials: int = 50) -> Dict:
        """LightGBMãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        
        def objective(trial):
            params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                'verbosity': -1
            }
            
            model = lgb.LGBMClassifier(**params, n_estimators=100, random_state=42)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            
            # é«˜ä¿¡é ¼åº¦äºˆæ¸¬ã®ã¿ã§Precisionè¨ˆç®—
            y_proba = model.predict_proba(X_val)[:, 1]
            high_conf_threshold = np.percentile(y_proba, 85)  # ä¸Šä½15%
            high_conf_mask = y_proba >= high_conf_threshold
            
            if high_conf_mask.sum() > 0:
                precision = precision_score(y_val[high_conf_mask], y_pred[high_conf_mask], zero_division=0)
            else:
                precision = 0
            
            return precision
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        return study.best_params
    
    def optimize_catboost(self, X_train, y_train, X_val, y_val, n_trials: int = 50) -> Dict:
        """CatBoostãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        
        def objective(trial):
            params = {
                'iterations': 100,
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'depth': trial.suggest_int('depth', 4, 10),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
                'border_count': trial.suggest_int('border_count', 30, 200),
                'random_seed': 42,
                'verbose': False
            }
            
            model = cb.CatBoostClassifier(**params)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            
            # é«˜ä¿¡é ¼åº¦äºˆæ¸¬ã®ã¿ã§Precisionè¨ˆç®—
            y_proba = model.predict_proba(X_val)[:, 1]
            high_conf_threshold = np.percentile(y_proba, 85)
            high_conf_mask = y_proba >= high_conf_threshold
            
            if high_conf_mask.sum() > 0:
                precision = precision_score(y_val[high_conf_mask], y_pred[high_conf_mask], zero_division=0)
            else:
                precision = 0
            
            return precision
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        return study.best_params
    
    def run_ultimate_backtest(self) -> Dict[str, float]:
        """ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("=== ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        
        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ8åˆ†å‰²ï¼‰
        tscv = TimeSeriesSplit(n_splits=8, gap=5)
        
        all_results = []
        fold_results = []
        
        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(self.features)):
            logger.info(f"Fold {fold_idx + 1}/8 å®Ÿè¡Œä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test = self.features.iloc[train_idx], self.features.iloc[test_idx]
            y_train, y_test = self.target.iloc[train_idx], self.target.iloc[test_idx]
            
            # ã•ã‚‰ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆæ¤œè¨¼ç”¨ï¼‰
            val_split = int(len(X_train) * 0.8)
            X_train_opt, X_val = X_train.iloc[:val_split], X_train.iloc[val_split:]
            y_train_opt, y_val = y_train.iloc[:val_split], y_train.iloc[val_split:]
            
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
            logger.info("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")
            
            lgb_params = self.optimize_lightgbm(X_train_opt, y_train_opt, X_val, y_val, n_trials=30)
            cb_params = self.optimize_catboost(X_train_opt, y_train_opt, X_val, y_val, n_trials=30)
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            lgb_model = lgb.LGBMClassifier(**lgb_params, n_estimators=200, random_state=42)
            cb_model = cb.CatBoostClassifier(**cb_params, iterations=200, random_state=42, verbose=False)
            lr_model = LogisticRegression(random_state=42, max_iter=1000)
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ç”¨ï¼‰
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            lgb_model.fit(X_train, y_train)
            cb_model.fit(X_train, y_train)
            lr_model.fit(X_train_scaled, y_train)
            
            # äºˆæ¸¬
            lgb_proba = lgb_model.predict_proba(X_test)[:, 1]
            cb_proba = cb_model.predict_proba(X_test)[:, 1]
            lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
            ensemble_proba = 0.45 * lgb_proba + 0.45 * cb_proba + 0.1 * lr_proba
            
            # é«˜ä¿¡é ¼åº¦é–¾å€¤ã§ã®äºˆæ¸¬
            thresholds = [0.75, 0.8, 0.85, 0.9, 0.95]
            
            for threshold in thresholds:
                high_conf_threshold = np.percentile(ensemble_proba, threshold * 100)
                high_conf_mask = ensemble_proba >= high_conf_threshold
                
                if high_conf_mask.sum() > 0:
                    ensemble_pred = (ensemble_proba >= high_conf_threshold).astype(int)
                    y_pred_high_conf = ensemble_pred[high_conf_mask]
                    y_test_high_conf = y_test.iloc[high_conf_mask]
                    
                    precision = precision_score(y_test_high_conf, y_pred_high_conf, zero_division=0)
                    recall = recall_score(y_test_high_conf, y_pred_high_conf, zero_division=0)
                    f1 = f1_score(y_test_high_conf, y_pred_high_conf, zero_division=0)
                    
                    results = {
                        'fold': fold_idx + 1,
                        'threshold': threshold,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1,
                        'n_predictions': high_conf_mask.sum(),
                        'n_total': len(y_test)
                    }
                    
                    all_results.append(results)
                    
                    if threshold == 0.85:  # ä¸»è¦é–¾å€¤ã§ã®çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
                        logger.info(f"  Fold {fold_idx + 1} - Precision: {precision:.3f}, Recall: {recall:.3f}")
            
            fold_results.append({
                'fold': fold_idx + 1,
                'lgb_params': lgb_params,
                'cb_params': cb_params,
                'completed': True
            })
        
        # çµæœé›†ç´„
        results_df = pd.DataFrame(all_results)
        
        final_results = {}
        for threshold in thresholds:
            threshold_results = results_df[results_df['threshold'] == threshold]
            if len(threshold_results) > 0:
                avg_precision = threshold_results['precision'].mean()
                avg_recall = threshold_results['recall'].mean()
                avg_f1 = threshold_results['f1'].mean()
                avg_predictions = threshold_results['n_predictions'].mean()
                
                final_results[f'precision_{threshold}'] = avg_precision
                final_results[f'recall_{threshold}'] = avg_recall
                final_results[f'f1_{threshold}'] = avg_f1
                final_results[f'n_predictions_{threshold}'] = avg_predictions
        
        logger.info("=== ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Œäº† ===")
        
        # çµæœè¡¨ç¤º
        for threshold in thresholds:
            if f'precision_{threshold}' in final_results:
                precision = final_results[f'precision_{threshold}']
                recall = final_results[f'recall_{threshold}']
                n_pred = final_results[f'n_predictions_{threshold}']
                logger.info(f"é–¾å€¤ {threshold}: Precision={precision:.3f}, Recall={recall:.3f}, äºˆæ¸¬æ•°={n_pred:.0f}")
                
                if precision >= 0.75:
                    logger.info(f"ğŸ‰ ç›®æ¨™é”æˆ! é–¾å€¤{threshold}ã§Precision={precision:.3f} â‰¥ 0.75")
        
        return final_results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    data_file = "data/maximum_period_data/maximum_period_10.00years_38stocks_20250831_013317.pkl"
    
    if not os.path.exists(data_file):
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_file}")
        
        # ä»£æ›¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        data_dir = Path("data/maximum_period_data")
        if data_dir.exists():
            pkl_files = list(data_dir.glob("*.pkl"))
            if pkl_files:
                data_file = str(pkl_files[-1])  # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«
                logger.info(f"ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {data_file}")
            else:
                logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
        else:
            logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
    
    try:
        # ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        backtester = UltimateBacktester(data_file)
        backtester.load_data()
        backtester.create_ultimate_features()
        
        results = backtester.run_ultimate_backtest()
        
        print("\n=== ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœ€çµ‚çµæœ ===")
        print(f"ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: 38éŠ˜æŸ„ Ã— 10å¹´é–“")
        print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(backtester.features):,}ä»¶")
        print(f"ç‰¹å¾´é‡æ•°: {len(backtester.feature_names)}å€‹")
        
        # ä¸»è¦çµæœè¡¨ç¤º
        for threshold in [0.75, 0.8, 0.85, 0.9, 0.95]:
            if f'precision_{threshold}' in results:
                precision = results[f'precision_{threshold}']
                recall = results[f'recall_{threshold}']
                print(f"é–¾å€¤ {threshold}: Precision={precision:.3f}, Recall={recall:.3f}")
                
                if precision >= 0.75:
                    print(f"ğŸ¯ ç›®æ¨™é”æˆ! Precision={precision:.3f} â‰¥ 0.75")
        
        # æœ€é«˜ç²¾åº¦ã‚’ç‰¹å®š
        max_precision = 0
        best_threshold = 0
        for threshold in [0.75, 0.8, 0.85, 0.9, 0.95]:
            if f'precision_{threshold}' in results:
                precision = results[f'precision_{threshold}']
                if precision > max_precision:
                    max_precision = precision
                    best_threshold = threshold
        
        print(f"\næœ€é«˜ç²¾åº¦: {max_precision:.3f} (é–¾å€¤ {best_threshold})")
        
        if max_precision >= 0.75:
            print("âœ… ç›®æ¨™é”æˆï¼é«˜ç²¾åº¦AIæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†")
        else:
            print(f"âŒ ç›®æ¨™æœªé”æˆ (ç¾åœ¨: {max_precision:.3f} < ç›®æ¨™: 0.75)")
            print("ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦ã§ã™")
        
        return results
        
    except Exception as e:
        logger.error(f"ç©¶æ¥µãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«å¤±æ•—: {str(e)}")
        raise


if __name__ == "__main__":
    main()