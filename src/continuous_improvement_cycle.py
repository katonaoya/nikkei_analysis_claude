"""
ç¶™ç¶šçš„ç²¾åº¦æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«
é•·æ™‚é–“æ¤œè¨¼ã«ã‚ˆã‚‹æ®µéšçš„ç²¾åº¦å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import time
from datetime import datetime
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ContinuousImprovementSystem:
    """ç¶™ç¶šçš„æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.df = None
        self.best_score = 0.0
        self.best_config = {}
        self.improvement_history = []
        self.cycle_count = 0
        
        # çµæœä¿å­˜ç”¨
        self.results_dir = Path("results/continuous_improvement")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.load_data()
        logger.info("ç¶™ç¶šçš„æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        data_file = Path("data/nikkei225_full_data/nikkei225_full_10years_175stocks_20250831_020101.pkl")
        self.df = pd.read_pickle(data_file)
        
        # åŸºæœ¬å‰å‡¦ç†
        self.df = self.df.sort_values(['Code', 'Date']).reset_index(drop=True)
        self.df['close_price'] = pd.to_numeric(self.df['Close'], errors='coerce')
        self.df['high_price'] = pd.to_numeric(self.df['High'], errors='coerce') 
        self.df['low_price'] = pd.to_numeric(self.df['Low'], errors='coerce')
        self.df['volume'] = pd.to_numeric(self.df['Volume'], errors='coerce')
        
        self.df['daily_return'] = self.df.groupby('Code')['close_price'].pct_change(fill_method=None)
        self.df['next_day_return'] = self.df.groupby('Code')['close_price'].pct_change(fill_method=None).shift(-1)
        self.df['target'] = (self.df['next_day_return'] >= 0.01).astype(int)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {self.df['target'].mean():.1%}")
    
    def create_feature_set_v1(self, df):
        """åŸºæœ¬ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ v1"""
        df_features = df.copy()
        
        # ç§»å‹•å¹³å‡ç³»
        for window in [5, 10, 20]:
            sma = df_features.groupby('Code')['close_price'].transform(lambda x: x.rolling(window).mean())
            df_features[f'sma_ratio_{window}'] = df_features['close_price'] / sma
            
        # RSI
        def calc_rsi(prices, window=14):
            delta = prices.diff()
            gain = delta.where(delta > 0, 0).rolling(window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
            rs = gain / (loss + 1e-8)
            return 100 - (100 / (1 + rs))
        
        df_features['rsi'] = df_features.groupby('Code')['close_price'].transform(calc_rsi)
        
        # ãƒ©ã‚°ç‰¹å¾´é‡
        for lag in [1, 2, 3]:
            df_features[f'return_lag_{lag}'] = df_features.groupby('Code')['daily_return'].shift(lag)
        
        feature_cols = [col for col in df_features.columns 
                       if col.startswith(('sma_ratio', 'rsi', 'return_lag'))]
        return df_features, feature_cols
    
    def create_feature_set_v2(self, df):
        """æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆ v2"""
        df_features, basic_cols = self.create_feature_set_v1(df)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»
        for window in [5, 10, 20]:
            df_features[f'volatility_{window}'] = df_features.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).std()
            )
            
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç³»
        for period in [5, 10, 20]:
            df_features[f'momentum_{period}'] = df_features.groupby('Code')['close_price'].transform(
                lambda x: x.pct_change(period, fill_method=None)
            )
            
        # ä¾¡æ ¼ä½ç½®
        for window in [10, 20]:
            high_max = df_features.groupby('Code')['high_price'].transform(lambda x: x.rolling(window).max())
            low_min = df_features.groupby('Code')['low_price'].transform(lambda x: x.rolling(window).min())
            df_features[f'price_position_{window}'] = (
                (df_features['close_price'] - low_min) / (high_max - low_min + 1e-8)
            )
            
        enhanced_cols = [col for col in df_features.columns 
                        if col.startswith(('sma_ratio', 'rsi', 'return_lag', 'volatility', 
                                         'momentum', 'price_position'))]
        return df_features, enhanced_cols
    
    def create_feature_set_v3(self, df):
        """é«˜åº¦ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ v3"""
        df_features, v2_cols = self.create_feature_set_v2(df)
        
        # å¸‚å ´é–¢é€£
        market_return = df_features.groupby('Date')['daily_return'].mean()
        df_features['market_return'] = df_features['Date'].map(market_return)
        df_features['excess_return'] = df_features['daily_return'] - df_features['market_return']
        
        # ãƒ™ãƒ¼ã‚¿
        for window in [20, 60]:
            df_features[f'beta_{window}'] = df_features.groupby('Code').apply(
                lambda x: x['daily_return'].rolling(window).corr(x['market_return'])
            ).values
            
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆ†æ
        df_features['volume_ma_ratio'] = df_features.groupby('Code').apply(
            lambda x: x['volume'] / x['volume'].rolling(20).mean()
        ).values
        
        # çµ±è¨ˆçš„ç‰¹å¾´é‡
        for window in [10, 20]:
            df_features[f'return_skew_{window}'] = df_features.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).skew()
            )
            df_features[f'return_kurt_{window}'] = df_features.groupby('Code')['daily_return'].transform(
                lambda x: x.rolling(window).kurt()
            )
            
        # ä¾¡æ ¼ã‚®ãƒ£ãƒƒãƒ—
        df_features['gap_up'] = ((df_features['close_price'] / 
                                df_features.groupby('Code')['close_price'].shift(1)) - 1 > 0.02).astype(int)
        df_features['gap_down'] = ((df_features['close_price'] / 
                                  df_features.groupby('Code')['close_price'].shift(1)) - 1 < -0.02).astype(int)
        
        advanced_cols = [col for col in df_features.columns 
                        if col.startswith(('sma_ratio', 'rsi', 'return_lag', 'volatility', 
                                         'momentum', 'price_position', 'excess_return', 
                                         'beta', 'volume_ma_ratio', 'return_skew', 
                                         'return_kurt', 'gap_up', 'gap_down'))]
        return df_features, advanced_cols
    
    def test_models(self, X_train, y_train, X_val, y_val):
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, 
                                                  random_state=42, n_jobs=-1),
            'ExtraTrees': ExtraTreesClassifier(n_estimators=200, max_depth=10,
                                             random_state=42, n_jobs=-1),
            'LightGBM': lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05,
                                         max_depth=8, random_state=42, verbosity=-1),
            'XGBoost': xgb.XGBClassifier(n_estimators=200, learning_rate=0.05,
                                       max_depth=8, random_state=42, verbosity=0),
            'CatBoost': cb.CatBoostClassifier(iterations=200, learning_rate=0.05,
                                            depth=8, random_seed=42, verbose=False),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=200, 
                                                         learning_rate=0.05,
                                                         max_depth=8, random_state=42)
        }
        
        results = {}
        
        for name, model in models.items():
            try:
                model.fit(X_train, y_train)
                proba = model.predict_proba(X_val)[:, 1]
                
                # è¤‡æ•°é–¾å€¤ã§è©•ä¾¡
                best_score = 0
                best_threshold = 0.5
                best_predictions = 0
                
                for threshold in np.arange(0.5, 0.85, 0.05):
                    predictions = (proba >= threshold).astype(int)
                    if predictions.sum() > 0:
                        precision = precision_score(y_val, predictions)
                        if precision > best_score:
                            best_score = precision
                            best_threshold = threshold
                            best_predictions = predictions.sum()
                
                results[name] = {
                    'precision': best_score,
                    'threshold': best_threshold,
                    'predictions': best_predictions,
                    'model': model
                }
                
            except Exception as e:
                logger.error(f"{name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results[name] = {'precision': 0, 'error': str(e)}
        
        return results
    
    def test_feature_selection(self, X_train, y_train, X_val, y_val):
        """ç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã®ãƒ†ã‚¹ãƒˆ"""
        base_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        
        selectors = {
            'All_Features': None,
            'SelectKBest_f': SelectKBest(f_classif, k=min(20, X_train.shape[1])),
            'SelectKBest_mi': SelectKBest(mutual_info_classif, k=min(20, X_train.shape[1])),
            'RFE': RFE(base_model, n_features_to_select=min(15, X_train.shape[1]), step=2)
        }
        
        results = {}
        
        for name, selector in selectors.items():
            try:
                if selector is None:
                    X_train_selected = X_train
                    X_val_selected = X_val
                else:
                    X_train_selected = selector.fit_transform(X_train, y_train)
                    X_val_selected = selector.transform(X_val)
                
                model = RandomForestClassifier(n_estimators=200, max_depth=10,
                                             random_state=42, n_jobs=-1)
                model.fit(X_train_selected, y_train)
                proba = model.predict_proba(X_val_selected)[:, 1]
                
                best_score = 0
                for threshold in [0.5, 0.6, 0.7]:
                    predictions = (proba >= threshold).astype(int)
                    if predictions.sum() > 0:
                        precision = precision_score(y_val, predictions)
                        if precision > best_score:
                            best_score = precision
                
                results[name] = {
                    'precision': best_score,
                    'features_selected': X_train_selected.shape[1]
                }
                
            except Exception as e:
                logger.error(f"ç‰¹å¾´é‡é¸æŠ {name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results[name] = {'precision': 0, 'error': str(e)}
        
        return results
    
    def test_preprocessing(self, X_train, y_train, X_val, y_val):
        """å‰å‡¦ç†æ‰‹æ³•ã®ãƒ†ã‚¹ãƒˆ"""
        scalers = {
            'RobustScaler': RobustScaler(),
            'StandardScaler': StandardScaler(),
            'MinMaxScaler': MinMaxScaler(),
            'NoScaling': None
        }
        
        results = {}
        
        for name, scaler in scalers.items():
            try:
                if scaler is None:
                    X_train_scaled = X_train
                    X_val_scaled = X_val
                else:
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_val_scaled = scaler.transform(X_val)
                
                model = RandomForestClassifier(n_estimators=200, max_depth=10,
                                             random_state=42, n_jobs=-1)
                model.fit(X_train_scaled, y_train)
                proba = model.predict_proba(X_val_scaled)[:, 1]
                
                best_score = 0
                for threshold in [0.5, 0.6, 0.7]:
                    predictions = (proba >= threshold).astype(int)
                    if predictions.sum() > 0:
                        precision = precision_score(y_val, predictions)
                        if precision > best_score:
                            best_score = precision
                
                results[name] = {'precision': best_score}
                
            except Exception as e:
                logger.error(f"å‰å‡¦ç† {name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results[name] = {'precision': 0, 'error': str(e)}
        
        return results
    
    def run_improvement_cycle(self):
        """æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ"""
        self.cycle_count += 1
        cycle_start_time = time.time()
        
        logger.info(f"=== æ”¹å–„ã‚µã‚¤ã‚¯ãƒ« #{self.cycle_count} é–‹å§‹ ===")
        
        # ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ¯”è¼ƒ
        feature_sets = {
            'v1_basic': self.create_feature_set_v1,
            'v2_enhanced': self.create_feature_set_v2,
            'v3_advanced': self.create_feature_set_v3
        }
        
        best_cycle_score = 0
        best_cycle_config = {}
        
        for set_name, create_func in feature_sets.items():
            logger.info(f"\nç‰¹å¾´é‡ã‚»ãƒƒãƒˆ {set_name} ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            
            try:
                df_features, feature_cols = create_func(self.df)
                
                X = df_features[feature_cols].fillna(0)
                y = df_features['target']
                
                # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ã¿
                valid_mask = ~(y.isna() | X.isna().any(axis=1))
                X, y = X[valid_mask], y[valid_mask]
                
                if len(X) < 10000:
                    logger.warning(f"{set_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                    continue
                
                # Train/Validationåˆ†å‰²
                split_point = int(len(X) * 0.8)
                X_train, X_val = X.iloc[:split_point], X.iloc[split_point:]
                y_train, y_val = y.iloc[:split_point], y.iloc[split_point:]
                
                logger.info(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}, è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}")
                
                # 1. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
                
                model_results = self.test_models(X_train_scaled, y_train, X_val_scaled, y_val)
                
                best_model_name = max(model_results.keys(), 
                                    key=lambda x: model_results[x].get('precision', 0))
                best_model_score = model_results[best_model_name]['precision']
                
                logger.info(f"æœ€é«˜ãƒ¢ãƒ‡ãƒ« {best_model_name}: {best_model_score:.3f}")
                
                # 2. ç‰¹å¾´é‡é¸æŠãƒ†ã‚¹ãƒˆ
                selection_results = self.test_feature_selection(X_train_scaled, y_train, 
                                                              X_val_scaled, y_val)
                
                best_selection = max(selection_results.keys(),
                                   key=lambda x: selection_results[x].get('precision', 0))
                best_selection_score = selection_results[best_selection]['precision']
                
                logger.info(f"æœ€é«˜ç‰¹å¾´é‡é¸æŠ {best_selection}: {best_selection_score:.3f}")
                
                # 3. å‰å‡¦ç†ãƒ†ã‚¹ãƒˆ
                preprocessing_results = self.test_preprocessing(X_train, y_train, X_val, y_val)
                
                best_preprocessing = max(preprocessing_results.keys(),
                                       key=lambda x: preprocessing_results[x].get('precision', 0))
                best_preprocessing_score = preprocessing_results[best_preprocessing]['precision']
                
                logger.info(f"æœ€é«˜å‰å‡¦ç† {best_preprocessing}: {best_preprocessing_score:.3f}")
                
                # ã“ã®ã‚»ãƒƒãƒˆã®æœ€é«˜ã‚¹ã‚³ã‚¢
                set_best_score = max(best_model_score, best_selection_score, best_preprocessing_score)
                
                if set_best_score > best_cycle_score:
                    best_cycle_score = set_best_score
                    best_cycle_config = {
                        'feature_set': set_name,
                        'feature_count': len(feature_cols),
                        'best_model': best_model_name,
                        'best_selection': best_selection,
                        'best_preprocessing': best_preprocessing,
                        'score': set_best_score,
                        'model_results': model_results,
                        'selection_results': selection_results,
                        'preprocessing_results': preprocessing_results
                    }
                
            except Exception as e:
                logger.error(f"ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ {set_name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚µã‚¤ã‚¯ãƒ«çµæœã®è¨˜éŒ²
        cycle_time = time.time() - cycle_start_time
        
        improvement_record = {
            'cycle': self.cycle_count,
            'timestamp': datetime.now().isoformat(),
            'duration_minutes': cycle_time / 60,
            'best_score': best_cycle_score,
            'config': best_cycle_config,
            'improvement': best_cycle_score - self.best_score
        }
        
        self.improvement_history.append(improvement_record)
        
        # å…¨ä½“æœ€é«˜è¨˜éŒ²æ›´æ–°ãƒã‚§ãƒƒã‚¯
        if best_cycle_score > self.best_score:
            self.best_score = best_cycle_score
            self.best_config = best_cycle_config.copy()
            logger.info(f"ğŸ‰ NEW BEST SCORE: {best_cycle_score:.3f} (æ”¹å–„: +{best_cycle_score - self.best_score:.3f})")
        else:
            logger.info(f"ã‚µã‚¤ã‚¯ãƒ«æœ€é«˜: {best_cycle_score:.3f}, å…¨ä½“æœ€é«˜: {self.best_score:.3f}")
        
        # çµæœä¿å­˜
        self.save_results()
        
        return improvement_record
    
    def save_results(self):
        """çµæœä¿å­˜"""
        # å±¥æ­´ä¿å­˜
        history_file = self.results_dir / f"improvement_history_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        
        import json
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump({
                'best_score': self.best_score,
                'best_config': self.best_config,
                'cycle_count': self.cycle_count,
                'history': self.improvement_history
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"çµæœä¿å­˜: {history_file}")
    
    def run_continuous_improvement(self, max_cycles=10, target_score=0.85):
        """ç¶™ç¶šçš„æ”¹å–„ã®å®Ÿè¡Œ"""
        logger.info(f"=== ç¶™ç¶šçš„æ”¹å–„é–‹å§‹ ===")
        logger.info(f"æœ€å¤§ã‚µã‚¤ã‚¯ãƒ«æ•°: {max_cycles}, ç›®æ¨™ç²¾åº¦: {target_score:.1%}")
        
        start_time = time.time()
        
        for cycle in range(max_cycles):
            try:
                cycle_result = self.run_improvement_cycle()
                
                # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                if self.best_score >= target_score:
                    logger.info(f"ğŸ¯ ç›®æ¨™ç²¾åº¦ {target_score:.1%} é”æˆï¼")
                    break
                
                # é€²æ—å ±å‘Š
                elapsed_hours = (time.time() - start_time) / 3600
                logger.info(f"çµŒéæ™‚é–“: {elapsed_hours:.1f}æ™‚é–“, é€²æ—: {cycle+1}/{max_cycles} ã‚µã‚¤ã‚¯ãƒ«")
                
                # çŸ­ã„ä¼‘æ†©
                time.sleep(10)
                
            except KeyboardInterrupt:
                logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
                break
            except Exception as e:
                logger.error(f"ã‚µã‚¤ã‚¯ãƒ« {cycle+1} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æœ€çµ‚çµæœ
        total_time = (time.time() - start_time) / 3600
        
        logger.info(f"\n=== ç¶™ç¶šçš„æ”¹å–„å®Œäº† ===")
        logger.info(f"å®Ÿè¡Œæ™‚é–“: {total_time:.1f}æ™‚é–“")
        logger.info(f"å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°: {self.cycle_count}")
        logger.info(f"æœ€é«˜ç²¾åº¦: {self.best_score:.3f}")
        logger.info(f"æœ€é©è¨­å®š: {self.best_config['feature_set']} + {self.best_config['best_model']}")
        
        return {
            'final_score': self.best_score,
            'best_config': self.best_config,
            'total_cycles': self.cycle_count,
            'total_time_hours': total_time,
            'improvement_history': self.improvement_history
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=== ç¶™ç¶šçš„ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ  ===")
    
    try:
        system = ContinuousImprovementSystem()
        
        # é•·æ™‚é–“ç¶™ç¶šçš„æ”¹å–„ã‚’å®Ÿè¡Œ
        final_results = system.run_continuous_improvement(
            max_cycles=20,  # æœ€å¤§20ã‚µã‚¤ã‚¯ãƒ«
            target_score=0.85  # 85%ç›®æ¨™
        )
        
        print(f"\n=== æœ€çµ‚çµæœ ===")
        print(f"æœ€é«˜ç²¾åº¦: {final_results['final_score']:.1%}")
        print(f"å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«: {final_results['total_cycles']}")
        print(f"ç·å®Ÿè¡Œæ™‚é–“: {final_results['total_time_hours']:.1f}æ™‚é–“")
        
        if final_results['final_score'] >= 0.85:
            print("ğŸ‰ ç›®æ¨™ç²¾åº¦85%é”æˆï¼")
        else:
            print(f"ç›®æ¨™ã¾ã§: {0.85 - final_results['final_score']:.3f}")
            
    except Exception as e:
        logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        raise


if __name__ == "__main__":
    main()