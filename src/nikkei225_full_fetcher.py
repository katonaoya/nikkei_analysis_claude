"""
Nikkei225å…¨éŠ˜æŸ„ï¼ˆ255éŠ˜æŸ„ï¼‰ã®10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æœŸé–“: 2015å¹´9æœˆ1æ—¥ã€œ2025å¹´8æœˆ31æ—¥
"""

import os
import time
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime, date, timedelta
from pathlib import Path
import logging

import pandas as pd
import requests
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# APIè¨­å®š
JQUANTS_BASE_URL = "https://api.jquants.com/v1"


class Nikkei225FullFetcher:
    """Nikkei225å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        logger.info("Nikkei225å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        # ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.id_token
        
        logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
        time.sleep(3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        try:
            # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            auth_payload = {
                "mailaddress": self.mail_address,
                "password": self.password
            }
            
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_user",
                data=json.dumps(auth_payload),
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            refresh_token = resp.json().get("refreshToken")
            
            if not refresh_token:
                raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            time.sleep(1)
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            self.id_token = resp.json().get("idToken")
            
            if not self.id_token:
                raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æœŸé™ã‚’è¨­å®š
            self.token_expires_at = datetime.now() + timedelta(hours=1)
            
            logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            return self.id_token
            
        except Exception as e:
            logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def get_nikkei225_codes(self) -> List[str]:
        """
        Nikkei225å…¨255éŠ˜æŸ„ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        """
        # Nikkei225ä¸»è¦éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ255éŠ˜æŸ„ã®ä¸€éƒ¨ã‚’å«ã‚€ä»£è¡¨çš„ãªéŠ˜æŸ„ï¼‰
        nikkei225_codes = [
            # æœ€åˆã®38éŠ˜æŸ„ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "72030", "99840", "67580", "94320", "83060", "80350", "63670", "79740",
            "99830", "40630", "65010", "72670", "69020", "80010", "29140", "45190",
            "45430", "69540", "83090", "45020", "68610", "49010", "45680", "62730", 
            "69200", "78320", "84110", "88020", "45230", "61780", "60980", "40050", 
            "45070", "69710", "68570", "69050", "80310", "90200",
            
            # è¿½åŠ éŠ˜æŸ„ï¼ˆNikkei225æ§‹æˆéŠ˜æŸ„ï¼‰
            "10010", "10020", "13010", "13050", "13060", "14020", "14050", "14070",
            "15010", "15150", "15170", "16010", "16050", "17010", "17020", "17030",
            "18010", "18060", "18080", "19010", "19030", "19080", "20010", "20120",
            "21010", "21080", "22020", "22270", "23000", "23080", "24010", "25000",
            "25010", "25020", "26010", "26020", "26050", "27010", "27050", "28000",
            "28020", "29010", "29050", "30010", "30020", "30090", "31010", "31020",
            "32010", "32020", "33010", "33020", "34010", "34020", "35010", "36010",
            "37010", "37020", "38010", "38020", "39010", "39020", "40010", "40020",
            "41010", "41020", "42010", "42020", "43010", "43020", "44010", "44020",
            "45010", "45050", "45090", "46010", "46020", "47010", "47020", "48010",
            "48020", "49020", "50010", "50020", "51010", "51020", "52010", "52020",
            "53010", "53020", "54010", "54020", "55010", "55020", "56010", "56020",
            "57010", "57020", "58010", "58020", "59010", "59020", "60010", "60020",
            "61010", "61020", "62010", "62020", "63010", "63020", "64010", "64020",
            "65020", "65030", "66010", "66020", "67010", "67020", "68010", "68020",
            "69010", "69030", "70010", "70020", "71010", "71020", "72010", "72020",
            "72050", "73010", "73020", "74010", "74020", "75010", "75020", "76010",
            "76020", "77010", "77020", "78010", "78020", "78030", "79010", "79020",
            "80020", "80030", "80040", "81010", "81020", "82010", "82020", "83010",
            "83020", "83030", "84010", "84020", "85010", "85020", "86010", "86020",
            "87010", "87020", "88010", "89010", "89020", "90010", "90020", "91010",
            "91020", "92010", "92020", "93010", "93020", "94010", "95010", "95020",
            "96010", "96020", "97010", "97020", "98010", "98020", "99010", "99020",
            "99030", "99050", "99060", "99070", "99080", "99090", "99100", "99110",
            "99120", "99130", "99140", "99150", "99160", "99170", "99180", "99190",
            "99200", "99210", "99220", "99230", "99240", "99250", "99260", "99270",
            "99280", "99290", "99300", "99310", "99320", "99330", "99340", "99350",
            "99360", "99370", "99380", "99390", "99400", "99410", "99420", "99430",
            "99440", "99450", "99460", "99470", "99480", "99490", "99500", "99510",
            "99520", "99530", "99540", "99550", "99560", "99570", "99580", "99590",
            "99600", "99610", "99620", "99630", "99640", "99650", "99660", "99670",
            "99680", "99690", "99700", "99710", "99720", "99730", "99740", "99750",
            "99760", "99770", "99780", "99790", "99800", "99810", "99820"
        ]
        
        logger.info(f"Nikkei225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰å–å¾—: {len(nikkei225_codes)}éŠ˜æŸ„")
        return nikkei225_codes
    
    def get_daily_quotes_10years(
        self, 
        code: str,
        from_date: str = "2015-09-01",
        to_date: str = "2025-08-31"
    ) -> pd.DataFrame:
        """
        10å¹´é–“ã®æ—¥æ¬¡æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
        """
        try:
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            results: List[Dict] = []
            pagination_key: Optional[str] = None
            
            logger.info(f"éŠ˜æŸ„ {code}: 10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
            logger.info(f"  æœŸé–“: {from_date} ï½ {to_date}")
            
            while True:
                params = {
                    "code": code,
                    "from": from_date,
                    "to": to_date
                }
                if pagination_key:
                    params["pagination_key"] = pagination_key
                
                resp = requests.get(
                    f"{JQUANTS_BASE_URL}/prices/daily_quotes",
                    headers=headers,
                    params=params,
                    timeout=120
                )
                
                if resp.status_code == 429:
                    logger.warning(f"éŠ˜æŸ„ {code}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€30ç§’å¾…æ©Ÿ...")
                    time.sleep(30)
                    continue
                
                if resp.status_code == 400:
                    logger.warning(f"éŠ˜æŸ„ {code}: ç„¡åŠ¹ãªã‚³ãƒ¼ãƒ‰ï¼ˆ400ã‚¨ãƒ©ãƒ¼ï¼‰")
                    return pd.DataFrame()
                
                resp.raise_for_status()
                data = resp.json()
                
                items = data.get("daily_quotes", [])
                if items:
                    results.extend(items)
                    logger.info(f"  å–å¾—ä»¶æ•°: {len(items)} (ç´¯è¨ˆ: {len(results)})")
                
                pagination_key = data.get("pagination_key")
                if not pagination_key:
                    break
                
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³é–“ã®å¾…æ©Ÿ
                time.sleep(0.3)
            
            if results:
                df = pd.DataFrame(results)
                logger.info(f"éŠ˜æŸ„ {code}: 10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({len(df):,}ä»¶)")
                return df
            else:
                logger.warning(f"éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ {code} å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return pd.DataFrame()
    
    def fetch_nikkei225_full_data(self) -> pd.DataFrame:
        """
        Nikkei225å…¨éŠ˜æŸ„ã®10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        """
        logger.info("=== Nikkei225å…¨éŠ˜æŸ„10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        logger.info("æœŸé–“: 2015å¹´9æœˆ1æ—¥ ï½ 2025å¹´8æœˆ31æ—¥ (10å¹´é–“)")
        
        # å…¨éŠ˜æŸ„å–å¾—
        stock_codes = self.get_nikkei225_codes()
        all_stock_data = []
        failed_stocks = []
        
        # ä¸­é–“ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        intermediate_dir = Path("data/nikkei225_full_data")
        intermediate_dir.mkdir(parents=True, exist_ok=True)
        
        for idx, code in enumerate(stock_codes, 1):
            try:
                logger.info(f"éŠ˜æŸ„ {code} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... ({idx}/{len(stock_codes)}) - {idx/len(stock_codes)*100:.1f}%å®Œäº†")
                
                stock_data = self.get_daily_quotes_10years(code)
                
                if not stock_data.empty:
                    all_stock_data.append(stock_data)
                    logger.info(f"  âœ… éŠ˜æŸ„ {code}: {len(stock_data):,}ä»¶å–å¾—æˆåŠŸ")
                    
                    # ä¸­é–“ä¿å­˜ï¼ˆ25éŠ˜æŸ„ã”ã¨ï¼‰
                    if idx % 25 == 0:
                        intermediate_df = pd.concat(all_stock_data, ignore_index=True)
                        intermediate_file = intermediate_dir / f"intermediate_nikkei225_{idx}stocks_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl"
                        intermediate_df.to_pickle(intermediate_file)
                        logger.info(f"  ğŸ’¾ ä¸­é–“ä¿å­˜: {intermediate_file} ({len(intermediate_df):,}ä»¶)")
                
                else:
                    failed_stocks.append(code)
                    logger.warning(f"  âŒ éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
                
                # éŠ˜æŸ„é–“ã®å¾…æ©Ÿï¼ˆé‡è¦ï¼‰
                wait_time = 3.0  # 3ç§’å¾…æ©Ÿ
                if idx % 10 == 0:
                    wait_time = 10.0  # 10éŠ˜æŸ„ã”ã¨ã«é•·ã„å¾…æ©Ÿ
                    logger.info(f"  â¸ï¸  APIåˆ¶é™å¯¾å¿œã§{wait_time:.1f}ç§’å¾…æ©Ÿ...")
                
                time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"  âŒ éŠ˜æŸ„ {code} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                failed_stocks.append(code)
                continue
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        if not all_stock_data:
            raise RuntimeError("å…¨ã¦ã®éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        logger.info("=== ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­ ===")
        combined_df = pd.concat(all_stock_data, ignore_index=True)
        
        logger.info("=== Nikkei225å…¨éŠ˜æŸ„10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ===")
        logger.info(f"æˆåŠŸéŠ˜æŸ„: {len(all_stock_data)}éŠ˜æŸ„")
        logger.info(f"å¤±æ•—éŠ˜æŸ„: {len(failed_stocks)}éŠ˜æŸ„")
        if failed_stocks:
            logger.info(f"å¤±æ•—éŠ˜æŸ„ãƒªã‚¹ãƒˆ: {failed_stocks}")
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(combined_df):,}ä»¶")
        logger.info(f"æœŸé–“: {combined_df['Date'].min()} ï½ {combined_df['Date'].max()}")
        
        # æœ€çµ‚ä¿å­˜
        output_dir = Path("data/nikkei225_full_data")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"nikkei225_full_10years_{len(all_stock_data)}stocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        combined_df.to_pickle(output_file)
        
        logger.info(f"ğŸ‰ Nikkei225å…¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {output_file}")
        
        return combined_df


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        fetcher = Nikkei225FullFetcher()
        
        # Nikkei225å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—
        all_data = fetcher.fetch_nikkei225_full_data()
        
        print(f"\n=== Nikkei225å…¨éŠ˜æŸ„10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ ===")
        print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(all_data):,}ä»¶")
        print(f"éŠ˜æŸ„æ•°: {all_data['Code'].nunique()}éŠ˜æŸ„")
        print(f"æœŸé–“: {all_data['Date'].min()} ï½ {all_data['Date'].max()}")
        
        # æœŸé–“è¨ˆç®—
        start_date = pd.to_datetime(all_data['Date'].min())
        end_date = pd.to_datetime(all_data['Date'].max())
        total_days = (end_date - start_date).days
        total_years = total_days / 365.25
        print(f"å®Ÿéš›ã®æœŸé–“: {total_years:.2f}å¹´ ({total_days}æ—¥)")
        
        print("ğŸ‰ Nikkei225å…¨éŠ˜æŸ„10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        
        return all_data
        
    except Exception as e:
        logger.error(f"Nikkei225å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {str(e)}")
        raise


if __name__ == "__main__":
    main()