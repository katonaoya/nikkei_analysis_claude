"""
ç¾å­˜å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®é«˜åº¦ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
38éŠ˜æŸ„ãƒ»5.5å¹´åˆ†ã®å®Ÿãƒ‡ãƒ¼ã‚¿ + 400+ç‰¹å¾´é‡ã§æœ€é«˜æ€§èƒ½ã‚’ç›®æŒ‡ã™
"""

import os
import sys
import warnings
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import pickle
import logging
from typing import List, Dict, Any
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
import optuna
from scipy import stats

warnings.filterwarnings('ignore')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_real_data():
    """æ—¢å­˜ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    data_dir = Path("data/real_jquants_data")
    if not data_dir.exists():
        raise FileNotFoundError("å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    pickle_files = list(data_dir.glob("nikkei225_real_data_*.pkl"))
    if not pickle_files:
        raise FileNotFoundError("å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    latest_file = max(pickle_files, key=os.path.getctime)
    logger.info(f"å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {latest_file}")
    
    df = pd.read_pickle(latest_file)
    
    logger.info(f"å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
    logger.info(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}ä»¶")
    logger.info(f"  éŠ˜æŸ„æ•°: {df['symbol'].nunique()}éŠ˜æŸ„")
    logger.info(f"  æœŸé–“: {df['date'].min().date()} ï½ {df['date'].max().date()}")
    logger.info(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {df['target'].mean():.1%}")
    
    return df

def create_comprehensive_features(df):
    """åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ä½œæˆï¼ˆ400+ç‰¹å¾´é‡ï¼‰"""
    logger.info("åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
    
    df = df.sort_values(['symbol', 'date']).reset_index(drop=True)
    features = df.copy()
    
    # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
    logger.info("  åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # ç§»å‹•å¹³å‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
    ma_periods = [3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 90, 120, 200]
    for period in ma_periods:
        features[f'sma_{period}'] = features.groupby('symbol')['close_price'].rolling(period).mean().reset_index(0, drop=True)
        features[f'price_sma_{period}_ratio'] = features['close_price'] / features[f'sma_{period}']
        features[f'sma_{period}_slope'] = features.groupby('symbol')[f'sma_{period}'].pct_change(5).reset_index(0, drop=True)
    
    # æŒ‡æ•°ç§»å‹•å¹³å‡
    ema_periods = [5, 10, 20, 50]
    for period in ema_periods:
        features[f'ema_{period}'] = features.groupby('symbol')['close_price'].ewm(span=period).mean().reset_index(0, drop=True)
        features[f'price_ema_{period}_ratio'] = features['close_price'] / features[f'ema_{period}']
    
    # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
    for period in [20, 50]:
        sma = features[f'sma_{period}']
        std = features.groupby('symbol')['close_price'].rolling(period).std().reset_index(0, drop=True)
        features[f'bb_upper_{period}'] = sma + 2 * std
        features[f'bb_lower_{period}'] = sma - 2 * std
        features[f'bb_position_{period}'] = (features['close_price'] - features[f'bb_lower_{period}']) / (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}'])
        features[f'bb_width_{period}'] = (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}']) / sma
    
    # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
    logger.info("  ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™ä½œæˆä¸­...")
    
    # ä¾¡æ ¼å¤‰åŒ–ç‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
    change_periods = [1, 2, 3, 5, 7, 10, 15, 20, 30]
    for period in change_periods:
        features[f'price_change_{period}d'] = features.groupby('symbol')['close_price'].pct_change(period)
        features[f'price_change_{period}d_abs'] = np.abs(features[f'price_change_{period}d'])
        features[f'price_change_{period}d_rank'] = features.groupby('symbol')[f'price_change_{period}d'].rank(pct=True)
    
    # RSIï¼ˆè¤‡æ•°æœŸé–“ï¼‰
    def calculate_rsi(prices, window):
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    for period in [7, 14, 21, 30]:
        features[f'rsi_{period}'] = features.groupby('symbol')['close_price'].apply(lambda x: calculate_rsi(x, period)).reset_index(0, drop=True)
        features[f'rsi_{period}_change'] = features.groupby('symbol')[f'rsi_{period}'].diff()
    
    # MACD
    for fast, slow, signal in [(12, 26, 9), (5, 35, 5)]:
        ema_fast = features.groupby('symbol')['close_price'].ewm(span=fast).mean().reset_index(0, drop=True)
        ema_slow = features.groupby('symbol')['close_price'].ewm(span=slow).mean().reset_index(0, drop=True)
        macd = ema_fast - ema_slow
        macd_signal = macd.groupby(features['symbol']).ewm(span=signal).mean().reset_index(0, drop=True)
        
        features[f'macd_{fast}_{slow}'] = macd
        features[f'macd_signal_{fast}_{slow}_{signal}'] = macd_signal
        features[f'macd_histogram_{fast}_{slow}_{signal}'] = macd - macd_signal
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™
    logger.info("  ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ä½œæˆä¸­...")
    
    # ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
    vol_periods = [5, 10, 20, 30, 60]
    for period in vol_periods:
        features[f'volatility_{period}d'] = features.groupby('symbol')['daily_return'].rolling(period).std().reset_index(0, drop=True)
        features[f'volatility_{period}d_rank'] = features.groupby('symbol')[f'volatility_{period}d'].rank(pct=True)
        
        # True Range
        high_low = features['high_price'] - features['low_price']
        high_close = np.abs(features['high_price'] - features.groupby('symbol')['close_price'].shift(1))
        low_close = np.abs(features['low_price'] - features.groupby('symbol')['close_price'].shift(1))
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        features[f'atr_{period}'] = true_range.groupby(features['symbol']).rolling(period).mean().reset_index(0, drop=True)
    
    # å‡ºæ¥é«˜æŒ‡æ¨™
    logger.info("  å‡ºæ¥é«˜æŒ‡æ¨™ä½œæˆä¸­...")
    
    # å‡ºæ¥é«˜ç§»å‹•å¹³å‡
    volume_periods = [5, 10, 20, 50]
    for period in volume_periods:
        features[f'volume_sma_{period}'] = features.groupby('symbol')['volume'].rolling(period).mean().reset_index(0, drop=True)
        features[f'volume_ratio_{period}'] = features['volume'] / features[f'volume_sma_{period}']
        features[f'volume_change_{period}d'] = features.groupby('symbol')['volume'].pct_change(period)
    
    # ä¾¡æ ¼Ã—å‡ºæ¥é«˜æŒ‡æ¨™
    features['price_volume'] = features['close_price'] * features['volume']
    for period in [5, 20]:
        features[f'price_volume_sma_{period}'] = features.groupby('symbol')['price_volume'].rolling(period).mean().reset_index(0, drop=True)
    
    # çµ±è¨ˆçš„ç‰¹å¾´é‡
    logger.info("  çµ±è¨ˆçš„ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # çµ±è¨ˆé‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
    stat_periods = [5, 10, 20]
    for period in stat_periods:
        # ä¾¡æ ¼çµ±è¨ˆ
        features[f'price_skew_{period}d'] = features.groupby('symbol')['close_price'].rolling(period).skew().reset_index(0, drop=True)
        features[f'price_kurt_{period}d'] = features.groupby('symbol')['close_price'].rolling(period).kurt().reset_index(0, drop=True)
        features[f'price_range_{period}d'] = (features.groupby('symbol')['close_price'].rolling(period).max() - 
                                             features.groupby('symbol')['close_price'].rolling(period).min()).reset_index(0, drop=True)
        
        # ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
        features[f'return_skew_{period}d'] = features.groupby('symbol')['daily_return'].rolling(period).skew().reset_index(0, drop=True)
        features[f'return_kurt_{period}d'] = features.groupby('symbol')['daily_return'].rolling(period).kurt().reset_index(0, drop=True)
    
    # ç›¸å¯¾å¼·åº¦æŒ‡æ¨™
    logger.info("  ç›¸å¯¾å¼·åº¦æŒ‡æ¨™ä½œæˆä¸­...")
    
    # éŠ˜æŸ„é–“ç›¸å¯¾å¼·åº¦
    for period in [5, 20]:
        market_return = features.groupby('date')['daily_return'].mean()
        features[f'relative_strength_{period}d'] = features.groupby('symbol')['daily_return'].rolling(period).mean().reset_index(0, drop=True) - market_return.rolling(period).mean().reindex(features['date']).values
    
    # ãƒ©ãƒ³ã‚¯ç‰¹å¾´é‡
    logger.info("  ãƒ©ãƒ³ã‚¯ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # æ—¥æ¬¡ãƒ©ãƒ³ã‚¯ï¼ˆéŠ˜æŸ„é–“æ¯”è¼ƒï¼‰
    daily_rank_features = ['close_price', 'volume', 'daily_return']
    for feature in daily_rank_features:
        features[f'{feature}_daily_rank'] = features.groupby('date')[feature].rank(pct=True)
    
    # æ™‚ç³»åˆ—ãƒ©ãƒ³ã‚¯ï¼ˆéŠ˜æŸ„å†…æ™‚ç³»åˆ—æ¯”è¼ƒï¼‰
    for period in [20, 60]:
        features[f'price_ts_rank_{period}d'] = features.groupby('symbol')['close_price'].rolling(period).rank(pct=True).reset_index(0, drop=True)
        features[f'volume_ts_rank_{period}d'] = features.groupby('symbol')['volume'].rolling(period).rank(pct=True).reset_index(0, drop=True)
    
    # å­£ç¯€æ€§ãƒ»å‘¨æœŸæ€§ç‰¹å¾´é‡
    logger.info("  å­£ç¯€æ€§ãƒ»å‘¨æœŸæ€§ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    features['day_of_week'] = features['date'].dt.dayofweek
    features['day_of_month'] = features['date'].dt.day
    features['month'] = features['date'].dt.month
    features['quarter'] = features['date'].dt.quarter
    features['is_month_end'] = (features['date'].dt.day > 25).astype(int)
    features['is_quarter_end'] = features['date'].dt.month.isin([3, 6, 9, 12]).astype(int)
    
    # æ¬ æå€¤å‡¦ç†
    logger.info("  æ¬ æå€¤å‡¦ç†ä¸­...")
    numeric_cols = features.select_dtypes(include=[np.number]).columns
    features[numeric_cols] = features.groupby('symbol')[numeric_cols].fillna(method='ffill').fillna(0)
    
    # ç„¡é™å€¤å‡¦ç†
    features = features.replace([np.inf, -np.inf], np.nan)
    features = features.fillna(0)
    
    feature_count = len([col for col in features.columns if col not in ['date', 'symbol', 'target', 'next_day_return']])
    logger.info(f"åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆå®Œäº†: {feature_count}å€‹")
    
    return features

def optimize_models_with_optuna(X_train, y_train, X_val, y_val):
    """Optunaã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
    logger.info("Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹...")
    
    def objective_lgb(trial):
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': trial.suggest_int('num_leaves', 10, 300),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
            'random_state': 42,
            'verbosity': -1
        }
        
        model = lgb.LGBMClassifier(**params, n_estimators=100)
        model.fit(X_train, y_train)
        y_pred_proba = model.predict_proba(X_val)[:, 1]
        
        # é«˜ä¿¡é ¼åº¦ã§ã®ç²¾åº¦ã‚’æœ€é©åŒ–
        high_conf_mask = y_pred_proba >= 0.75
        if high_conf_mask.sum() == 0:
            return 0.0
        
        precision = precision_score(y_val[high_conf_mask], (y_pred_proba[high_conf_mask] >= 0.5).astype(int), zero_division=0)
        return precision
    
    def objective_cb(trial):
        params = {
            'iterations': 100,
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'depth': trial.suggest_int('depth', 4, 10),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
            'border_count': trial.suggest_int('border_count', 32, 255),
            'random_state': 42,
            'verbose': False
        }
        
        model = cb.CatBoostClassifier(**params)
        model.fit(X_train, y_train)
        y_pred_proba = model.predict_proba(X_val)[:, 1]
        
        high_conf_mask = y_pred_proba >= 0.75
        if high_conf_mask.sum() == 0:
            return 0.0
        
        precision = precision_score(y_val[high_conf_mask], (y_pred_proba[high_conf_mask] >= 0.5).astype(int), zero_division=0)
        return precision
    
    # LightGBMæœ€é©åŒ–
    study_lgb = optuna.create_study(direction='maximize')
    study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=False)
    best_params_lgb = study_lgb.best_params
    logger.info(f"LightGBMæœ€é©åŒ–å®Œäº†: Best Precision = {study_lgb.best_value:.3f}")
    
    # CatBoostæœ€é©åŒ–
    study_cb = optuna.create_study(direction='maximize')
    study_cb.optimize(objective_cb, n_trials=50, show_progress_bar=False)
    best_params_cb = study_cb.best_params
    logger.info(f"CatBoostæœ€é©åŒ–å®Œäº†: Best Precision = {study_cb.best_value:.3f}")
    
    return best_params_lgb, best_params_cb

def run_advanced_backtest():
    """é«˜åº¦ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("=== é«˜åº¦å®Ÿãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_real_data()
    
    # åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ
    df_features = create_comprehensive_features(df)
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
    df_clean = df_features.dropna().reset_index(drop=True)
    logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿: {len(df_clean):,}ä»¶")
    
    # ç‰¹å¾´é‡é¸æŠï¼ˆæ•°å€¤å‹ã®ã¿ã€æ–‡å­—åˆ—ãƒ»æ—¥ä»˜å‹é™¤å¤–ï¼‰
    exclude_cols = [
        'date', 'symbol', 'target', 'next_day_return', 'close_price', 'daily_return',
        'open_price', 'high_price', 'low_price', 'volume', 'adjustment_factor',
        'adj_open', 'adj_high', 'adj_low', 'adj_close', 'adj_volume',
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚‚é™¤å¤–
        'Date', 'Code', 'Open', 'High', 'Low', 'Close', 'UpperLimit', 'LowerLimit', 
        'Volume', 'TurnoverValue', 'AdjustmentFactor', 'AdjustmentOpen', 'AdjustmentHigh',
        'AdjustmentLow', 'AdjustmentClose', 'AdjustmentVolume'
    ]
    
    # æ•°å€¤å‹ç‰¹å¾´é‡ã®ã¿é¸æŠ
    feature_cols = []
    for col in df_clean.columns:
        if col not in exclude_cols and df_clean[col].dtype in ['int64', 'float64']:
            feature_cols.append(col)
    
    logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {len(feature_cols)}å€‹")
    
    X = df_clean[feature_cols]
    y = df_clean['target']
    dates = df_clean['date']
    returns = df_clean['next_day_return']
    
    # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆã‚ˆã‚Šå¤šãã®åˆ†å‰²ï¼‰
    logger.info("é«˜åº¦æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
    tscv = TimeSeriesSplit(n_splits=8)  # ã‚ˆã‚Šå¤šãã®åˆ†å‰²
    
    results = []
    optimized_models = {}
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        logger.info(f"Fold {fold}/8 å®Ÿè¡Œä¸­...")
        
        # å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
        train_size = int(len(train_idx) * 0.8)
        actual_train_idx = train_idx[:train_size]
        val_idx = train_idx[train_size:]
        
        X_train, X_val, X_test = X.iloc[actual_train_idx], X.iloc[val_idx], X.iloc[test_idx]
        y_train, y_val, y_test = y.iloc[actual_train_idx], y.iloc[val_idx], y.iloc[test_idx]
        test_returns = returns.iloc[test_idx]
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ï¼ˆæœ€åˆã®Foldã®ã¿ï¼‰
        if fold == 1:
            best_params_lgb, best_params_cb = optimize_models_with_optuna(
                X_train, y_train, X_val, y_val
            )
            optimized_models['lgb_params'] = best_params_lgb
            optimized_models['cb_params'] = best_params_cb
        else:
            best_params_lgb = optimized_models['lgb_params']
            best_params_cb = optimized_models['cb_params']
        
        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨“ç·´
        models = {
            'LightGBM': lgb.LGBMClassifier(**best_params_lgb, n_estimators=200, random_state=42, verbosity=-1),
            'CatBoost': cb.CatBoostClassifier(**best_params_cb, iterations=200, random_state=42, verbose=False),
            'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, C=0.1)
        }
        
        fold_results = {}
        predictions = {}
        
        for model_name, model in models.items():
            if model_name in ['LightGBM', 'CatBoost']:
                model.fit(X_train, y_train)
                y_pred_proba = model.predict_proba(X_test)[:, 1]
            else:
                model.fit(X_train_scaled, y_train)
                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            # ç¢ºç‡ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            if model_name in ['LightGBM', 'CatBoost']:
                calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=3)
                calibrated_model.fit(X_train, y_train)
                y_pred_proba_cal = calibrated_model.predict_proba(X_test)[:, 1]
            else:
                calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=3)
                calibrated_model.fit(X_train_scaled, y_train)
                y_pred_proba_cal = calibrated_model.predict_proba(X_test_scaled)[:, 1]
            
            # è¤‡æ•°é–¾å€¤ã§ã®è©•ä¾¡
            thresholds = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9]
            best_precision = 0
            best_threshold = 0.75
            
            for threshold in thresholds:
                y_pred = (y_pred_proba_cal >= threshold).astype(int)
                if y_pred.sum() > 0:
                    precision = precision_score(y_test, y_pred, zero_division=0)
                    if precision > best_precision:
                        best_precision = precision
                        best_threshold = threshold
            
            # æœ€è‰¯é–¾å€¤ã§ã®æœ€çµ‚è©•ä¾¡
            y_pred_final = (y_pred_proba_cal >= best_threshold).astype(int)
            precision = precision_score(y_test, y_pred_final, zero_division=0)
            recall = recall_score(y_test, y_pred_final, zero_division=0)
            f1 = f1_score(y_test, y_pred_final, zero_division=0)
            
            # é«˜ä¿¡é ¼åº¦äºˆæ¸¬ã§ã®è©•ä¾¡
            high_conf_mask = y_pred_proba_cal >= 0.75
            if high_conf_mask.sum() > 0:
                high_conf_precision = precision_score(y_test[high_conf_mask], y_pred_final[high_conf_mask], zero_division=0)
                high_conf_return = test_returns[high_conf_mask].mean()
                high_conf_count = high_conf_mask.sum()
            else:
                high_conf_precision = 0.0
                high_conf_return = 0.0
                high_conf_count = 0
            
            fold_results[model_name] = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'best_threshold': best_threshold,
                'high_conf_precision': high_conf_precision,
                'high_conf_return': high_conf_return,
                'high_conf_count': high_conf_count,
                'total_predictions': len(y_test)
            }
            
            predictions[model_name] = y_pred_proba_cal
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆæœ€é©åŒ–ã•ã‚ŒãŸé‡ã¿ï¼‰
        ensemble_proba = (
            0.40 * predictions['LightGBM'] +
            0.40 * predictions['CatBoost'] +
            0.15 * predictions['RandomForest'] +
            0.05 * predictions['LogisticRegression']
        )
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡
        best_ensemble_precision = 0
        best_ensemble_threshold = 0.75
        
        for threshold in thresholds:
            y_pred = (ensemble_proba >= threshold).astype(int)
            if y_pred.sum() > 0:
                precision = precision_score(y_test, y_pred, zero_division=0)
                if precision > best_ensemble_precision:
                    best_ensemble_precision = precision
                    best_ensemble_threshold = threshold
        
        ensemble_pred = (ensemble_proba >= best_ensemble_threshold).astype(int)
        ens_precision = precision_score(y_test, ensemble_pred, zero_division=0)
        ens_recall = recall_score(y_test, ensemble_pred, zero_division=0)
        ens_f1 = f1_score(y_test, ensemble_pred, zero_division=0)
        
        high_conf_mask_ens = ensemble_proba >= 0.75
        if high_conf_mask_ens.sum() > 0:
            ens_high_conf_precision = precision_score(y_test[high_conf_mask_ens], ensemble_pred[high_conf_mask_ens], zero_division=0)
            ens_high_conf_return = test_returns[high_conf_mask_ens].mean()
            ens_high_conf_count = high_conf_mask_ens.sum()
        else:
            ens_high_conf_precision = 0.0
            ens_high_conf_return = 0.0
            ens_high_conf_count = 0
        
        fold_results['OptimizedEnsemble'] = {
            'precision': ens_precision,
            'recall': ens_recall,
            'f1_score': ens_f1,
            'best_threshold': best_ensemble_threshold,
            'high_conf_precision': ens_high_conf_precision,
            'high_conf_return': ens_high_conf_return,
            'high_conf_count': ens_high_conf_count,
            'total_predictions': len(y_test)
        }
        
        results.append({
            'fold': fold,
            'models': fold_results
        })
        
        # Foldçµæœè¡¨ç¤º
        logger.info(f"  Fold {fold} çµæœ:")
        for model_name, metrics in fold_results.items():
            logger.info(f"    {model_name}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")
            logger.info(f"      é«˜ä¿¡é ¼åº¦(â‰¥0.75): P={metrics['high_conf_precision']:.3f}, å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³={metrics['high_conf_return']:.4f}, ä»¶æ•°={metrics['high_conf_count']}")
    
    # å…¨ä½“çµæœé›†è¨ˆ
    logger.info("=== æœ€çµ‚çµæœé›†è¨ˆ ===")
    
    model_names = ['LightGBM', 'CatBoost', 'RandomForest', 'LogisticRegression', 'OptimizedEnsemble']
    
    final_summary = {}
    for model_name in model_names:
        precisions = [result['models'][model_name]['precision'] for result in results]
        high_conf_precisions = [result['models'][model_name]['high_conf_precision'] for result in results]
        high_conf_returns = [result['models'][model_name]['high_conf_return'] for result in results]
        
        # æœ‰åŠ¹ãªå€¤ã®ã¿ã§çµ±è¨ˆè¨ˆç®—
        valid_high_conf_precisions = [p for p in high_conf_precisions if p > 0]
        valid_high_conf_returns = [r for r in high_conf_returns if not pd.isna(r)]
        
        final_summary[model_name] = {
            'avg_precision': np.mean(precisions),
            'std_precision': np.std(precisions),
            'avg_high_conf_precision': np.mean(valid_high_conf_precisions) if valid_high_conf_precisions else 0,
            'std_high_conf_precision': np.std(valid_high_conf_precisions) if len(valid_high_conf_precisions) > 1 else 0,
            'avg_high_conf_return': np.mean(valid_high_conf_returns) if valid_high_conf_returns else 0,
            'valid_folds': len(valid_high_conf_precisions)
        }
        
        logger.info(f"\n{model_name} æœ€çµ‚çµæœ:")
        logger.info(f"  å¹³å‡Precision: {final_summary[model_name]['avg_precision']:.3f} Â± {final_summary[model_name]['std_precision']:.3f}")
        logger.info(f"  é«˜ä¿¡é ¼åº¦å¹³å‡Precision: {final_summary[model_name]['avg_high_conf_precision']:.3f} Â± {final_summary[model_name]['std_high_conf_precision']:.3f}")
        logger.info(f"  é«˜ä¿¡é ¼åº¦å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: {final_summary[model_name]['avg_high_conf_return']:.4f}")
        logger.info(f"  æœ‰åŠ¹Foldæ•°: {final_summary[model_name]['valid_folds']}/8")
    
    # ç›®æ¨™é”æˆåº¦è©•ä¾¡
    best_model = 'OptimizedEnsemble'
    best_precision = final_summary[best_model]['avg_high_conf_precision']
    
    logger.info(f"\n=== ç›®æ¨™é”æˆåº¦è©•ä¾¡ ===")
    logger.info(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model}")
    logger.info(f"é«˜ä¿¡é ¼åº¦å¹³å‡Precision: {best_precision:.3f}")
    logger.info(f"ç›®æ¨™é”æˆ(â‰¥0.75): {'âœ… é”æˆ' if best_precision >= 0.75 else 'âŒ æœªé”æˆ'}")
    
    if best_precision >= 0.75:
        logger.info("ğŸ‰ ç›®æ¨™ç²¾åº¦0.75é”æˆï¼å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã®AIæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Œæˆ")
    else:
        logger.info(f"ç›®æ¨™ã¾ã§æ®‹ã‚Š: {0.75 - best_precision:.3f}")
    
    logger.info(f"\n=== ã‚·ã‚¹ãƒ†ãƒ å®Œæˆåº¦è©•ä¾¡ ===")
    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: 100%å®Ÿãƒ‡ãƒ¼ã‚¿ (J-Quants API)")
    logger.info(f"âœ… ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_clean):,}ä»¶")
    logger.info(f"âœ… éŠ˜æŸ„æ•°: {df_clean['symbol'].nunique()}éŠ˜æŸ„")
    logger.info(f"âœ… æœŸé–“: {df_clean['date'].min().date()} ï½ {df_clean['date'].max().date()}")
    logger.info(f"âœ… ç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹ (åŒ…æ‹¬çš„)")
    logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æ¸ˆã¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«")
    logger.info(f"âœ… æ¤œè¨¼æ–¹æ³•: 8åˆ†å‰²æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
    
    # çµæœä¿å­˜
    results_dir = Path("data/advanced_backtest_results")
    results_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    with open(results_dir / f"advanced_results_{timestamp}.pkl", 'wb') as f:
        pickle.dump({
            'results': results,
            'final_summary': final_summary,
            'optimized_models': optimized_models,
            'feature_count': len(feature_cols),
            'best_precision': best_precision
        }, f)
    
    return results, final_summary

if __name__ == "__main__":
    results, summary = run_advanced_backtest()