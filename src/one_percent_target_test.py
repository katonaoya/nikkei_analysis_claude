"""
1%ä»¥ä¸Šä¸Šæ˜‡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ã®ç²¾åº¦æ¤œè¨¼
å°‘æ•°ãƒ»é«˜ç²¾åº¦ã®å®Ÿç”¨æ€§ã‚’è©•ä¾¡
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import precision_score, recall_score, confusion_matrix
import lightgbm as lgb

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_one_percent_target():
    """1%ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ã®å®Ÿç”¨æ€§ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== 1%ä¸Šæ˜‡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¤œè¨¼é–‹å§‹ ===")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_file = Path("data/nikkei225_full_data/nikkei225_full_10years_175stocks_20250831_020101.pkl")
    df = pd.read_pickle(data_file)
    
    # åŸºæœ¬å‰å‡¦ç†
    df = df.sort_values(['Code', 'Date']).reset_index(drop=True)
    df['close_price'] = pd.to_numeric(df['Close'], errors='coerce')
    df['daily_return'] = df.groupby('Code')['close_price'].pct_change(fill_method=None)
    df['next_day_return'] = df.groupby('Code')['close_price'].pct_change(fill_method=None).shift(-1)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”è¼ƒ
    target_1pct = (df['next_day_return'] >= 0.01).astype(int)
    target_2pct = (df['next_day_return'] >= 0.02).astype(int)
    
    print(f"=== ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”è¼ƒ ===")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(df):,}")
    print(f"1%ä»¥ä¸Šä¸Šæ˜‡: {target_1pct.mean():.1%} ({target_1pct.sum():,}ä»¶)")
    print(f"2%ä»¥ä¸Šä¸Šæ˜‡: {target_2pct.mean():.1%} ({target_2pct.sum():,}ä»¶)")
    print(f"ãƒ‡ãƒ¼ã‚¿å¢—åŠ ç‡: {target_1pct.sum() / target_2pct.sum():.1f}å€")
    
    # 1%ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ç‰¹å¾´é‡ä½œæˆ
    df['target'] = target_1pct
    
    # ã‚·ãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡
    windows = [5, 10, 20]
    for window in windows:
        sma = df.groupby('Code')['close_price'].transform(lambda x: x.rolling(window).mean())
        df[f'price_to_sma_{window}'] = df['close_price'] / sma
        df[f'volatility_{window}'] = df.groupby('Code')['daily_return'].transform(
            lambda x: x.rolling(window).std()
        )
    
    # RSI
    def calc_rsi(prices, window=14):
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
        rs = gain / (loss + 1e-8)
        return 100 - (100 / (1 + rs))
    
    df['rsi_14'] = df.groupby('Code')['close_price'].transform(calc_rsi)
    
    # ãƒ©ã‚°ç‰¹å¾´é‡
    for lag in range(1, 4):
        df[f'return_lag_{lag}'] = df.groupby('Code')['daily_return'].shift(lag)
    
    # ç‰¹å¾´é‡æº–å‚™
    feature_cols = [col for col in df.columns if col.startswith(('price_to_sma', 'volatility', 'rsi', 'return_lag'))]
    X = df[feature_cols].fillna(0)
    y = df['target']
    
    # NaNé™¤å»
    valid_mask = ~(y.isna() | X.isna().any(axis=1))
    X = X[valid_mask]
    y = y[valid_mask]
    
    print(f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(X):,}")
    
    # æ™‚ç³»åˆ—åˆ†å‰²è©•ä¾¡ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªè¨­å®šï¼‰
    tscv = TimeSeriesSplit(n_splits=5, gap=10)
    results = []
    daily_predictions = []
    
    scaler = RobustScaler()
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        logger.info(f"Fold {fold + 1}/5 å®Ÿè¡Œä¸­...")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆã‚ˆã‚Šä¿å®ˆçš„è¨­å®šï¼‰
        model = lgb.LGBMClassifier(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            min_child_samples=50,  # ã‚ˆã‚Šä¿å®ˆçš„
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbosity=-1
        )
        
        model.fit(X_train_scaled, y_train)
        proba = model.predict_proba(X_val_scaled)[:, 1]
        
        # é«˜ç²¾åº¦é–¾å€¤ã§ã®è©•ä¾¡
        high_precision_thresholds = [0.75, 0.80, 0.85, 0.90, 0.95]
        
        for threshold in high_precision_thresholds:
            predictions = (proba >= threshold).astype(int)
            
            if predictions.sum() > 0:
                precision = precision_score(y_val, predictions)
                recall = recall_score(y_val, predictions)
                tn, fp, fn, tp = confusion_matrix(y_val, predictions).ravel()
                
                # æ—¥æ¬¡äºˆæ¸¬æ•°æ¨å®š
                val_days = len(y_val) // 175  # ç´„175éŠ˜æŸ„
                daily_pred_rate = predictions.sum() / val_days if val_days > 0 else 0
                
                results.append({
                    'fold': fold + 1,
                    'threshold': threshold,
                    'precision': precision,
                    'recall': recall,
                    'predictions': predictions.sum(),
                    'daily_predictions': daily_pred_rate,
                    'true_positives': tp,
                    'false_positives': fp,
                    'validation_days': val_days
                })
                
                print(f"  é–¾å€¤{threshold:.2f}: ç²¾åº¦={precision:.3f}, "
                      f"äºˆæ¸¬æ•°={predictions.sum()}, æ—¥æ¬¡={daily_pred_rate:.1f}ä»¶")
            else:
                print(f"  é–¾å€¤{threshold:.2f}: äºˆæ¸¬ãªã—")
    
    # çµæœåˆ†æ
    print(f"\n=== 1%ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè©³ç´°çµæœ ===")
    
    if results:
        df_results = pd.DataFrame(results)
        
        # é«˜ç²¾åº¦ï¼ˆ90%ä»¥ä¸Šï¼‰ã®çµæœã«çµã‚‹
        high_precision_results = df_results[df_results['precision'] >= 0.90]
        
        if len(high_precision_results) > 0:
            print(f"\n90%ä»¥ä¸Šç²¾åº¦ã®çµæœ:")
            print(f"å¹³å‡ç²¾åº¦: {high_precision_results['precision'].mean():.3f}")
            print(f"å¹³å‡æ—¥æ¬¡äºˆæ¸¬æ•°: {high_precision_results['daily_predictions'].mean():.1f}ä»¶")
            print(f"äºˆæ¸¬æ•°ç¯„å›²: {high_precision_results['daily_predictions'].min():.1f} ï½ "
                  f"{high_precision_results['daily_predictions'].max():.1f}ä»¶/æ—¥")
            print(f"å¹³å‡å†ç¾ç‡: {high_precision_results['recall'].mean():.3f}")
            
            print(f"\nè©³ç´°:")
            for _, row in high_precision_results.iterrows():
                print(f"  Fold {row['fold']}, é–¾å€¤{row['threshold']:.2f}: "
                      f"ç²¾åº¦{row['precision']:.3f}, æ—¥æ¬¡{row['daily_predictions']:.1f}ä»¶, "
                      f"TP={row['true_positives']}, FP={row['false_positives']}")
        
        # 85%ä»¥ä¸Šç²¾åº¦ã§ã®å®Ÿç”¨æ€§è©•ä¾¡
        practical_results = df_results[df_results['precision'] >= 0.85]
        
        if len(practical_results) > 0:
            print(f"\n=== å®Ÿç”¨æ€§è©•ä¾¡ï¼ˆ85%ä»¥ä¸Šç²¾åº¦ï¼‰ ===")
            avg_daily = practical_results['daily_predictions'].mean()
            avg_precision = practical_results['precision'].mean()
            
            print(f"å¹³å‡ç²¾åº¦: {avg_precision:.1%}")
            print(f"å¹³å‡æ—¥æ¬¡äºˆæ¸¬æ•°: {avg_daily:.1f}ä»¶")
            
            if 2 <= avg_daily <= 5:
                print("âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼å¸Œæœ›ï¼ˆ2-5ä»¶/æ—¥ï¼‰ã«åˆè‡´")
            elif avg_daily < 2:
                print("âš ï¸  äºˆæ¸¬æ•°ãŒå°‘ãªã™ãã‚‹å¯èƒ½æ€§")
            else:
                print("âš ï¸  äºˆæ¸¬æ•°ãŒå¤šã™ãã‚‹å¯èƒ½æ€§")
            
            # å®‰å®šæ€§è©•ä¾¡
            precision_std = practical_results['precision'].std()
            daily_std = practical_results['daily_predictions'].std()
            
            print(f"ç²¾åº¦å®‰å®šæ€§: Â±{precision_std:.3f}")
            print(f"äºˆæ¸¬æ•°å®‰å®šæ€§: Â±{daily_std:.1f}ä»¶")
            
            if precision_std < 0.05 and daily_std < 2:
                print("âœ… å®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
            else:
                print("âš ï¸  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã°ã‚‰ã¤ãã‚ã‚Š")
    
    return results


def analyze_target_stability():
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ›´ã®å®‰å®šæ€§åˆ†æ"""
    print(f"\n=== ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ›´ã«ã‚ˆã‚‹æ”¹å–„åŠ¹æœ ===")
    
    print("1%ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ©ç‚¹:")
    print("âœ… ãƒ‡ãƒ¼ã‚¿é‡ç´„2.2å€å¢—åŠ ï¼ˆ23% vs 10.5%ï¼‰")
    print("âœ… ã‚ˆã‚Šå¤šãã®å­¦ç¿’æ©Ÿä¼š")
    print("âœ… çµ±è¨ˆçš„å®‰å®šæ€§å‘ä¸Š")
    print("âœ… å½é™½æ€§(FP)ãƒªã‚¹ã‚¯åˆ†æ•£")
    
    print(f"\næœŸå¾…ã•ã‚Œã‚‹æ”¹å–„:")
    print("ğŸ¯ ç²¾åº¦85-95%ã§æ—¥æ¬¡2-4ä»¶ã®å®Ÿç”¨çš„äºˆæ¸¬")
    print("ğŸ“ˆ ã‚ˆã‚Šå®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    print("ğŸ’° å®ŸæŠ•è³‡æˆ¦ç•¥ã¨ã—ã¦æˆç«‹ã™ã‚‹è¦æ¨¡")
    
    print(f"\né‹ç”¨æ™‚ã®ç¾å®Ÿæ€§:")
    print("â€¢ æ¯æ—¥2-4éŠ˜æŸ„ã®æŠ•è³‡å€™è£œ")
    print("â€¢ 85-90%ã®æˆåŠŸç¢ºç‡")
    print("â€¢ æœˆé–“40-80å›ã®æŠ•è³‡æ©Ÿä¼š")
    print("â€¢ ãƒªã‚¹ã‚¯åˆ†æ•£ã•ã‚ŒãŸå–å¼•")


if __name__ == "__main__":
    results = test_one_percent_target()
    analyze_target_stability()