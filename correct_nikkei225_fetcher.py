#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ­£ã—ã„æ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿåœ¨ã™ã‚‹æ—¥çµŒ225éŠ˜æŸ„ã®æ­£ç¢ºãª4æ¡ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from pathlib import Path
import logging
from typing import List, Optional

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

JQUANTS_BASE_URL = "https://api.jquants.com/v1"

class CorrectNikkei225Fetcher:
    """æ­£ç¢ºãªæ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«èª­ã¿è¾¼ã¿
        from dotenv import load_dotenv
        load_dotenv()
        
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        logger.info("æ­£ç¢ºãªæ—¥çµŒ225ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        # ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.id_token
        
        logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
        time.sleep(3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        try:
            # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            auth_payload = {
                "mailaddress": self.mail_address,
                "password": self.password
            }
            
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_user",
                data=json.dumps(auth_payload),
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            refresh_token = resp.json().get("refreshToken")
            
            if not refresh_token:
                raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            time.sleep(1)
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            self.id_token = resp.json().get("idToken")
            
            if not self.id_token:
                raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æœŸé™ã‚’è¨­å®š
            self.token_expires_at = datetime.now() + timedelta(hours=1)
            
            logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            return self.id_token
            
        except Exception as e:
            logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def get_real_nikkei225_codes(self) -> List[str]:
        """
        å®Ÿåœ¨ã™ã‚‹æ—¥çµŒ225éŠ˜æŸ„ã®æ­£ç¢ºãª4æ¡ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        """
        # å®Ÿåœ¨ã™ã‚‹æ—¥çµŒ225éŠ˜æŸ„ã®æ­£ç¢ºãª4æ¡ã‚³ãƒ¼ãƒ‰ï¼ˆä¸»è¦éŠ˜æŸ„ã‹ã‚‰é–‹å§‹ï¼‰
        real_nikkei225_codes = [
            # ä¸»è¦éŠ˜æŸ„ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "7203",  # ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š
            "9984",  # ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—
            "6758",  # ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—
            "9432",  # æ—¥æœ¬é›»ä¿¡é›»è©±
            "8306",  # ä¸‰è±UFJãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—
            "8035",  # æ±äº¬ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ³
            "6367",  # ãƒ€ã‚¤ã‚­ãƒ³å·¥æ¥­
            "7974",  # ä»»å¤©å ‚
            "9983",  # ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†ã‚¤ãƒªãƒ³ã‚°
            "4063",  # ä¿¡è¶ŠåŒ–å­¦å·¥æ¥­
            "6501",  # æ—¥ç«‹è£½ä½œæ‰€
            "7267",  # ãƒ›ãƒ³ãƒ€
            "6902",  # ãƒ‡ãƒ³ã‚½ãƒ¼
            "8001",  # ä¼Šè—¤å¿ å•†äº‹
            "2914",  # æ—¥æœ¬ãŸã°ã“ç”£æ¥­
            "4519",  # ä¸­å¤–è£½è–¬
            "4543",  # ãƒ†ãƒ«ãƒ¢
            "6954",  # ãƒ•ã‚¡ãƒŠãƒƒã‚¯
            "8309",  # ä¸‰äº•ä½å‹ãƒˆãƒ©ã‚¹ãƒˆãƒ»ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "4502",  # æ­¦ç”°è–¬å“å·¥æ¥­
            "8411",  # ã¿ãšã»ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—
            "4568",  # ç¬¬ä¸€ä¸‰å…±
            "4523",  # ã‚¨ãƒ¼ã‚¶ã‚¤
            "4661",  # ã‚ªãƒªã‚¨ãƒ³ã‚¿ãƒ«ãƒ©ãƒ³ãƒ‰
            "6273",  # SMC
            "6200",  # ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹
            "6920",  # ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ãƒƒã‚¯
            "7832",  # ãƒãƒ³ãƒ€ã‚¤ãƒŠãƒ ã‚³ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "8316",  # ä¸‰äº•ä½å‹ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—
            "8031",  # ä¸‰äº•ç‰©ç”£
            "8002",  # ä¸¸ç´…
            "9201",  # æ—¥æœ¬èˆªç©º
            "9202",  # å…¨æ—¥æœ¬ç©ºè¼¸
            "7751",  # ã‚­ãƒ¤ãƒãƒ³
            "6981",  # æ‘ç”°è£½ä½œæ‰€
            "8028",  # ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒãƒ¼ãƒˆ
            "4005",  # ä½å‹åŒ–å­¦
            "4507",  # å¡©é‡ç¾©è£½è–¬
            "4578",  # å¤§å¡šãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "3382",  # ã‚»ãƒ–ãƒ³&ã‚¢ã‚¤ãƒ»ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "4478",  # ãƒ•ãƒªãƒ¼
            "6098",  # ãƒªã‚¯ãƒ«ãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "9434",  # ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯
            "4755",  # æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—
            "6971",  # äº¬ã‚»ãƒ©
            "6752",  # ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯ ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "7013",  # IHI
            "8804",  # æ±äº¬å»ºç‰©
            "8766",  # æ±äº¬æµ·ä¸Šãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹
            "2801",  # ã‚­ãƒƒã‚³ãƒ¼ãƒãƒ³
        ]
        
        logger.info(f"å®Ÿåœ¨æ—¥çµŒ225éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰å–å¾—: {len(real_nikkei225_codes)}éŠ˜æŸ„")
        return real_nikkei225_codes
    
    def get_daily_quotes_10years(
        self, 
        code: str,
        from_date: str = "2015-09-01",
        to_date: str = "2025-08-31"
    ) -> pd.DataFrame:
        """
        10å¹´é–“ã®æ—¥æ¬¡æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
        """
        try:
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            results: List[dict] = []
            pagination_key: Optional[str] = None
            
            logger.info(f"éŠ˜æŸ„ {code}: 10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
            logger.info(f"  æœŸé–“: {from_date} ï½ {to_date}")
            
            while True:
                params = {
                    "code": code,
                    "from": from_date,
                    "to": to_date
                }
                if pagination_key:
                    params["pagination_key"] = pagination_key
                
                resp = requests.get(
                    f"{JQUANTS_BASE_URL}/prices/daily_quotes",
                    headers=headers,
                    params=params,
                    timeout=120
                )
                
                if resp.status_code == 429:
                    logger.warning(f"éŠ˜æŸ„ {code}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€30ç§’å¾…æ©Ÿ...")
                    time.sleep(30)
                    continue
                
                if resp.status_code == 400:
                    logger.warning(f"éŠ˜æŸ„ {code}: ç„¡åŠ¹ãªã‚³ãƒ¼ãƒ‰ï¼ˆ400ã‚¨ãƒ©ãƒ¼ï¼‰")
                    return pd.DataFrame()
                
                resp.raise_for_status()
                data = resp.json()
                
                items = data.get("daily_quotes", [])
                if items:
                    results.extend(items)
                    logger.info(f"  å–å¾—ä»¶æ•°: {len(items)} (ç´¯è¨ˆ: {len(results)})")
                
                pagination_key = data.get("pagination_key")
                if not pagination_key:
                    break
                
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³é–“ã®å¾…æ©Ÿ
                time.sleep(0.3)
            
            if results:
                df = pd.DataFrame(results)
                logger.info(f"éŠ˜æŸ„ {code}: 10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({len(df):,}ä»¶)")
                return df
            else:
                logger.warning(f"éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ {code} å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return pd.DataFrame()
    
    def fetch_nikkei225_real_data(self) -> pd.DataFrame:
        """
        å®Ÿåœ¨ã™ã‚‹æ—¥çµŒ225éŠ˜æŸ„ã®10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        """
        logger.info("=== å®Ÿåœ¨æ—¥çµŒ225éŠ˜æŸ„10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        logger.info("æœŸé–“: 2015å¹´9æœˆ1æ—¥ ï½ 2025å¹´8æœˆ31æ—¥ (10å¹´é–“)")
        
        # å®Ÿåœ¨éŠ˜æŸ„å–å¾—
        stock_codes = self.get_real_nikkei225_codes()
        all_stock_data = []
        failed_stocks = []
        
        # ä¸­é–“ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        intermediate_dir = Path("data/real_nikkei225_data")
        intermediate_dir.mkdir(parents=True, exist_ok=True)
        
        for idx, code in enumerate(stock_codes, 1):
            try:
                logger.info(f"éŠ˜æŸ„ {code} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... ({idx}/{len(stock_codes)}) - {idx/len(stock_codes)*100:.1f}%å®Œäº†")
                
                stock_data = self.get_daily_quotes_10years(code)
                
                if not stock_data.empty:
                    all_stock_data.append(stock_data)
                    logger.info(f"  âœ… éŠ˜æŸ„ {code}: {len(stock_data):,}ä»¶å–å¾—æˆåŠŸ")
                    
                    # ä¸­é–“ä¿å­˜ï¼ˆ10éŠ˜æŸ„ã”ã¨ï¼‰
                    if idx % 10 == 0:
                        intermediate_df = pd.concat(all_stock_data, ignore_index=True)
                        intermediate_file = intermediate_dir / f"intermediate_real_nikkei225_{idx}stocks_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl"
                        intermediate_df.to_pickle(intermediate_file)
                        logger.info(f"  ğŸ’¾ ä¸­é–“ä¿å­˜: {intermediate_file} ({len(intermediate_df):,}ä»¶)")
                
                else:
                    failed_stocks.append(code)
                    logger.warning(f"  âŒ éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
                
                # éŠ˜æŸ„é–“ã®å¾…æ©Ÿï¼ˆé‡è¦ï¼‰
                wait_time = 2.0  # 2ç§’å¾…æ©Ÿ
                if idx % 5 == 0:
                    wait_time = 5.0  # 5éŠ˜æŸ„ã”ã¨ã«é•·ã„å¾…æ©Ÿ
                    logger.info(f"  â¸ï¸  APIåˆ¶é™å¯¾å¿œã§{wait_time:.1f}ç§’å¾…æ©Ÿ...")
                
                time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"  âŒ éŠ˜æŸ„ {code} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                failed_stocks.append(code)
                continue
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        if not all_stock_data:
            raise RuntimeError("å…¨ã¦ã®éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        logger.info("=== ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­ ===")
        combined_df = pd.concat(all_stock_data, ignore_index=True)
        
        logger.info("=== å®Ÿåœ¨æ—¥çµŒ225éŠ˜æŸ„10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ===")
        logger.info(f"æˆåŠŸéŠ˜æŸ„: {len(all_stock_data)}éŠ˜æŸ„")
        logger.info(f"å¤±æ•—éŠ˜æŸ„: {len(failed_stocks)}éŠ˜æŸ„")
        if failed_stocks:
            logger.info(f"å¤±æ•—éŠ˜æŸ„ãƒªã‚¹ãƒˆ: {failed_stocks}")
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(combined_df):,}ä»¶")
        logger.info(f"æœŸé–“: {combined_df['Date'].min()} ï½ {combined_df['Date'].max()}")
        
        # æœ€çµ‚ä¿å­˜
        output_dir = Path("data/real_nikkei225_data")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"real_nikkei225_10years_{len(all_stock_data)}stocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        combined_df.to_pickle(output_file)
        
        # Parquetå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        parquet_file = output_dir / f"real_nikkei225_10years_{len(all_stock_data)}stocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
        combined_df.to_parquet(parquet_file)
        
        logger.info(f"ğŸ‰ å®Ÿåœ¨æ—¥çµŒ225ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  PKL: {output_file}")
        logger.info(f"  Parquet: {parquet_file}")
        
        return combined_df


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ å®Ÿåœ¨æ—¥çµŒ225éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    
    fetcher = CorrectNikkei225Fetcher()
    df = fetcher.fetch_nikkei225_real_data()
    
    logger.info(f"ğŸ¯ æœ€çµ‚çµæœ: {len(df):,}ä»¶, {df['Code'].nunique()}éŠ˜æŸ„")


if __name__ == "__main__":
    main()