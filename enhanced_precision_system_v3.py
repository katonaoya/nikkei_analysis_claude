#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Precision System V3
å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ + å³å¯†ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹æ”¹å–„ç‰ˆ

æ”¹å–„ç‚¹ï¼š
1. å¤–éƒ¨çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆï¼ˆUSD/JPY, VIX, æ—¥çµŒ225æŒ‡æ•°ç­‰ï¼‰
2. ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–ã«ã‚ˆã‚‹å³å¯†ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
3. è¤‡é›‘æ€§ã‚’æŠ‘ãˆãŸã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
from datetime import datetime, timedelta
import logging
import warnings
import os
from pathlib import Path

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedPrecisionSystemV3:
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ + å³å¯†ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç‰ˆ"""
    
    def __init__(self, stock_file: str = None, external_file: str = None):
        """åˆæœŸåŒ–"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        if stock_file is None:
            stock_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
        if external_file is None:
            external_file = "data/external_extended/external_integrated_10years_20250909_231815.parquet"
            
        self.stock_file = stock_file
        self.external_file = external_file
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("models/enhanced_v3")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ¯ Enhanced Precision System V3 åˆæœŸåŒ–å®Œäº†")
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {self.stock_file}")
        logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {self.external_file}")
    
    def load_and_integrate_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨çµ±åˆ"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        stock_df = pd.read_parquet(self.stock_file)
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶, {stock_df['Code'].nunique()}éŠ˜æŸ„")
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        external_df = None
        if os.path.exists(self.external_file):
            try:
                external_df = pd.read_parquet(self.external_file)
                logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_df):,}ä»¶, {len(external_df.columns)}ã‚«ãƒ©ãƒ ")
            except Exception as e:
                logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                external_df = None
        else:
            logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.external_file}")
        
        # æ—¥ä»˜å‹çµ±ä¸€
        stock_df['Date'] = pd.to_datetime(stock_df['Date']).dt.tz_localize(None)
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ
        if external_df is not None:
            try:
                external_df['Date'] = pd.to_datetime(external_df['Date']).dt.tz_localize(None)
                
                # é‡è¦ãªå¤–éƒ¨æŒ‡æ¨™ã®ã¿é¸æŠï¼ˆè¤‡é›‘æ€§ã‚’æŠ‘åˆ¶ï¼‰
                important_external_cols = ['Date']
                for col in external_df.columns:
                    if any(key in col.lower() for key in ['usdjpy', 'vix', 'nikkei225_close', 'sp500_close']):
                        important_external_cols.append(col)
                
                if len(important_external_cols) > 1:
                    external_selected = external_df[important_external_cols].copy()
                    stock_df = pd.merge(stock_df, external_selected, on='Date', how='left')
                    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(important_external_cols)-1}æŒ‡æ¨™")
                else:
                    logger.warning("é‡è¦ãªå¤–éƒ¨æŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info(f"çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶, {len(stock_df.columns)}ã‚«ãƒ©ãƒ ")
        return stock_df
    
    def create_enhanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‹¡å¼µç‰¹å¾´é‡ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
        logger.info("ğŸ”¥ æ‹¡å¼µç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        
        enhanced_df = df.copy()
        
        # éŠ˜æŸ„åˆ¥ã«ç‰¹å¾´é‡ä½œæˆ
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy().sort_values('Date')
            
            # åŸºæœ¬ç‰¹å¾´é‡
            code_data['Returns'] = code_data['Close'].pct_change()
            code_data['Log_Returns'] = np.log(code_data['Close'] / code_data['Close'].shift(1))
            code_data['High_Low_Ratio'] = code_data['High'] / code_data['Low']
            
            # ç§»å‹•å¹³å‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            for window in [5, 10, 20, 50]:
                code_data[f'MA_{window}'] = code_data['Close'].rolling(window).mean()
                code_data[f'MA_{window}_ratio'] = code_data['Close'] / code_data[f'MA_{window}']
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            for window in [5, 20]:
                code_data[f'Volatility_{window}'] = code_data['Returns'].rolling(window).std()
            
            # RSI
            for window in [14, 30]:
                delta = code_data['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_data[f'RSI_{window}'] = 100 - (100 / (1 + rs))
            
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
            rolling_mean = code_data['Close'].rolling(20).mean()
            rolling_std = code_data['Close'].rolling(20).std()
            code_data['BB_upper'] = rolling_mean + (rolling_std * 2)
            code_data['BB_lower'] = rolling_mean - (rolling_std * 2)
            code_data['BB_ratio'] = (code_data['Close'] - code_data['BB_lower']) / (code_data['BB_upper'] - code_data['BB_lower'])
            
            # MACD
            exp1 = code_data['Close'].ewm(span=12).mean()
            exp2 = code_data['Close'].ewm(span=26).mean()
            code_data['MACD'] = exp1 - exp2
            code_data['MACD_signal'] = code_data['MACD'].ewm(span=9).mean()
            code_data['MACD_histogram'] = code_data['MACD'] - code_data['MACD_signal']
            
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡
            code_data['Volume_MA_20'] = code_data['Volume'].rolling(20).mean()
            code_data['Volume_ratio'] = code_data['Volume'] / code_data['Volume_MA_20']
            
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¨ã®ç›¸é–¢ç‰¹å¾´é‡ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
            for col in code_data.columns:
                if any(key in col.lower() for key in ['usdjpy', 'vix', 'nikkei225', 'sp500']):
                    if code_data[col].notna().sum() > 100:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                        # å¤–éƒ¨æŒ‡æ¨™ã¨ã®æ¯”ç‡
                        code_data[f'{col}_change'] = code_data[col].pct_change()
                        # 5æ—¥ç§»å‹•å¹³å‡
                        code_data[f'{col}_MA5'] = code_data[col].rolling(5).mean()
            
            enhanced_df.loc[mask] = code_data
        
        # ç›®çš„å¤‰æ•°ä½œæˆ
        logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
        enhanced_df['Target'] = 0
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # ç„¡é™å€¤ãƒ»æ¬ æå€¤å‡¦ç†
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
        
        logger.info(f"ğŸ”¥ ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶")
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(enhanced_df.columns)}ã‚«ãƒ©ãƒ ")
        logger.info(f"æ­£ä¾‹ç‡: {enhanced_df['Target'].mean():.3f}")
        
        return enhanced_df
    
    def walk_forward_optimization(self, df: pd.DataFrame, initial_train_size: int = 252*3) -> list:
        """ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–"""
        logger.info("ğŸ“ˆ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–é–‹å§‹...")
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df.sort_values(['Date', 'Code']).copy()
        unique_dates = sorted(df_sorted['Date'].unique())
        
        results = []
        step_size = 21  # æœˆæ¬¡ãƒªãƒãƒ©ãƒ³ã‚¹
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ é¸æŠ
        feature_cols = [col for col in df_sorted.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       df_sorted[col].dtype in ['int64', 'float64']]
        
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
        
        for i in range(initial_train_size, len(unique_dates) - step_size, step_size):
            try:
                # æœŸé–“è¨­å®š
                train_end_idx = i
                test_start_idx = i
                test_end_idx = min(i + step_size, len(unique_dates))
                
                train_dates = unique_dates[:train_end_idx]
                test_dates = unique_dates[test_start_idx:test_end_idx]
                
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
                train_df = df_sorted[df_sorted['Date'].isin(train_dates)]
                test_df = df_sorted[df_sorted['Date'].isin(test_dates)]
                
                if len(train_df) == 0 or len(test_df) == 0:
                    continue
                
                # ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°åˆ†é›¢
                X_train = train_df[feature_cols]
                y_train = train_df['Target']
                X_test = test_df[feature_cols]
                y_test = test_df['Target']
                
                # æ¬ æå€¤å‡¦ç†
                X_train = X_train.fillna(method='ffill').fillna(0)
                X_test = X_test.fillna(method='ffill').fillna(0)
                
                # ç‰¹å¾´é‡é¸æŠ
                selector = SelectKBest(score_func=f_classif, k=min(50, len(feature_cols)))
                X_train_selected = selector.fit_transform(X_train, y_train)
                X_test_selected = selector.transform(X_test)
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train_selected)
                X_test_scaled = scaler.transform(X_test_selected)
                
                # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                model = lgb.LGBMClassifier(
                    objective='binary',
                    n_estimators=200,
                    max_depth=8,
                    learning_rate=0.05,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    random_state=42,
                    verbose=-1
                )
                
                model.fit(X_train_scaled, y_train)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                
                # è©•ä¾¡
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                
                result = {
                    'period': f"{train_dates[-1].strftime('%Y-%m-%d')} to {test_dates[-1].strftime('%Y-%m-%d')}",
                    'train_size': len(train_df),
                    'test_size': len(test_df),
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'positive_rate': y_test.mean()
                }
                
                results.append(result)
                logger.info(f"æœŸé–“ {result['period']}: ç²¾åº¦={accuracy:.4f}, é©åˆç‡={precision:.4f}")
                
            except Exception as e:
                logger.warning(f"æœŸé–“ {i} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return results
    
    def train_final_model(self, df: pd.DataFrame) -> dict:
        """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        logger.info("ğŸ¤– æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")
        
        # ç‰¹å¾´é‡æº–å‚™
        feature_cols = [col for col in df.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       df.dtype in ['int64', 'float64']]
        
        X = df[feature_cols].fillna(method='ffill').fillna(0)
        y = df['Target']
        
        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœ€å¾Œ20%ã‚’ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        df_sorted = df.sort_values('Date')
        split_idx = int(len(df_sorted) * 0.8)
        
        X_train = X[:split_idx]
        X_test = X[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        # ç‰¹å¾´é‡é¸æŠ
        selector = SelectKBest(score_func=f_classif, k=min(50, len(feature_cols)))
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = lgb.LGBMClassifier(
            objective='binary',
            n_estimators=300,
            max_depth=8,
            learning_rate=0.03,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbose=-1
        )
        
        model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        logger.info(f"ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        logger.info(f"  ç²¾åº¦: {accuracy:.4f}")
        logger.info(f"  é©åˆç‡: {precision:.4f}")
        logger.info(f"  å†ç¾ç‡: {recall:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_data = {
            'model': model,
            'scaler': scaler,
            'selector': selector,
            'feature_cols': feature_cols,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'train_size': len(X_train),
            'test_size': len(X_test)
        }
        
        model_file = self.output_dir / f"enhanced_model_v3_{accuracy:.4f}acc_{timestamp}.joblib"
        joblib.dump(model_data, model_file)
        logger.info(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_data
    
    def run_enhanced_system(self):
        """æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        logger.info("ğŸš€ Enhanced Precision System V3 å®Ÿè¡Œé–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            df = self.load_and_integrate_data()
            
            # ç‰¹å¾´é‡ä½œæˆ
            enhanced_df = self.create_enhanced_features(df)
            
            # ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–
            wfo_results = self.walk_forward_optimization(enhanced_df)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            final_model = self.train_final_model(enhanced_df)
            
            # çµæœçµ±è¨ˆ
            if wfo_results:
                wfo_accuracies = [r['accuracy'] for r in wfo_results]
                wfo_mean_acc = np.mean(wfo_accuracies)
                wfo_std_acc = np.std(wfo_accuracies)
                
                logger.info(f"\nğŸ“Š ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–çµæœ:")
                logger.info(f"  æœŸé–“æ•°: {len(wfo_results)}")
                logger.info(f"  å¹³å‡ç²¾åº¦: {wfo_mean_acc:.4f} Â± {wfo_std_acc:.4f}")
                logger.info(f"  æœ€é«˜ç²¾åº¦: {max(wfo_accuracies):.4f}")
                logger.info(f"  æœ€ä½ç²¾åº¦: {min(wfo_accuracies):.4f}")
            
            # çµæœä¿å­˜
            results = {
                'final_model_accuracy': final_model['accuracy'],
                'wfo_mean_accuracy': wfo_mean_acc if wfo_results else 0,
                'wfo_std_accuracy': wfo_std_acc if wfo_results else 0,
                'wfo_results': wfo_results,
                'data_size': len(enhanced_df),
                'feature_count': len(final_model['feature_cols']),
                'external_data_integrated': os.path.exists(self.external_file)
            }
            
            results_file = self.output_dir / f"enhanced_results_v3_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
            joblib.dump(results, results_file)
            
            logger.info(f"ğŸ‰ Enhanced Precision System V3 å®Œäº†!")
            logger.info(f"æœ€çµ‚ç²¾åº¦: {final_model['accuracy']:.4f}")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {'âœ…' if results['external_data_integrated'] else 'âŒ'}")
            logger.info(f"çµæœä¿å­˜: {results_file}")
            
            return results
            
        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    system = EnhancedPrecisionSystemV3()
    results = system.run_enhanced_system()
    
    if results:
        print(f"\nâœ… Enhanced Precision System V3 å®Ÿè¡Œå®Œäº†!")
        print(f"ğŸ“Š æœ€çµ‚ç²¾åº¦: {results['final_model_accuracy']:.4f}")
        if results['wfo_mean_accuracy'] > 0:
            print(f"ğŸ“ˆ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å¹³å‡ç²¾åº¦: {results['wfo_mean_accuracy']:.4f}")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {'æˆåŠŸ' if results['external_data_integrated'] else 'å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãªã—'}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿é‡: {results['data_size']:,}ä»¶")
        print(f"ğŸ”§ ç‰¹å¾´é‡æ•°: {results['feature_count']}å€‹")
    else:
        print("\nâŒ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()