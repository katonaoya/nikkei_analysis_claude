#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹¡å¼µJ-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®J-Quants APIã‹ã‚‰æ—¥çµŒ225éŠ˜æŸ„ã®10å¹´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from pathlib import Path
import logging
from typing import List, Optional
from dotenv import load_dotenv

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

JQUANTS_BASE_URL = "https://api.jquants.com/v1"

class ExpandedJQuantsFetcher:
    """æ‹¡å¼µJ-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        logger.info("æ‹¡å¼µJ-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.id_token
        
        logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
        time.sleep(3)
        
        try:
            # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            auth_payload = {
                "mailaddress": self.mail_address,
                "password": self.password
            }
            
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_user",
                data=json.dumps(auth_payload),
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            refresh_token = resp.json().get("refreshToken")
            
            if not refresh_token:
                raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            time.sleep(1)
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            self.id_token = resp.json().get("idToken")
            
            if not self.id_token:
                raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            self.token_expires_at = datetime.now() + timedelta(hours=1)
            
            logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            return self.id_token
            
        except Exception as e:
            logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def get_listed_companies(self) -> pd.DataFrame:
        """ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‚’å–å¾—ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª"""
        logger.info("ğŸ“‹ ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—ä¸­...")
        
        try:
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            
            resp = requests.get(
                f"{JQUANTS_BASE_URL}/listed/info",
                headers=headers,
                timeout=60
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€30ç§’å¾…æ©Ÿ...")
                time.sleep(30)
                return self.get_listed_companies()
            
            resp.raise_for_status()
            data = resp.json()
            
            companies_df = pd.DataFrame(data['info'])
            logger.info(f"âœ… ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—å®Œäº†: {len(companies_df)}ç¤¾")
            
            # æ—¥çµŒ225ã«å«ã¾ã‚Œãã†ãªå¤§æ‰‹ä¼æ¥­ã‚’ãƒ•ã‚£ãƒ«ã‚¿
            major_companies = companies_df[
                (companies_df['MarketCode'] == '111') |  # æ±è¨¼ãƒ—ãƒ©ã‚¤ãƒ 
                (companies_df['CompanyName'].str.contains('ãƒˆãƒ¨ã‚¿|ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯|ã‚½ãƒ‹ãƒ¼|æ—¥æœ¬é›»ä¿¡é›»è©±|ä¸‰è±UFJ', na=False))
            ]
            
            logger.info(f"ğŸ“Š ä¸»è¦éŠ˜æŸ„å€™è£œ: {len(major_companies)}ç¤¾")
            return major_companies
            
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return pd.DataFrame()
    
    def get_daily_quotes_batch(self, codes: List[str], from_date: str, to_date: str) -> pd.DataFrame:
        """è¤‡æ•°éŠ˜æŸ„ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—"""
        logger.info(f"ğŸ“ˆ æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—: {len(codes)}éŠ˜æŸ„")
        logger.info(f"æœŸé–“: {from_date} ï½ {to_date}")
        
        all_data = []
        successful_codes = []
        
        for idx, code in enumerate(codes, 1):
            try:
                logger.info(f"éŠ˜æŸ„ {code} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... ({idx}/{len(codes)}) - {idx/len(codes)*100:.1f}%å®Œäº†")
                
                headers = {"Authorization": f"Bearer {self._get_id_token()}"}
                params = {
                    "code": code,
                    "from": from_date,
                    "to": to_date
                }
                
                resp = requests.get(
                    f"{JQUANTS_BASE_URL}/prices/daily_quotes",
                    headers=headers,
                    params=params,
                    timeout=120
                )
                
                if resp.status_code == 429:
                    logger.warning(f"éŠ˜æŸ„ {code}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€30ç§’å¾…æ©Ÿ...")
                    time.sleep(30)
                    continue
                
                if resp.status_code != 200:
                    logger.warning(f"éŠ˜æŸ„ {code}: ã‚¨ãƒ©ãƒ¼ {resp.status_code}")
                    continue
                
                data = resp.json()
                daily_quotes = data.get("daily_quotes", [])
                
                if daily_quotes:
                    stock_df = pd.DataFrame(daily_quotes)
                    all_data.append(stock_df)
                    successful_codes.append(code)
                    logger.info(f"  âœ… éŠ˜æŸ„ {code}: {len(daily_quotes)}ä»¶å–å¾—æˆåŠŸ")
                else:
                    logger.warning(f"  âŒ éŠ˜æŸ„ {code}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                time.sleep(1.5)
                
                # 10éŠ˜æŸ„ã”ã¨ã«é•·ã„å¾…æ©Ÿ
                if idx % 10 == 0:
                    logger.info(f"  â¸ï¸  10éŠ˜æŸ„å‡¦ç†å®Œäº†ã€5ç§’å¾…æ©Ÿ...")
                    time.sleep(5)\n                \n            except Exception as e:\n                logger.error(f\"éŠ˜æŸ„ {code} å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n                continue\n        \n        if not all_data:\n            logger.error(\"âŒ å…¨éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—\")\n            return pd.DataFrame()\n        \n        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ\n        logger.info(\"ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...\")\n        combined_df = pd.concat(all_data, ignore_index=True)\n        \n        logger.info(f\"âœ… ä¸€æ‹¬å–å¾—å®Œäº†: {len(combined_df):,}ä»¶, {len(successful_codes)}éŠ˜æŸ„\")\n        logger.info(f\"æˆåŠŸéŠ˜æŸ„: {successful_codes}\")\n        \n        return combined_df\n    \n    def expand_existing_data(self) -> pd.DataFrame:\n        \"\"\"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µï¼ˆæœŸé–“ãƒ»éŠ˜æŸ„æ•°ã‚’å¢—åŠ ï¼‰\"\"\"\n        logger.info(\"ğŸš€ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µé–‹å§‹\")\n        \n        # 1. ä¸Šå ´éŠ˜æŸ„ä¸€è¦§ã‹ã‚‰æœ‰åŠ¹ãªéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—\n        companies_df = self.get_listed_companies()\n        \n        if companies_df.empty:\n            logger.error(\"âŒ éŠ˜æŸ„ä¸€è¦§ãŒå–å¾—ã§ãã¾ã›ã‚“\")\n            return pd.DataFrame()\n        \n        # 2. ä¸»è¦éŠ˜æŸ„ã‚’é¸æŠï¼ˆæœ€å¤§100éŠ˜æŸ„ï¼‰\n        selected_codes = companies_df['Code'].unique()[:100]  # ä¸Šä½100éŠ˜æŸ„\n        logger.info(f\"ğŸ“Š é¸æŠéŠ˜æŸ„æ•°: {len(selected_codes)}éŠ˜æŸ„\")\n        \n        # 3. æœŸé–“ã‚’æ‹¡å¼µï¼ˆéå»5å¹´é–“ï¼‰\n        to_date = datetime.now().strftime('%Y-%m-%d')\n        from_date = (datetime.now() - timedelta(days=365*5)).strftime('%Y-%m-%d')\n        \n        logger.info(f\"ğŸ“… æ‹¡å¼µæœŸé–“: {from_date} ï½ {to_date} (5å¹´é–“)\")\n        \n        # 4. ãƒ‡ãƒ¼ã‚¿å–å¾—\n        expanded_df = self.get_daily_quotes_batch(selected_codes, from_date, to_date)\n        \n        if expanded_df.empty:\n            logger.error(\"âŒ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—\")\n            return pd.DataFrame()\n        \n        # 5. ãƒ‡ãƒ¼ã‚¿ä¿å­˜\n        output_dir = Path(\"data/expanded_jquants_data\")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_file = output_dir / f\"expanded_jquants_5years_{len(selected_codes)}stocks_{timestamp}.parquet\"\n        \n        expanded_df.to_parquet(output_file, index=False)\n        logger.info(f\"ğŸ’¾ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_file}\")\n        \n        logger.info(\"ğŸ‰ ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Œäº†\")\n        logger.info(f\"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(expanded_df):,}ä»¶, {expanded_df['Code'].nunique()}éŠ˜æŸ„\")\n        logger.info(f\"ğŸ“… æœŸé–“: {expanded_df['Date'].min()} ï½ {expanded_df['Date'].max()}\")\n        \n        return expanded_df\n    \n    def create_enhanced_dataset(self) -> pd.DataFrame:\n        \"\"\"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ãŸå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\"\"\"\n        logger.info(\"ğŸ”§ å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹\")\n        \n        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n        existing_path = Path(\"data/processed/real_jquants_data.parquet\")\n        existing_df = pd.DataFrame()\n        \n        if existing_path.exists():\n            existing_df = pd.read_parquet(existing_path)\n            logger.info(f\"ğŸ“ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_df):,}ä»¶, {existing_df['Code'].nunique()}éŠ˜æŸ„\")\n        \n        # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—\n        expanded_df = self.expand_existing_data()\n        \n        if expanded_df.empty and existing_df.empty:\n            logger.error(\"âŒ åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n            return pd.DataFrame()\n        \n        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ\n        if not existing_df.empty and not expanded_df.empty:\n            # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯çµ±åˆ\n            combined_df = pd.concat([existing_df, expanded_df], ignore_index=True)\n            combined_df = combined_df.drop_duplicates(subset=['Date', 'Code']).sort_values(['Code', 'Date'])\n            logger.info(f\"ğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df):,}ä»¶, {combined_df['Code'].nunique()}éŠ˜æŸ„\")\n        elif not expanded_df.empty:\n            combined_df = expanded_df\n            logger.info(\"ğŸ“Š æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨\")\n        else:\n            combined_df = existing_df\n            logger.info(\"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨\")\n        \n        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜\n        output_dir = Path(\"data/enhanced_datasets\")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_file = output_dir / f\"enhanced_jquants_dataset_{timestamp}.parquet\"\n        \n        combined_df.to_parquet(output_file, index=False)\n        logger.info(f\"ğŸ’¾ å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {output_file}\")\n        \n        logger.info(\"ğŸ‰ å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†\")\n        return combined_df


def main():\n    \"\"\"ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°\"\"\"\n    logger.info(\"ğŸš€ æ‹¡å¼µJ-Quantsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹\")\n    \n    try:\n        fetcher = ExpandedJQuantsFetcher()\n        enhanced_df = fetcher.create_enhanced_dataset()\n        \n        if not enhanced_df.empty:\n            logger.info(\"=\"*60)\n            logger.info(\"ğŸ‰ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†\")\n            logger.info(\"=\"*60)\n            logger.info(f\"ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(enhanced_df):,}ä»¶\")\n            logger.info(f\"ğŸ“Š ç·éŠ˜æŸ„æ•°: {enhanced_df['Code'].nunique()}éŠ˜æŸ„\")\n            logger.info(f\"ğŸ“… æœŸé–“: {enhanced_df['Date'].min()} ï½ {enhanced_df['Date'].max()}\")\n            \n            # ç°¡å˜ãªçµ±è¨ˆ\n            print(\"\\nğŸ“ˆ éŠ˜æŸ„åˆ¥ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ï¼ˆä¸Šä½10éŠ˜æŸ„ï¼‰:\")\n            top_stocks = enhanced_df['Code'].value_counts().head(10)\n            for code, count in top_stocks.items():\n                print(f\"  {code}: {count:,}ä»¶\")\n                \n        else:\n            logger.error(\"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n            \n    except Exception as e:\n        logger.error(f\"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()