#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
J-Quants APIã‹ã‚‰æ—¥çµŒ225æ§‹æˆéŠ˜æŸ„ã®å®Œå…¨ãª10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from pathlib import Path
import logging
from typing import List, Optional, Dict
from dotenv import load_dotenv
import concurrent.futures
import threading
from queue import Queue

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

JQUANTS_BASE_URL = "https://api.jquants.com/v1"

class Nikkei225Full10YearFetcher:
    """æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_workers=5):
        """åˆæœŸåŒ–"""
        self.mail_address = os.getenv("JQUANTS_MAIL_ADDRESS")
        self.password = os.getenv("JQUANTS_PASSWORD")
        self.id_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        self.max_workers = max_workers
        self.rate_limit_lock = threading.Lock()
        self.last_request_time = 0
        self.min_interval = 0.5  # 500msé–“éš”
        
        if not self.mail_address or not self.password:
            raise ValueError("JQuantsã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ (.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„)")
        
        logger.info(f"æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº† (ä¸¦åˆ—åº¦: {max_workers})")
    
    def _get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        if self.id_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.id_token
        
        logger.info("JQuantsèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")
        time.sleep(3)
        
        try:
            # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            auth_payload = {
                "mailaddress": self.mail_address,
                "password": self.password
            }
            
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_user",
                data=json.dumps(auth_payload),
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            refresh_token = resp.json().get("refreshToken")
            
            if not refresh_token:
                raise RuntimeError("ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            time.sleep(1)
            resp = requests.post(
                f"{JQUANTS_BASE_URL}/token/auth_refresh?refreshtoken={refresh_token}",
                timeout=30
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š2åˆ†å¾…æ©Ÿ...")
                time.sleep(120)
                return self._get_id_token()
                
            resp.raise_for_status()
            self.id_token = resp.json().get("idToken")
            
            if not self.id_token:
                raise RuntimeError("IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            self.token_expires_at = datetime.now() + timedelta(hours=1)
            
            logger.info("èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å®Œäº†")
            return self.id_token
            
        except Exception as e:
            logger.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def _rate_limit_wait(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®å¾…æ©Ÿå‡¦ç†"""
        with self.rate_limit_lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                time.sleep(sleep_time)
            self.last_request_time = time.time()
    
    def get_nikkei225_companies(self) -> pd.DataFrame:
        """æ—¥çµŒ225æ§‹æˆéŠ˜æŸ„ä¸€è¦§ã‚’å–å¾—"""
        logger.info("ğŸ“‹ æ—¥çµŒ225æ§‹æˆéŠ˜æŸ„ä¸€è¦§å–å¾—é–‹å§‹...")
        
        try:
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            
            logger.info("J-Quantsä¸Šå ´éŠ˜æŸ„ä¸€è¦§APIå‘¼ã³å‡ºã—ä¸­...")
            resp = requests.get(
                f"{JQUANTS_BASE_URL}/listed/info",
                headers=headers,
                timeout=60
            )
            
            if resp.status_code == 429:
                logger.warning("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€30ç§’å¾…æ©Ÿ...")
                time.sleep(30)
                return self.get_nikkei225_companies()
            
            resp.raise_for_status()
            data = resp.json()
            
            companies_df = pd.DataFrame(data['info'])
            logger.info(f"âœ… ä¸Šå ´éŠ˜æŸ„ä¸€è¦§å–å¾—å®Œäº†: {len(companies_df)}ç¤¾")
            
            # æ—¥çµŒ225ç›¸å½“ã®é¸æŠï¼ˆãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´ã®å¤§å‹æ ªã‚’æœ€å¤§225ç¤¾é¸æŠï¼‰
            nikkei225_companies = companies_df[
                (companies_df['MarketCode'] == '0111') &  # ãƒ—ãƒ©ã‚¤ãƒ å¸‚å ´
                (companies_df['ScaleCategory'].isin(['TOPIX Large70', 'TOPIX Mid400']))
            ].copy()
            
            # æœ‰åä¼æ¥­ã‚’è¿½åŠ ã§ç¢ºä¿
            major_companies = companies_df[
                companies_df['CompanyName'].str.contains(
                    'ãƒˆãƒ¨ã‚¿|ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯|ã‚½ãƒ‹ãƒ¼|æ—¥æœ¬é›»ä¿¡é›»è©±|ä¸‰è±UFJ|æ—¥ç«‹|ãƒ›ãƒ³ãƒ€|ä»»å¤©å ‚|ã‚­ãƒ¤ãƒãƒ³|ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯|'
                    'NTT|KDDI|æ­¦ç”°è–¬å“|ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†ã‚¤ãƒªãƒ³ã‚°|ãƒ•ã‚¡ãƒŠãƒƒã‚¯|ä¿¡è¶ŠåŒ–å­¦|æ±äº¬ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ³|'
                    'ãƒ€ã‚¤ã‚­ãƒ³å·¥æ¥­|æ‘ç”°è£½ä½œæ‰€|æ—¥æœ¬é›»ç”£|ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹|ã‚¨ãƒ ã‚¹ãƒªãƒ¼|ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ|ã‚ªãƒªã‚¨ãƒ³ã‚¿ãƒ«ãƒ©ãƒ³ãƒ‰|'
                    'ã‚»ã‚³ãƒ |ãƒ†ãƒ«ãƒ¢|ã‚·ã‚¹ãƒ¡ãƒƒã‚¯ã‚¹|æ—¥æœ¬M&Aã‚»ãƒ³ã‚¿ãƒ¼|ãƒ¢ãƒã‚¿ãƒ­ã‚¦|ãƒšãƒ—ãƒãƒ‰ãƒªãƒ¼ãƒ ', 
                    na=False
                )
            ]
            
            # çµ±åˆã—ã¦225ç¤¾ã‚’é¸æŠ
            selected_companies = pd.concat([nikkei225_companies, major_companies]).drop_duplicates()
            
            # 225ç¤¾ã«åˆ¶é™ï¼ˆæ™‚ä¾¡ç·é¡ã®å¤§ãã„é †ãªã©ã§é¸æŠï¼‰
            if len(selected_companies) > 225:
                selected_companies = selected_companies.head(225)
            
            logger.info(f"ğŸ“ˆ æ—¥çµŒ225ç›¸å½“é¸æŠéŠ˜æŸ„æ•°: {len(selected_companies)}ç¤¾")
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            logger.info("ğŸ“Š é¸æŠéŠ˜æŸ„ã‚µãƒ³ãƒ—ãƒ«:")
            for i, (_, company) in enumerate(selected_companies.head(10).iterrows()):
                logger.info(f"  {company['Code']}: {company['CompanyName']}")
            
            return selected_companies
            
        except Exception as e:
            logger.error(f"éŠ˜æŸ„ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return pd.DataFrame()
    
    def get_single_stock_data(self, company_info: tuple) -> Optional[pd.DataFrame]:
        """å˜ä¸€éŠ˜æŸ„ã®10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        code, company_name = company_info
        
        try:
            self._rate_limit_wait()
            
            # 10å¹´é–“ã®æœŸé–“è¨­å®š
            to_date = datetime.now().strftime('%Y-%m-%d')
            from_date = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')
            
            headers = {"Authorization": f"Bearer {self._get_id_token()}"}
            params = {
                "code": code,
                "from": from_date,
                "to": to_date
            }
            
            resp = requests.get(
                f"{JQUANTS_BASE_URL}/prices/daily_quotes",
                headers=headers,
                params=params,
                timeout=120
            )
            
            if resp.status_code == 429:
                logger.warning(f"éŠ˜æŸ„ {code}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€60ç§’å¾…æ©Ÿ...")
                time.sleep(60)
                return self.get_single_stock_data((code, company_name))
            
            if resp.status_code != 200:
                logger.warning(f"âŒ éŠ˜æŸ„ {code} ({company_name}): ã‚¨ãƒ©ãƒ¼ {resp.status_code}")
                return None
            
            data = resp.json()
            daily_quotes = data.get("daily_quotes", [])
            
            if daily_quotes:
                stock_df = pd.DataFrame(daily_quotes)
                stock_df['CompanyName'] = company_name
                logger.info(f"âœ… éŠ˜æŸ„ {code} ({company_name}): {len(daily_quotes)}ä»¶å–å¾—æˆåŠŸ")
                return stock_df
            else:
                logger.warning(f"âŒ éŠ˜æŸ„ {code} ({company_name}): ãƒ‡ãƒ¼ã‚¿ãªã—")
                return None
                
        except Exception as e:
            logger.error(f"âŒ éŠ˜æŸ„ {code} ({company_name}): å–å¾—ã‚¨ãƒ©ãƒ¼ {str(e)}")
            return None
    
    def get_all_nikkei225_data_parallel(self, companies_df: pd.DataFrame) -> pd.DataFrame:
        """æ—¥çµŒ225å…¨éŠ˜æŸ„ã®10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—"""
        logger.info(f"ğŸš€ æ—¥çµŒ225å…¨éŠ˜æŸ„ä¸¦åˆ—å–å¾—é–‹å§‹: {len(companies_df)}éŠ˜æŸ„ Ã— 10å¹´é–“")
        
        # æœŸé–“è¨­å®šè¡¨ç¤º
        to_date = datetime.now().strftime('%Y-%m-%d')
        from_date = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')
        logger.info(f"ğŸ“… å–å¾—æœŸé–“: {from_date} ï½ {to_date}")
        
        # äºˆæƒ³ãƒ‡ãƒ¼ã‚¿é‡è¨ˆç®—
        expected_records = len(companies_df) * 10 * 245  # éŠ˜æŸ„æ•° Ã— å¹´æ•° Ã— å–¶æ¥­æ—¥æ•°
        logger.info(f"ğŸ“Š äºˆæƒ³ãƒ‡ãƒ¼ã‚¿é‡: ç´„{expected_records:,}ä»¶")
        
        all_stock_data = []
        successful_companies = []
        failed_companies = []
        
        # éŠ˜æŸ„æƒ…å ±ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        company_list = [(row['Code'], row['CompanyName']) for _, row in companies_df.iterrows()]
        
        # ä¸¦åˆ—å‡¦ç†ã§éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_company = {
                executor.submit(self.get_single_stock_data, company_info): company_info 
                for company_info in company_list
            }
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_company), 1):
                company_info = future_to_company[future]
                code, company_name = company_info
                
                try:
                    stock_df = future.result()
                    if stock_df is not None:
                        all_stock_data.append(stock_df)
                        successful_companies.append(f"{code}({company_name})")
                    else:
                        failed_companies.append(f"{code}({company_name})")
                    
                    # é€²è¡ŒçŠ¶æ³è¡¨ç¤º
                    if i % 10 == 0:
                        progress = i / len(company_list) * 100
                        logger.info(f"ğŸ“Š é€²è¡ŒçŠ¶æ³: {i}/{len(company_list)} ({progress:.1f}%) - æˆåŠŸ: {len(successful_companies)}, å¤±æ•—: {len(failed_companies)}")
                        
                except Exception as e:
                    logger.error(f"âŒ {code}({company_name}): ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼ {str(e)}")
                    failed_companies.append(f"{code}({company_name})")
        
        if not all_stock_data:
            logger.error("âŒ å…¨éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            return pd.DataFrame()
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        logger.info("ğŸ”„ å…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
        combined_df = pd.concat(all_stock_data, ignore_index=True)
        
        logger.info("="*60)
        logger.info("ğŸ“Š æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ")
        logger.info("="*60)
        logger.info(f"âœ… æˆåŠŸéŠ˜æŸ„æ•°: {len(successful_companies)}/{len(companies_df)}éŠ˜æŸ„")
        logger.info(f"âŒ å¤±æ•—éŠ˜æŸ„æ•°: {len(failed_companies)}éŠ˜æŸ„")
        logger.info(f"ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(combined_df):,}ä»¶")
        logger.info(f"ğŸ“… æœŸé–“: {combined_df['Date'].min()} ï½ {combined_df['Date'].max()}")
        logger.info(f"ğŸ“ˆ å¹³å‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°/éŠ˜æŸ„: {len(combined_df)/len(successful_companies):.0f}ä»¶") if successful_companies else None
        
        if successful_companies:
            logger.info("âœ… æˆåŠŸéŠ˜æŸ„ï¼ˆæœ€åˆã®20ç¤¾ï¼‰:")
            for company in successful_companies[:20]:
                logger.info(f"  {company}")
        
        if failed_companies:
            logger.info("âŒ å¤±æ•—éŠ˜æŸ„:")
            for company in failed_companies:
                logger.info(f"  {company}")
        
        return combined_df
    
    def create_nikkei225_full_dataset(self) -> pd.DataFrame:
        """æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸš€ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹")
        
        # 1. æ—¥çµŒ225æ§‹æˆéŠ˜æŸ„ä¸€è¦§å–å¾—
        companies_df = self.get_nikkei225_companies()
        
        if companies_df.empty:
            logger.error("âŒ éŠ˜æŸ„ä¸€è¦§å–å¾—ã«å¤±æ•—")
            return pd.DataFrame()
        
        # 2. å…¨éŠ˜æŸ„ã®10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—
        full_df = self.get_all_nikkei225_data_parallel(companies_df)
        
        if full_df.empty:
            logger.error("âŒ å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            return pd.DataFrame()
        
        # 3. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        output_dir = Path("data/nikkei225_full")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"nikkei225_full_{len(full_df)}records_{timestamp}.parquet"
        
        full_df.to_parquet(output_file, index=False)
        logger.info(f"ğŸ’¾ å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {output_file}")
        
        # 4. ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆè¡¨ç¤º
        logger.info("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
        logger.info(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(full_df):,}ä»¶")
        logger.info(f"  éŠ˜æŸ„æ•°: {full_df['Code'].nunique()}éŠ˜æŸ„")
        logger.info(f"  æœŸé–“: {full_df['Date'].min()} ï½ {full_df['Date'].max()}")
        
        # ä¸Šä½éŠ˜æŸ„ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        top_stocks = full_df['Code'].value_counts().head(10)
        logger.info("\nğŸ“ˆ ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ä¸Šä½10éŠ˜æŸ„:")
        for code, count in top_stocks.items():
            company_name = full_df[full_df['Code'] == code]['CompanyName'].iloc[0] if 'CompanyName' in full_df.columns else 'N/A'
            logger.info(f"  {code} ({company_name}): {count:,}ä»¶")
        
        logger.info("ğŸ‰ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†")
        return full_df


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    try:
        # ä¸¦åˆ—åº¦5ã§ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
        fetcher = Nikkei225Full10YearFetcher(max_workers=5)
        full_df = fetcher.create_nikkei225_full_dataset()
        
        if not full_df.empty:
            logger.info("="*60)
            logger.info("ğŸ‰ æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´é–“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
            logger.info("="*60)
            logger.info(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿é‡: {len(full_df):,}ä»¶")
            logger.info(f"ğŸ“Š å–å¾—éŠ˜æŸ„æ•°: {full_df['Code'].nunique()}éŠ˜æŸ„")
            logger.info(f"ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {full_df['Date'].min()} ï½ {full_df['Date'].max()}")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã¨ã®æ¯”è¼ƒ
            target_records = 550000
            achievement_rate = len(full_df) / target_records * 100
            logger.info(f"ğŸ¯ ç›®æ¨™é”æˆç‡: {achievement_rate:.1f}% ({len(full_df):,}/{target_records:,}ä»¶)")
            
            logger.info("ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: enhanced_precision_with_full_data.pyã§ã®ç²¾åº¦æ¤œè¨¼ã‚’å®Ÿè¡Œäºˆå®š")
                
        else:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise


if __name__ == "__main__":
    main()