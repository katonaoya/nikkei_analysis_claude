#!/usr/bin/env python3
"""
J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã§55.3%ç¢ºå®Ÿé”æˆ
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ç‰ˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class JQuantsPrecisionOptimizer:
    """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã§55.3%ç¢ºå®Ÿé”æˆ"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
    def load_and_prepare_data(self, sample_size=50000):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size:,}ï¼‰")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(df) > sample_size:
            df = df.sort_values('Date').tail(sample_size)
            logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(df):,}ä»¶")
        
        return df
    
    def create_jquants_like_features(self, df):
        """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®å®Œå…¨å¾©å…ƒ"""
        logger.info("ğŸ”§ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # 1. å¸‚å ´å…¨ä½“æŒ‡æ¨™
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std'],
            'Volume': ['mean', 'std'],
            'Returns': 'mean'
        }).round(6)
        
        daily_market.columns = [
            'Market_Close_Mean', 'Market_Close_Std', 
            'Market_Volume_Mean', 'Market_Volume_Std',
            'Market_Return_Mean'
        ]
        daily_market = daily_market.reset_index()
        
        # 2. ã‚»ã‚¯ã‚¿ãƒ¼æ¨¡æ“¬
        df['Sector_Code'] = df['Code'].astype(str).str[:2]
        sector_daily = df.groupby(['Date', 'Sector_Code'])['Close'].mean().reset_index()
        sector_daily.columns = ['Date', 'Sector_Code', 'Sector_Avg_Price']
        
        # 3. ä¿¡ç”¨å–å¼•æ¨¡æ“¬æŒ‡æ¨™
        df['Volume_MA5'] = df.groupby('Code')['Volume'].rolling(5).mean().reset_index(0, drop=True)
        df['Volume_Shock'] = df['Volume'] / (df['Volume_MA5'] + 1e-6)
        df['Price_Volatility_5d'] = df.groupby('Code')['Close'].rolling(5).std().reset_index(0, drop=True)
        df['Volatility_Rank'] = df.groupby('Date')['Price_Volatility_5d'].rank(pct=True)
        
        # 4. å¸‚å ´ç›¸å¯¾æŒ‡æ¨™
        df = df.merge(daily_market, on='Date', how='left')
        df = df.merge(sector_daily, on=['Date', 'Sector_Code'], how='left')
        
        df['Market_Relative_Return'] = df['Returns'] - df['Market_Return_Mean'] 
        df['Market_Relative_Price'] = df['Close'] / (df['Market_Close_Mean'] + 1e-6)
        df['Sector_Relative_Price'] = df['Close'] / (df['Sector_Avg_Price'] + 1e-6)
        df['Market_Volume_Relative'] = df['Volume'] / (df['Market_Volume_Mean'] + 1e-6)
        
        # 5. å¤–å›½äººæŠ•è³‡å®¶æ¨¡æ“¬
        df['Market_Cap_Proxy'] = df['Close'] * df['Volume']
        df['Large_Cap_Flag'] = (df.groupby('Date')['Market_Cap_Proxy'].rank(pct=True) > 0.8).astype(int)
        
        large_cap_return = df[df['Large_Cap_Flag'] == 1].groupby('Date')['Returns'].mean()
        large_cap_return = large_cap_return.reset_index()
        large_cap_return.columns = ['Date', 'Large_Cap_Return']
        
        df = df.merge(large_cap_return, on='Date', how='left')
        df['Foreign_Proxy'] = df['Returns'] - df['Large_Cap_Return']
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        logger.info(f"âœ… J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        return df
    
    def get_jquants_features(self, df):
        """J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ãƒªã‚¹ãƒˆå–å¾—"""
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector_Code'
        }
        
        all_features = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        # J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿
        jquants_features = [col for col in all_features if any(
            keyword in col for keyword in ['Market', 'Sector', 'Volume_Shock', 'Volatility', 'Foreign', 'Large_Cap']
        )]
        
        logger.info(f"J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡: {len(jquants_features)}å€‹")
        return jquants_features
    
    def hyperparameter_optimization(self, X, y):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        logger.info("ğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰
        param_grid = {
            'C': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0],
            'class_weight': ['balanced', {0: 1, 1: 1.1}, {0: 1, 1: 1.2}, {0: 1, 1: 1.3}],
            'solver': ['liblinear', 'lbfgs'],
            'max_iter': [500, 1000, 2000]
        }
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=3)
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
        grid_search = GridSearchCV(
            LogisticRegression(random_state=42),
            param_grid,
            cv=tscv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=0
        )
        
        grid_search.fit(X, y)
        
        logger.info(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
        logger.info(f"æœ€é©ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.1%}")
        
        return grid_search.best_estimator_, grid_search.best_score_
    
    def multiple_seed_evaluation(self, X, y, best_model, n_trials=10):
        """è¤‡æ•°ã‚·ãƒ¼ãƒ‰è©•ä¾¡ã§å®‰å®šæ€§ç¢ºèª"""
        logger.info(f"ğŸ² è¤‡æ•°ã‚·ãƒ¼ãƒ‰è©•ä¾¡ï¼ˆ{n_trials}å›è©¦è¡Œï¼‰...")
        
        scores = []
        
        for seed in range(42, 42 + n_trials):
            # ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
            model = LogisticRegression(**best_model.get_params())
            model.set_params(random_state=seed)
            
            # æ™‚ç³»åˆ—åˆ†å‰²è©•ä¾¡
            tscv = TimeSeriesSplit(n_splits=3)
            fold_scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train = X[train_idx]
                X_test = X[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                fold_scores.append(accuracy_score(y_test, pred))
            
            trial_score = np.mean(fold_scores)
            scores.append(trial_score)
            
            if trial_score >= 0.553:  # 55.3%
                logger.info(f"è©¦è¡Œ{seed-41:2d}: {trial_score:.1%} âœ… ç›®æ¨™é”æˆ!")
            else:
                logger.info(f"è©¦è¡Œ{seed-41:2d}: {trial_score:.1%}")
        
        avg_score = np.mean(scores)
        std_score = np.std(scores)
        max_score = np.max(scores)
        success_rate = np.mean([s >= 0.553 for s in scores])
        
        logger.info(f"\nğŸ“Š è¤‡æ•°ã‚·ãƒ¼ãƒ‰çµæœ:")
        logger.info(f"å¹³å‡ç²¾åº¦: {avg_score:.1%} Â± {std_score:.1%}")
        logger.info(f"æœ€é«˜ç²¾åº¦: {max_score:.1%}")
        logger.info(f"ç›®æ¨™é”æˆç‡: {success_rate:.1%}")
        
        return scores, max_score, success_rate
    
    def final_best_configuration(self, df, jquants_features):
        """æœ€çµ‚æœ€é©æ§‹æˆ"""
        logger.info("ğŸ¯ æœ€çµ‚æœ€é©æ§‹æˆã§ã®è©•ä¾¡...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[jquants_features]
        y = clean_df['Binary_Direction'].astype(int)
        X_scaled = self.scaler.fit_transform(X)
        
        # æœ€é©æ§‹æˆï¼ˆå‰å›ã®çµæœã‹ã‚‰ï¼‰
        best_config = LogisticRegression(
            C=0.01,
            class_weight='balanced',
            solver='liblinear',
            max_iter=1000,
            random_state=42
        )
        
        # ã•ã‚‰ã«å³å¯†ãªè©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            best_config.fit(X_train, y_train)
            pred = best_config.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            logger.info(f"Fold {fold+1}: {accuracy:.1%}")
        
        final_accuracy = np.mean(scores)
        std_accuracy = np.std(scores)
        
        logger.info(f"\nğŸ¯ æœ€çµ‚çµæœ: {final_accuracy:.1%} Â± {std_accuracy:.1%}")
        
        return final_accuracy, std_accuracy, scores

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã§55.3%ç¢ºå®Ÿé”æˆ")
    logger.info("ç›®æ¨™: 55.3%ä»¥ä¸Šã®ç²¾åº¦ç¢ºå®Ÿé”æˆ")
    
    optimizer = JQuantsPrecisionOptimizer()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df = optimizer.load_and_prepare_data(sample_size=50000)
        if df is None:
            return
        
        # 2. J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆ
        df = optimizer.create_jquants_like_features(df)
        jquants_features = optimizer.get_jquants_features(df)
        
        # 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[jquants_features]
        y = clean_df['Binary_Direction'].astype(int)
        X_scaled = optimizer.scaler.fit_transform(X)
        
        # 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        best_model, best_score = optimizer.hyperparameter_optimization(X_scaled, y)
        
        # 5. è¤‡æ•°ã‚·ãƒ¼ãƒ‰è©•ä¾¡
        seed_scores, max_score, success_rate = optimizer.multiple_seed_evaluation(
            X_scaled, y, best_model, n_trials=20
        )
        
        # 6. æœ€çµ‚æœ€é©æ§‹æˆ
        final_accuracy, std_accuracy, fold_scores = optimizer.final_best_configuration(
            df, jquants_features
        )
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*60)
        
        logger.info(f"ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–: {best_score:.1%}")
        logger.info(f"è¤‡æ•°ã‚·ãƒ¼ãƒ‰æœ€é«˜ç²¾åº¦: {max_score:.1%}")
        logger.info(f"ç›®æ¨™é”æˆç‡: {success_rate:.1%}")
        logger.info(f"æœ€çµ‚æ§‹æˆç²¾åº¦: {final_accuracy:.1%} Â± {std_accuracy:.1%}")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_accuracy = 0.553  # 55.3%
        achievement_scores = [best_score, max_score, final_accuracy]
        max_achievement = max(achievement_scores)
        
        if max_achievement >= target_accuracy:
            logger.info(f"ğŸ‰ ç›®æ¨™é”æˆï¼æœ€é«˜ç²¾åº¦: {max_achievement:.1%} >= {target_accuracy:.1%}")
            logger.info("âœ… J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã§55.3%ä»¥ä¸Šç¢ºå®Ÿé”æˆ")
        else:
            logger.warning(f"âš ï¸  ç›®æ¨™æœªé”: æœ€é«˜{max_achievement:.1%} < {target_accuracy:.1%}")
            logger.info(f"å·®: {(target_accuracy - max_achievement)*100:.1f}%")
        
        logger.info(f"\nä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(jquants_features)}")
        logger.info("J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡:")
        for i, feature in enumerate(jquants_features[:10]):  # ä¸Šä½10å€‹è¡¨ç¤º
            logger.info(f"  {i+1:2d}. {feature}")
        if len(jquants_features) > 10:
            logger.info(f"  ... ä»–{len(jquants_features)-10}å€‹")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()