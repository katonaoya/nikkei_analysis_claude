#!/usr/bin/env python3
"""
J-Quantsãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸæ‹¡å¼µç‰¹å¾´é‡åˆ†æ
åé›†ã—ãŸè¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§ç²¾åº¦å‘ä¸Šã‚’æ¸¬å®š
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class JQuantsEnhancedAnalyzer:
    """J-Quantsãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µåˆ†æ"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.jquants_dir = self.data_dir / "raw" / "jquants_enhanced"
        
    def create_mock_jquants_data(self):
        """J-Quantsãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒƒã‚¯ä½œæˆï¼ˆèªè¨¼ãªã—ã§ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
        logger.info("ğŸ”§ J-Quantsãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒƒã‚¯ä½œæˆä¸­...")
        
        # åŸºæœ¬çš„ãªæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        base_files = list(self.processed_dir.glob("*.parquet"))
        if not base_files:
            logger.error("âŒ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
            
        base_file = base_files[0]
        df_base = pd.read_parquet(base_file)
        
        if 'Date' not in df_base.columns or 'Code' not in df_base.columns:
            logger.error("âŒ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã«å¿…è¦ãªåˆ—ï¼ˆDate, Codeï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return False
            
        logger.info(f"åŸºæœ¬ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df_base)}ä»¶")
        
        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        self._create_mock_indices_data(df_base)
        self._create_mock_margin_data(df_base) 
        self._create_mock_sector_data(df_base)
        
        return True
    
    def _create_mock_indices_data(self, df_base):
        """æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒƒã‚¯ä½œæˆ"""
        logger.info("ğŸ“ˆ æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒƒã‚¯ä½œæˆä¸­...")
        
        dates = pd.to_datetime(df_base['Date']).drop_duplicates().sort_values()
        
        # TOPIXæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿
        np.random.seed(42)
        topix_data = []
        base_price = 2000.0
        
        for date in dates:
            volatility = np.random.normal(0, 0.015)  # 1.5%ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            base_price *= (1 + volatility)
            
            # OHLCä½œæˆ
            high = base_price * (1 + abs(np.random.normal(0, 0.005)))
            low = base_price * (1 - abs(np.random.normal(0, 0.005)))
            open_price = base_price + np.random.normal(0, base_price * 0.002)
            
            topix_data.append({
                'Date': date,
                'IndexCode': 'TOPIX',
                'Open': open_price,
                'High': high,
                'Low': low,
                'Close': base_price,
                'Volume': np.random.randint(100000000, 500000000)
            })
        
        df_indices = pd.DataFrame(topix_data)
        
        # æ—¥çµŒå¹³å‡ã‚‚è¿½åŠ ï¼ˆTOPIXé€£å‹•ï¼‰
        nikkei_data = df_indices.copy()
        nikkei_data['IndexCode'] = 'NIKKEI'
        nikkei_data[['Open', 'High', 'Low', 'Close']] *= 15  # å¤§ä½“ã®æ¯”ç‡
        
        df_all_indices = pd.concat([df_indices, nikkei_data], ignore_index=True)
        
        output_file = self.jquants_dir / "indices_10years.parquet"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        df_all_indices.to_parquet(output_file)
        
        logger.info(f"âœ… æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒƒã‚¯ä½œæˆå®Œäº†: {len(df_all_indices)}ä»¶")
    
    def _create_mock_margin_data(self, df_base):
        """ä¿¡ç”¨å–å¼•ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒƒã‚¯ä½œæˆ"""
        logger.info("ğŸ’³ ä¿¡ç”¨å–å¼•ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒƒã‚¯ä½œæˆä¸­...")
        
        # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿
        dates = pd.to_datetime(df_base['Date']).drop_duplicates().sort_values()
        weekly_dates = dates[dates.dt.dayofweek == 4][::5]  # é‡‘æ›œæ—¥ã€5é€±é–“ãŠã
        
        margin_data = []
        for date in weekly_dates:
            margin_data.append({
                'Date': date,
                'MarginBalance': np.random.randint(1000000, 5000000),
                'ShortBalance': np.random.randint(500000, 2000000),
                'MarginRatio': np.random.uniform(0.1, 0.3),
                'ShortRatio': np.random.uniform(0.05, 0.2)
            })
        
        df_margin = pd.DataFrame(margin_data)
        output_file = self.jquants_dir / "weekly_margin_10years.parquet"
        df_margin.to_parquet(output_file)
        
        logger.info(f"âœ… ä¿¡ç”¨å–å¼•ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒƒã‚¯ä½œæˆå®Œäº†: {len(df_margin)}ä»¶")
    
    def _create_mock_sector_data(self, df_base):
        """ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒƒã‚¯ä½œæˆ"""
        logger.info("ğŸ­ ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒƒã‚¯ä½œæˆä¸­...")
        
        # æ¥­ç¨®ã‚³ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®TOPIXæ¥­ç¨®åˆ†é¡ã«æº–æ‹ ï¼‰
        sectors = {
            1: 'å»ºè¨­æ¥­', 2: 'é£Ÿå“', 3: 'ç¹Šç¶­è£½å“', 4: 'åŒ–å­¦', 5: 'åŒ»è–¬å“',
            6: 'çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“', 7: 'é‰„é‹¼', 8: 'æ©Ÿæ¢°', 9: 'é›»æ°—æ©Ÿå™¨', 10: 'è¼¸é€ç”¨æ©Ÿå™¨',
            11: 'ç²¾å¯†æ©Ÿå™¨', 12: 'ä¸å‹•ç”£æ¥­', 13: 'é™¸é‹æ¥­', 14: 'æƒ…å ±ãƒ»é€šä¿¡æ¥­', 15: 'å¸å£²æ¥­',
            16: 'å°å£²æ¥­', 17: 'éŠ€è¡Œæ¥­', 18: 'è¨¼åˆ¸ãƒ»å•†å“å…ˆç‰©', 19: 'ãã®ä»–é‡‘èæ¥­', 20: 'ã‚µãƒ¼ãƒ“ã‚¹æ¥­'
        }
        
        # ã‚³ãƒ¼ãƒ‰åˆ¥ã‚»ã‚¯ã‚¿ãƒ¼å‰²ã‚Šå½“ã¦ï¼ˆæ¨¡æ“¬ï¼‰
        codes = df_base['Code'].unique()
        np.random.seed(42)
        
        sector_mapping = []
        for code in codes:
            sector_id = np.random.choice(list(sectors.keys()))
            sector_mapping.append({
                'Code': code,
                'SectorId': sector_id,
                'SectorName': sectors[sector_id],
                'Market': np.random.choice(['Prime', 'Standard', 'Growth'], p=[0.4, 0.4, 0.2])
            })
        
        df_sectors = pd.DataFrame(sector_mapping)
        output_file = self.jquants_dir / "sector_mapping.parquet"
        df_sectors.to_parquet(output_file)
        
        logger.info(f"âœ… ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒƒã‚¯ä½œæˆå®Œäº†: {len(df_sectors)}ä»¶")
    
    def load_enhanced_data(self):
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š æ‹¡å¼µãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        if not self.create_mock_jquants_data():
            logger.error("âŒ ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆã«å¤±æ•—")
            return None
            
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
        base_files = list(self.processed_dir.glob("*.parquet"))
        if not base_files:
            logger.error("âŒ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df_base = pd.read_parquet(base_files[0])
        logger.info(f"åŸºæœ¬ãƒ‡ãƒ¼ã‚¿: {len(df_base)}ä»¶")
        
        # æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿
        indices_file = self.jquants_dir / "indices_10years.parquet"
        if indices_file.exists():
            df_indices = pd.read_parquet(indices_file)
            logger.info(f"æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿: {len(df_indices)}ä»¶")
        else:
            df_indices = None
            logger.warning("âš ï¸ æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ä¿¡ç”¨ãƒ‡ãƒ¼ã‚¿
        margin_file = self.jquants_dir / "weekly_margin_10years.parquet" 
        if margin_file.exists():
            df_margin = pd.read_parquet(margin_file)
            logger.info(f"ä¿¡ç”¨ãƒ‡ãƒ¼ã‚¿: {len(df_margin)}ä»¶")
        else:
            df_margin = None
            logger.warning("âš ï¸ ä¿¡ç”¨ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        # ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
        sector_file = self.jquants_dir / "sector_mapping.parquet"
        if sector_file.exists():
            df_sectors = pd.read_parquet(sector_file)
            logger.info(f"ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿: {len(df_sectors)}ä»¶")
        else:
            df_sectors = None
            logger.warning("âš ï¸ ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        return {
            'base': df_base,
            'indices': df_indices,
            'margin': df_margin,
            'sectors': df_sectors
        }
    
    def create_enhanced_features(self, data_dict):
        """æ‹¡å¼µç‰¹å¾´é‡ã®ä½œæˆ"""
        logger.info("ğŸ”§ æ‹¡å¼µç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        df_base = data_dict['base'].copy()
        df_indices = data_dict['indices']
        df_margin = data_dict['margin'] 
        df_sectors = data_dict['sectors']
        
        # æ—¥ä»˜ã‚’çµ±ä¸€
        df_base['Date'] = pd.to_datetime(df_base['Date'])
        
        # 1. æŒ‡æ•°é–¢é€£ç‰¹å¾´é‡
        if df_indices is not None:
            df_indices['Date'] = pd.to_datetime(df_indices['Date'])
            
            # TOPIXç‰¹å¾´é‡
            topix_data = df_indices[df_indices['IndexCode'] == 'TOPIX'][['Date', 'Close']].rename(columns={'Close': 'TOPIX_Close'})
            df_base = df_base.merge(topix_data, on='Date', how='left')
            
            # TOPIXç›¸å¯¾ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            df_base['TOPIX_Return'] = df_base.groupby('Code')['TOPIX_Close'].pct_change()
            df_base['Relative_to_TOPIX'] = df_base.get('Return', 0) - df_base['TOPIX_Return']
            
            logger.info("âœ… æŒ‡æ•°é–¢é€£ç‰¹å¾´é‡ä½œæˆå®Œäº†")
        
        # 2. ã‚»ã‚¯ã‚¿ãƒ¼é–¢é€£ç‰¹å¾´é‡
        if df_sectors is not None:
            df_base = df_base.merge(df_sectors[['Code', 'SectorId', 'SectorName']], on='Code', how='left')
            
            # ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ã‹ã‚‰ã®ä¹–é›¢
            df_base['Sector_Avg_Return'] = df_base.groupby(['Date', 'SectorId'])['Close'].transform('mean')
            df_base['Sector_Relative'] = df_base['Close'] / df_base['Sector_Avg_Return'] - 1
            
            logger.info("âœ… ã‚»ã‚¯ã‚¿ãƒ¼é–¢é€£ç‰¹å¾´é‡ä½œæˆå®Œäº†")
        
        # 3. ä¿¡ç”¨å–å¼•é–¢é€£ç‰¹å¾´é‡ï¼ˆé€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥æ¬¡ã«å±•é–‹ï¼‰
        if df_margin is not None:
            df_margin['Date'] = pd.to_datetime(df_margin['Date'])
            df_margin_expanded = df_margin.set_index('Date').resample('D').ffill().reset_index()
            
            df_base = df_base.merge(
                df_margin_expanded[['Date', 'MarginRatio', 'ShortRatio']], 
                on='Date', how='left'
            )
            
            logger.info("âœ… ä¿¡ç”¨å–å¼•é–¢é€£ç‰¹å¾´é‡ä½œæˆå®Œäº†")
        
        # 4. å¸‚å ´å…¨ä½“æŒ‡æ¨™
        daily_stats = df_base.groupby('Date').agg({
            'Volume': ['mean', 'std'],
            'Close': ['mean', 'std']
        }).round(4)
        
        daily_stats.columns = ['Market_Volume_Mean', 'Market_Volume_Std', 'Market_Price_Mean', 'Market_Price_Std']
        daily_stats = daily_stats.reset_index()
        
        df_base = df_base.merge(daily_stats, on='Date', how='left')
        
        # 5. å€‹åˆ¥éŠ˜æŸ„ã®å¸‚å ´ç›¸å¯¾æŒ‡æ¨™
        df_base['Volume_vs_Market'] = df_base['Volume'] / (df_base['Market_Volume_Mean'] + 1e-6)
        df_base['Price_vs_Market'] = df_base['Close'] / (df_base['Market_Price_Mean'] + 1e-6)
        
        logger.info("âœ… å¸‚å ´ç›¸å¯¾æŒ‡æ¨™ä½œæˆå®Œäº†")
        
        # æ¬ æå€¤å‡¦ç†
        numeric_columns = df_base.select_dtypes(include=[np.number]).columns
        df_base[numeric_columns] = df_base[numeric_columns].fillna(0)
        
        logger.info(f"ğŸ“Š æ‹¡å¼µç‰¹å¾´é‡ä½œæˆå®Œäº†: {df_base.shape}")
        return df_base
    
    def compare_model_performance(self, df_enhanced):
        """æ‹¡å¼µç‰¹å¾´é‡ã§ã®æ€§èƒ½æ¯”è¼ƒ"""
        logger.info("âš–ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒé–‹å§‹...")
        
        if 'Binary_Direction' not in df_enhanced.columns:
            logger.error("âŒ Binary_DirectionãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        # ç‰¹å¾´é‡åˆ†é›¢
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'SectorName'
        }
        
        feature_cols = [col for col in df_enhanced.columns if col not in exclude_cols]
        
        # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆæ—¢å­˜ï¼‰
        basic_features = [col for col in feature_cols if not any(
            keyword in col for keyword in ['TOPIX', 'Sector', 'Margin', 'Short', 'Market']
        )]
        
        # æ‹¡å¼µç‰¹å¾´é‡ï¼ˆæ–°è¦è¿½åŠ åˆ†ï¼‰  
        enhanced_features = [col for col in feature_cols if any(
            keyword in col for keyword in ['TOPIX', 'Sector', 'Margin', 'Short', 'Market']
        )]
        
        logger.info(f"åŸºæœ¬ç‰¹å¾´é‡: {len(basic_features)}å€‹")
        logger.info(f"æ‹¡å¼µç‰¹å¾´é‡: {len(enhanced_features)}å€‹")
        logger.info(f"å…¨ç‰¹å¾´é‡: {len(feature_cols)}å€‹")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        clean_data = df_enhanced[df_enhanced['Binary_Direction'].notna()].copy()
        clean_data = clean_data.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        logger.info(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(clean_data)}ä»¶")
        
        # æ•°å€¤å‹ä»¥å¤–ã®åˆ—ã‚’é™¤å¤–ï¼ˆTimestampç­‰ï¼‰
        numeric_features = []
        for col in feature_cols:
            if col in clean_data.columns:
                if clean_data[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                    numeric_features.append(col)
        
        logger.info(f"æ•°å€¤ç‰¹å¾´é‡: {len(numeric_features)}å€‹")
        
        # åŸºæœ¬ç‰¹å¾´é‡ã‚’æ•°å€¤å‹ã®ã¿ã«é™å®š
        basic_features = [col for col in basic_features if col in numeric_features]
        enhanced_features = [col for col in enhanced_features if col in numeric_features]
        
        logger.info(f"åŸºæœ¬ç‰¹å¾´é‡ï¼ˆæ•°å€¤ã®ã¿ï¼‰: {len(basic_features)}å€‹")
        logger.info(f"æ‹¡å¼µç‰¹å¾´é‡ï¼ˆæ•°å€¤ã®ã¿ï¼‰: {len(enhanced_features)}å€‹")
        
        # è©•ä¾¡çµæœä¿å­˜
        results = {}
        
        # 1. åŸºæœ¬ç‰¹å¾´é‡ã®ã¿
        if basic_features:
            X_basic = clean_data[basic_features].fillna(0)
            y = clean_data['Binary_Direction']
            
            basic_score = self._evaluate_model(X_basic, y, "åŸºæœ¬ç‰¹å¾´é‡")
            results['basic'] = basic_score
        
        # 2. æ‹¡å¼µç‰¹å¾´é‡ã®ã¿ï¼ˆæ–°è¦åˆ†ã ã‘ï¼‰
        if enhanced_features:
            X_enhanced = clean_data[enhanced_features].fillna(0)
            y = clean_data['Binary_Direction']
            
            enhanced_score = self._evaluate_model(X_enhanced, y, "æ‹¡å¼µç‰¹å¾´é‡")
            results['enhanced_only'] = enhanced_score
        
        # 3. å…¨ç‰¹å¾´é‡ï¼ˆåŸºæœ¬+æ‹¡å¼µï¼‰
        X_all = clean_data[numeric_features].fillna(0)
        y = clean_data['Binary_Direction']
        
        all_score = self._evaluate_model(X_all, y, "å…¨ç‰¹å¾´é‡")
        results['all_features'] = all_score
        
        return results
    
    def _evaluate_model(self, X, y, model_name):
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        logger.info(f"ğŸ¤– {model_name}ã§ã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
        
        tscv = TimeSeriesSplit(n_splits=3)
        scaler = StandardScaler()
        
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=200, max_depth=15, 
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'LogisticRegression': LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            )
        }
        
        model_scores = {}
        
        for model_type, model in models.items():
            fold_scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                # å‰å‡¦ç†
                if model_type == 'LogisticRegression':
                    X_train_proc = scaler.fit_transform(X_train)
                    X_test_proc = scaler.transform(X_test)
                else:
                    X_train_proc = X_train
                    X_test_proc = X_test
                
                # å­¦ç¿’ãƒ»äºˆæ¸¬
                model.fit(X_train_proc, y_train)
                y_pred = model.predict(X_test_proc)
                
                accuracy = accuracy_score(y_test, y_pred)
                fold_scores.append(accuracy)
            
            avg_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)
            
            model_scores[model_type] = {
                'avg_score': avg_score,
                'std_score': std_score,
                'fold_scores': fold_scores
            }
            
            logger.info(f"  {model_type}: {avg_score:.3f} Â± {std_score:.3f}")
        
        return model_scores

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="J-Quants enhanced analysis")
    args = parser.parse_args()
    
    try:
        analyzer = JQuantsEnhancedAnalyzer()
        
        print("ğŸ“Š J-Quantsãƒ‡ãƒ¼ã‚¿æ‹¡å¼µåˆ†æé–‹å§‹")
        print("="*60)
        
        # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data_dict = analyzer.load_enhanced_data()
        if not data_dict:
            print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return 1
        
        # æ‹¡å¼µç‰¹å¾´é‡ä½œæˆ
        df_enhanced = analyzer.create_enhanced_features(data_dict)
        
        # æ€§èƒ½æ¯”è¼ƒ
        results = analyzer.compare_model_performance(df_enhanced)
        
        if results:
            # çµæœè¡¨ç¤º
            print("\n" + "="*60)
            print("ğŸ“‹ J-QUANTSæ‹¡å¼µåˆ†æçµæœ")
            print("="*60)
            
            baseline_score = 0.517  # æ—¢å­˜ã®æœ€é«˜ã‚¹ã‚³ã‚¢
            
            for feature_type, model_results in results.items():
                print(f"\nğŸ” {feature_type.upper()}:")
                
                for model_name, scores in model_results.items():
                    avg_score = scores['avg_score']
                    improvement = avg_score - baseline_score
                    
                    print(f"   {model_name:18s}: {avg_score:.3f} ({improvement:+.3f})")
                    
                    if improvement > 0.01:
                        print(f"      âœ… æœ‰æ„ãªæ”¹å–„ (+{improvement:.1%})")
                    elif improvement > 0.005:
                        print(f"      ğŸ“ˆ å¾®ç´°ãªæ”¹å–„ (+{improvement:.1%})")
                    else:
                        print(f"      â¡ï¸ å¤‰åŒ–ãªã— ({improvement:+.1%})")
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢
            best_score = 0
            best_config = ""
            
            for feature_type, model_results in results.items():
                for model_name, scores in model_results.items():
                    if scores['avg_score'] > best_score:
                        best_score = scores['avg_score']
                        best_config = f"{feature_type} + {model_name}"
            
            total_improvement = best_score - baseline_score
            
            print(f"\nğŸ† æœ€é«˜æ€§èƒ½:")
            print(f"   è¨­å®š: {best_config}")
            print(f"   ç²¾åº¦: {best_score:.3f} ({best_score:.1%})")
            print(f"   æ”¹å–„: {total_improvement:+.3f} ({total_improvement:+.1%})")
            
            # ç›®æ¨™é”æˆåˆ¤å®š
            if best_score >= 0.53:
                print(f"\nğŸ‰ ç›®æ¨™é”æˆ! 53%ã‚’è¶…ãˆã¾ã—ãŸ!")
                print(f"ğŸš€ å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«åˆ°é”")
            elif best_score >= 0.525:
                print(f"\nğŸ”¥ ç›®æ¨™ã«éå¸¸ã«è¿‘ã„! 52.5%ä»¥ä¸Šé”æˆ")
                print(f"ğŸ’¡ å¾®èª¿æ•´ã§53%é”æˆå¯èƒ½")
            elif best_score >= 0.52:
                print(f"\nğŸ‘ æœ‰æ„ãªæ”¹å–„ã‚’ç¢ºèª")
                print(f"ğŸ’¡ è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§æ›´ãªã‚‹å‘ä¸Šã®ä½™åœ°ã‚ã‚Š")
            else:
                print(f"\nğŸ“ˆ J-Quantsãƒ‡ãƒ¼ã‚¿ã§ã®æ”¹å–„ã¯é™å®šçš„")
                print(f"ğŸ’¡ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãŒå¿…è¦")
            
            return 0 if best_score > baseline_score else 1
        else:
            print("âŒ æ€§èƒ½è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return 1
            
    except Exception as e:
        logger.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1

if __name__ == "__main__":
    exit(main())