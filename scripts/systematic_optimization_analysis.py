#!/usr/bin/env python3
"""
ç³»çµ±çš„æœ€é©åŒ–åˆ†æ - ãƒ‡ãƒ¼ã‚¿è¿½åŠ ä»¥å¤–ã®æ”¹å–„æ‰‹æ³•
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»ãƒ­ã‚¸ãƒƒã‚¯å¤‰æ›´ãƒ»å‰å‡¦ç†æ”¹å–„ç­‰ã®åŒ…æ‹¬çš„æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class SystematicOptimizer:
    """ç³»çµ±çš„æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡ï¼ˆç¾åœ¨æœ€é«˜ã®çµ„ã¿åˆã‚ã›ï¼‰
        self.baseline_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'RSI', 'Price_vs_MA20'
        ]
        
    def load_full_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ394,102ä»¶ï¼‰")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
        
        return df
    
    def prepare_data(self, df):
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[self.baseline_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        logger.info(f"ç‰¹å¾´é‡: {self.baseline_features}")
        
        return X, y, clean_df
    
    def baseline_evaluation(self, X, y):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡"""
        logger.info("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡...")
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, random_state=42)
        
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            scores.append(accuracy_score(y_test, pred))
        
        baseline_score = np.mean(scores)
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦: {baseline_score:.1%}")
        
        return baseline_score
    
    def preprocessing_optimization(self, X, y):
        """å‰å‡¦ç†æœ€é©åŒ–"""
        logger.info("ğŸ”§ å‰å‡¦ç†æœ€é©åŒ–...")
        
        # å„ç¨®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
        scalers = {
            'StandardScaler': StandardScaler(),
            'MinMaxScaler': MinMaxScaler(),
            'RobustScaler': RobustScaler(),
            'QuantileTransformer': QuantileTransformer(n_quantiles=1000, random_state=42),
            'None': None
        }
        
        # å¤–ã‚Œå€¤å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³
        outlier_methods = {
            'None': lambda x: x,
            'Clip_99': lambda x: x.clip(lower=x.quantile(0.01), upper=x.quantile(0.99), axis=0),
            'Clip_95': lambda x: x.clip(lower=x.quantile(0.025), upper=x.quantile(0.975), axis=0),
            'Winsorize': lambda x: x.clip(lower=x.quantile(0.05), upper=x.quantile(0.95), axis=0)
        }
        
        preprocessing_results = {}
        
        for outlier_name, outlier_func in outlier_methods.items():
            for scaler_name, scaler in scalers.items():
                logger.info(f"  {outlier_name} + {scaler_name}...")
                
                # å¤–ã‚Œå€¤å‡¦ç†
                X_processed = outlier_func(X.copy())
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                if scaler is not None:
                    X_scaled = scaler.fit_transform(X_processed)
                else:
                    X_scaled = X_processed.values
                
                # è©•ä¾¡
                model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, random_state=42)
                tscv = TimeSeriesSplit(n_splits=3)
                scores = []
                
                for train_idx, test_idx in tscv.split(X_scaled):
                    X_train = X_scaled[train_idx]
                    X_test = X_scaled[test_idx]
                    y_train = y.iloc[train_idx]
                    y_test = y.iloc[test_idx]
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_test)
                    scores.append(accuracy_score(y_test, pred))
                
                avg_score = np.mean(scores)
                combo_name = f"{outlier_name}+{scaler_name}"
                preprocessing_results[combo_name] = avg_score
                
                logger.info(f"    {combo_name}: {avg_score:.1%}")
        
        # æœ€é«˜å‰å‡¦ç†
        best_preprocessing = max(preprocessing_results.keys(), key=lambda k: preprocessing_results[k])
        best_preprocessing_score = preprocessing_results[best_preprocessing]
        
        logger.info(f"\nğŸ† æœ€é«˜å‰å‡¦ç†: {best_preprocessing} ({best_preprocessing_score:.1%})")
        
        return preprocessing_results, best_preprocessing
    
    def hyperparameter_optimization(self, X, y, best_preprocessing):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        logger.info("âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–...")
        
        # æœ€é©å‰å‡¦ç†ã‚’é©ç”¨
        outlier_method, scaler_method = best_preprocessing.split('+')
        
        # å¤–ã‚Œå€¤å‡¦ç†
        if outlier_method == 'Clip_99':
            X_processed = X.clip(lower=X.quantile(0.01), upper=X.quantile(0.99), axis=0)
        elif outlier_method == 'Clip_95':
            X_processed = X.clip(lower=X.quantile(0.025), upper=X.quantile(0.975), axis=0)
        elif outlier_method == 'Winsorize':
            X_processed = X.clip(lower=X.quantile(0.05), upper=X.quantile(0.95), axis=0)
        else:
            X_processed = X.copy()
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if scaler_method == 'StandardScaler':
            scaler = StandardScaler()
        elif scaler_method == 'MinMaxScaler':
            scaler = MinMaxScaler()
        elif scaler_method == 'RobustScaler':
            scaler = RobustScaler()
        elif scaler_method == 'QuantileTransformer':
            scaler = QuantileTransformer(n_quantiles=1000, random_state=42)
        else:
            scaler = None
        
        if scaler is not None:
            X_scaled = scaler.fit_transform(X_processed)
        else:
            X_scaled = X_processed.values
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        hyperparameter_results = {}
        
        # 1. LogisticRegression
        logger.info("  LogisticRegressionæœ€é©åŒ–...")
        lr_param_grid = {
            'C': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0],
            'class_weight': ['balanced', {0: 1, 1: 1.1}, {0: 1, 1: 1.2}, {0: 1, 1: 1.3}, {0: 1, 1: 1.5}],
            'solver': ['liblinear', 'lbfgs'],
            'penalty': ['l1', 'l2']
        }
        
        # solverã¨penaltyã®äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
        compatible_params = []
        for params in self._generate_param_combinations(lr_param_grid):
            if params['solver'] == 'liblinear' or params['penalty'] == 'l2':
                compatible_params.append(params)
        
        best_lr_score = 0
        best_lr_params = None
        
        # åŠ¹ç‡çš„ãªã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        import random
        random.seed(42)
        sampled_params = random.sample(compatible_params, min(20, len(compatible_params)))
        
        for params in sampled_params:
            try:
                model = LogisticRegression(**params, max_iter=2000, random_state=42)
                tscv = TimeSeriesSplit(n_splits=3)
                scores = []
                
                for train_idx, test_idx in tscv.split(X_scaled):
                    X_train = X_scaled[train_idx]
                    X_test = X_scaled[test_idx]
                    y_train = y.iloc[train_idx]
                    y_test = y.iloc[test_idx]
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_test)
                    scores.append(accuracy_score(y_test, pred))
                
                avg_score = np.mean(scores)
                
                if avg_score > best_lr_score:
                    best_lr_score = avg_score
                    best_lr_params = params
                    
            except Exception as e:
                continue
        
        hyperparameter_results['LogisticRegression'] = {
            'score': best_lr_score,
            'params': best_lr_params
        }
        
        logger.info(f"    æœ€é©LR: {best_lr_score:.1%} {best_lr_params}")
        
        # 2. RandomForest
        logger.info("  RandomForestæœ€é©åŒ–...")
        rf_param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 8, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'class_weight': ['balanced', 'balanced_subsample']
        }
        
        rf_combinations = list(self._generate_param_combinations(rf_param_grid))
        sampled_rf_params = random.sample(rf_combinations, min(15, len(rf_combinations)))
        
        best_rf_score = 0
        best_rf_params = None
        
        for params in sampled_rf_params:
            try:
                model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)
                tscv = TimeSeriesSplit(n_splits=3)
                scores = []
                
                for train_idx, test_idx in tscv.split(X_scaled):
                    X_train = X_scaled[train_idx]
                    X_test = X_scaled[test_idx]
                    y_train = y.iloc[train_idx]
                    y_test = y.iloc[test_idx]
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_test)
                    scores.append(accuracy_score(y_test, pred))
                
                avg_score = np.mean(scores)
                
                if avg_score > best_rf_score:
                    best_rf_score = avg_score
                    best_rf_params = params
                    
            except Exception as e:
                continue
        
        hyperparameter_results['RandomForest'] = {
            'score': best_rf_score,
            'params': best_rf_params
        }
        
        logger.info(f"    æœ€é©RF: {best_rf_score:.1%} {best_rf_params}")
        
        return hyperparameter_results
    
    def _generate_param_combinations(self, param_grid):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ç”Ÿæˆ"""
        import itertools
        keys = param_grid.keys()
        values = param_grid.values()
        combinations = list(itertools.product(*values))
        return [dict(zip(keys, combo)) for combo in combinations]
    
    def advanced_modeling_techniques(self, X, y, best_preprocessing):
        """é«˜åº¦ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•"""
        logger.info("ğŸ§  é«˜åº¦ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•...")
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        outlier_method, scaler_method = best_preprocessing.split('+')
        X_processed = self._apply_preprocessing(X, outlier_method, scaler_method)
        
        advanced_results = {}
        
        # 1. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
        logger.info("  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•...")
        
        base_models = [
            ('lr', LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, random_state=42)),
            ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)),
            ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42))
        ]
        
        voting_clf = VotingClassifier(estimators=base_models, voting='soft')
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=3)
        ensemble_scores = []
        
        for train_idx, test_idx in tscv.split(X_processed):
            X_train = X_processed[train_idx]
            X_test = X_processed[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            voting_clf.fit(X_train, y_train)
            pred = voting_clf.predict(X_test)
            ensemble_scores.append(accuracy_score(y_test, pred))
        
        advanced_results['VotingEnsemble'] = np.mean(ensemble_scores)
        logger.info(f"    VotingEnsemble: {np.mean(ensemble_scores):.1%}")
        
        # 2. Neural Network
        logger.info("  Neural Network...")
        try:
            mlp = MLPClassifier(
                hidden_layer_sizes=(100, 50),
                max_iter=500,
                random_state=42,
                early_stopping=True,
                validation_fraction=0.1
            )
            
            nn_scores = []
            for train_idx, test_idx in tscv.split(X_processed):
                X_train = X_processed[train_idx]
                X_test = X_processed[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                mlp.fit(X_train, y_train)
                pred = mlp.predict(X_test)
                nn_scores.append(accuracy_score(y_test, pred))
            
            advanced_results['NeuralNetwork'] = np.mean(nn_scores)
            logger.info(f"    NeuralNetwork: {np.mean(nn_scores):.1%}")
            
        except Exception as e:
            logger.info(f"    NeuralNetwork: ã‚¹ã‚­ãƒƒãƒ— ({str(e)[:50]})")
        
        # 3. Gradient Boostingè©³ç´°èª¿æ•´
        logger.info("  Gradient Boosting...")
        gb_params = [
            {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1},
            {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.05},
            {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.03}
        ]
        
        best_gb_score = 0
        for params in gb_params:
            gb = GradientBoostingClassifier(**params, random_state=42)
            gb_scores = []
            
            for train_idx, test_idx in tscv.split(X_processed):
                X_train = X_processed[train_idx]
                X_test = X_processed[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                gb.fit(X_train, y_train)
                pred = gb.predict(X_test)
                gb_scores.append(accuracy_score(y_test, pred))
            
            avg_gb_score = np.mean(gb_scores)
            if avg_gb_score > best_gb_score:
                best_gb_score = avg_gb_score
        
        advanced_results['GradientBoosting'] = best_gb_score
        logger.info(f"    GradientBoosting: {best_gb_score:.1%}")
        
        return advanced_results
    
    def _apply_preprocessing(self, X, outlier_method, scaler_method):
        """å‰å‡¦ç†é©ç”¨"""
        # å¤–ã‚Œå€¤å‡¦ç†
        if outlier_method == 'Clip_99':
            X_processed = X.clip(lower=X.quantile(0.01), upper=X.quantile(0.99), axis=0)
        elif outlier_method == 'Clip_95':
            X_processed = X.clip(lower=X.quantile(0.025), upper=X.quantile(0.975), axis=0)
        elif outlier_method == 'Winsorize':
            X_processed = X.clip(lower=X.quantile(0.05), upper=X.quantile(0.95), axis=0)
        else:
            X_processed = X.copy()
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if scaler_method == 'StandardScaler':
            scaler = StandardScaler()
        elif scaler_method == 'MinMaxScaler':
            scaler = MinMaxScaler()
        elif scaler_method == 'RobustScaler':
            scaler = RobustScaler()
        elif scaler_method == 'QuantileTransformer':
            scaler = QuantileTransformer(n_quantiles=1000, random_state=42)
        else:
            scaler = None
        
        if scaler is not None:
            return scaler.fit_transform(X_processed)
        else:
            return X_processed.values
    
    def class_imbalance_techniques(self, X, y, best_preprocessing):
        """ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œæŠ€è¡“"""
        logger.info("âš–ï¸ ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œæŠ€è¡“...")
        
        X_processed = self._apply_preprocessing(X, *best_preprocessing.split('+'))
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
        class_counts = y.value_counts()
        logger.info(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {class_counts.to_dict()}")
        
        imbalance_results = {}
        
        # 1. å„ç¨®class_weightè¨­å®š
        class_weights = [
            'balanced',
            {0: 1, 1: 1.1},
            {0: 1, 1: 1.2},
            {0: 1, 1: 1.3},
            {0: 1, 1: 1.5},
            {0: 1, 1: 2.0}
        ]
        
        for weight in class_weights:
            model = LogisticRegression(C=0.01, class_weight=weight, max_iter=1000, random_state=42)
            
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_processed):
                X_train = X_processed[train_idx]
                X_test = X_processed[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            weight_str = str(weight) if isinstance(weight, dict) else weight
            imbalance_results[f'weight_{weight_str}'] = avg_score
            
            logger.info(f"    class_weight={weight_str}: {avg_score:.1%}")
        
        return imbalance_results
    
    def final_rigorous_validation(self, X, y, best_config):
        """æœ€çµ‚å³å¯†æ¤œè¨¼"""
        logger.info("ğŸ¯ æœ€çµ‚å³å¯†æ¤œè¨¼...")
        
        # æœ€é©è¨­å®šé©ç”¨
        X_processed = self._apply_preprocessing(X, *best_config['preprocessing'].split('+'))
        
        # æœ€é©ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        if best_config['model_type'] == 'LogisticRegression':
            model = LogisticRegression(**best_config['params'], max_iter=2000, random_state=42)
        elif best_config['model_type'] == 'RandomForest':
            model = RandomForestClassifier(**best_config['params'], random_state=42, n_jobs=-1)
        else:
            model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=2000, random_state=42)
        
        # 5åˆ†å‰²å³å¯†æ¤œè¨¼
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        precision_scores = []
        recall_scores = []
        f1_scores = []
        
        logger.info("5åˆ†å‰²æ™‚ç³»åˆ—æ¤œè¨¼å®Ÿè¡Œä¸­...")
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_processed)):
            X_train = X_processed[train_idx]
            X_test = X_processed[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            
            scores.append(accuracy_score(y_test, pred))
            precision_scores.append(precision_score(y_test, pred))
            recall_scores.append(recall_score(y_test, pred))
            f1_scores.append(f1_score(y_test, pred))
            
            logger.info(f"  Fold {fold+1}: Acc={scores[-1]:.1%}, Prec={precision_scores[-1]:.1%}, Rec={recall_scores[-1]:.1%}")
        
        final_metrics = {
            'accuracy': {'mean': np.mean(scores), 'std': np.std(scores)},
            'precision': {'mean': np.mean(precision_scores), 'std': np.std(precision_scores)},
            'recall': {'mean': np.mean(recall_scores), 'std': np.std(recall_scores)},
            'f1': {'mean': np.mean(f1_scores), 'std': np.std(f1_scores)}
        }
        
        logger.info(f"\nğŸ¯ æœ€çµ‚çµæœ:")
        logger.info(f"ç²¾åº¦: {final_metrics['accuracy']['mean']:.1%} Â± {final_metrics['accuracy']['std']:.1%}")
        logger.info(f"é©åˆç‡: {final_metrics['precision']['mean']:.1%} Â± {final_metrics['precision']['std']:.1%}")
        logger.info(f"å†ç¾ç‡: {final_metrics['recall']['mean']:.1%} Â± {final_metrics['recall']['std']:.1%}")
        logger.info(f"F1ã‚¹ã‚³ã‚¢: {final_metrics['f1']['mean']:.1%} Â± {final_metrics['f1']['std']:.1%}")
        
        return final_metrics

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ ç³»çµ±çš„æœ€é©åŒ–åˆ†æ - ãƒ‡ãƒ¼ã‚¿è¿½åŠ ä»¥å¤–ã®æ”¹å–„æ‰‹æ³•")
    logger.info("ğŸ¯ ç›®æ¨™: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ­ã‚¸ãƒƒã‚¯ãƒ»å‰å‡¦ç†ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š")
    
    optimizer = SystematicOptimizer()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df = optimizer.load_full_data()
        if df is None:
            return
        
        X, y, clean_df = optimizer.prepare_data(df)
        
        # 2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
        baseline_score = optimizer.baseline_evaluation(X, y)
        
        # 3. å‰å‡¦ç†æœ€é©åŒ–
        preprocessing_results, best_preprocessing = optimizer.preprocessing_optimization(X, y)
        
        # 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        hyperparameter_results = optimizer.hyperparameter_optimization(X, y, best_preprocessing)
        
        # 5. é«˜åº¦ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•
        advanced_results = optimizer.advanced_modeling_techniques(X, y, best_preprocessing)
        
        # 6. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
        imbalance_results = optimizer.class_imbalance_techniques(X, y, best_preprocessing)
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ ç³»çµ±çš„æœ€é©åŒ–åˆ†æçµæœ")
        logger.info("="*80)
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦: {baseline_score:.1%}")
        
        # å‰å‡¦ç†çµæœ
        logger.info(f"\nğŸ”§ æœ€é©å‰å‡¦ç†: {best_preprocessing} ({preprocessing_results[best_preprocessing]:.1%})")
        logger.info(f"å‰å‡¦ç†ã«ã‚ˆã‚‹æ”¹å–„: {(preprocessing_results[best_preprocessing] - baseline_score)*100:+.1f}%")
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµæœ
        logger.info(f"\nâš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–çµæœ:")
        best_hp_score = 0
        best_hp_model = None
        for model_name, result in hyperparameter_results.items():
            logger.info(f"  {model_name}: {result['score']:.1%}")
            if result['score'] > best_hp_score:
                best_hp_score = result['score']
                best_hp_model = model_name
        logger.info(f"ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ”¹å–„: {(best_hp_score - baseline_score)*100:+.1f}%")
        
        # é«˜åº¦æ‰‹æ³•çµæœ
        logger.info(f"\nğŸ§  é«˜åº¦ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•çµæœ:")
        best_advanced_score = 0
        for method, score in advanced_results.items():
            logger.info(f"  {method}: {score:.1%}")
            best_advanced_score = max(best_advanced_score, score)
        logger.info(f"é«˜åº¦æ‰‹æ³•ã«ã‚ˆã‚‹æ”¹å–„: {(best_advanced_score - baseline_score)*100:+.1f}%")
        
        # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡çµæœ
        logger.info(f"\nâš–ï¸ ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œçµæœ:")
        best_imbalance_score = max(imbalance_results.values())
        best_imbalance_method = max(imbalance_results.keys(), key=lambda k: imbalance_results[k])
        logger.info(f"  æœ€é«˜: {best_imbalance_method}: {best_imbalance_score:.1%}")
        logger.info(f"ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œã«ã‚ˆã‚‹æ”¹å–„: {(best_imbalance_score - baseline_score)*100:+.1f}%")
        
        # å…¨ä½“ã®æœ€é«˜ç²¾åº¦
        all_scores = [
            baseline_score,
            preprocessing_results[best_preprocessing],
            best_hp_score,
            best_advanced_score,
            best_imbalance_score
        ]
        
        max_achieved = max(all_scores)
        improvement = (max_achieved - baseline_score) * 100
        
        logger.info(f"\nğŸ† æœ€é«˜é”æˆç²¾åº¦: {max_achieved:.1%}")
        logger.info(f"ç·æ”¹å–„å¹…: {improvement:+.1f}%")
        
        # æœ€é©æ§‹æˆç‰¹å®š
        best_config = {
            'preprocessing': best_preprocessing,
            'model_type': best_hp_model,
            'params': hyperparameter_results[best_hp_model]['params'] if best_hp_model else {},
            'score': max_achieved
        }
        
        # æœ€çµ‚æ¤œè¨¼
        final_metrics = optimizer.final_rigorous_validation(X, y, best_config)
        
        logger.info(f"\nğŸ“Š æ”¹å–„å¯èƒ½æ€§è©•ä¾¡:")
        if improvement > 0.5:
            logger.info("âœ… æœ‰æ„ãªæ”¹å–„ãŒå¯èƒ½ã§ã™")
        elif improvement > 0.2:
            logger.info("ğŸ”„ é™å®šçš„ãªæ”¹å–„ãŒå¯èƒ½ã§ã™")
        else:
            logger.info("âš ï¸ å¤§å¹…ãªæ”¹å–„ã¯å›°é›£ã§ã™")
        
        logger.info(f"\nâš ï¸ ã“ã®çµæœã¯394,102ä»¶ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼ã§ã™")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()