#!/usr/bin/env python3
"""
é«˜é€Ÿã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…
60%è¶…ãˆã‚’ç›®æŒ‡ã™ç¬¬1æ®µéšï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class QuickEnsembleImprovement:
    """é«˜é€Ÿã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
        # æœ€é©ç‰¹å¾´é‡
        self.optimal_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20',
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
        
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        df = pd.read_parquet(integrated_file)
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[self.optimal_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(clean_df):,}ä»¶, {len(self.optimal_features)}ç‰¹å¾´é‡")
        
        return X, y
    
    def create_optimized_models(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç¾¤ä½œæˆ"""
        logger.info("ğŸ§  æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ç¾¤ä½œæˆ...")
        
        models = {
            'lr_l1': LogisticRegression(
                C=0.001, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'lr_l2': LogisticRegression(
                C=0.001, penalty='l2', solver='lbfgs',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'rf_optimized': RandomForestClassifier(
                n_estimators=50,  # é«˜é€ŸåŒ–ã®ãŸã‚å‰Šæ¸›
                max_depth=8,
                min_samples_split=10,
                class_weight='balanced', 
                random_state=42, 
                n_jobs=-1
            )
        }
        
        logger.info(f"ãƒ¢ãƒ‡ãƒ«æ•°: {len(models)}å€‹")
        return models
    
    def evaluate_individual_models(self, X, y, models):
        """å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        logger.info("ğŸ“Š å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
        
        X_scaled = self.scaler.fit_transform(X)
        tscv = TimeSeriesSplit(n_splits=3)  # é«˜é€ŸåŒ–ã®ãŸã‚3åˆ†å‰²
        results = {}
        
        for name, model in models.items():
            logger.info(f"  {name} è©•ä¾¡ä¸­...")
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            results[name] = {
                'avg': avg_score,
                'std': np.std(scores),
                'scores': scores
            }
            
            logger.info(f"    {name}: {avg_score:.3%} Â± {np.std(scores):.3%}")
        
        return results
    
    def implement_ensemble_methods(self, X, y, models):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…"""
        logger.info("ğŸ”„ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…...")
        
        X_scaled = self.scaler.fit_transform(X)
        tscv = TimeSeriesSplit(n_splits=3)
        
        # æœ€é©ãªãƒ¢ãƒ‡ãƒ«çµ„ã¿åˆã‚ã›
        estimators = [
            ('lr_l2', models['lr_l2']),
            ('rf', models['rf_optimized'])
        ]
        
        ensemble_results = {}
        
        # 1. ãƒãƒ¼ãƒ‰æŠ•ç¥¨
        logger.info("  ãƒãƒ¼ãƒ‰æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«...")
        voting_hard = VotingClassifier(estimators=estimators, voting='hard')
        hard_scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            voting_hard.fit(X_train, y_train)
            pred = voting_hard.predict(X_test)
            hard_scores.append(accuracy_score(y_test, pred))
        
        ensemble_results['voting_hard'] = {
            'avg': np.mean(hard_scores),
            'std': np.std(hard_scores),
            'scores': hard_scores
        }
        
        # 2. ã‚½ãƒ•ãƒˆæŠ•ç¥¨
        logger.info("  ã‚½ãƒ•ãƒˆæŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«...")
        voting_soft = VotingClassifier(estimators=estimators, voting='soft')
        soft_scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            voting_soft.fit(X_train, y_train)
            pred = voting_soft.predict(X_test)
            soft_scores.append(accuracy_score(y_test, pred))
        
        ensemble_results['voting_soft'] = {
            'avg': np.mean(soft_scores),
            'std': np.std(soft_scores),
            'scores': soft_scores
        }
        
        # 3. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
        logger.info("  ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«...")
        meta_learner = LogisticRegression(C=1.0, random_state=42)
        stacking_clf = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_learner,
            cv=2  # é«˜é€ŸåŒ–ã®ãŸã‚2åˆ†å‰²
        )
        
        stacking_scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            stacking_clf.fit(X_train, y_train)
            pred = stacking_clf.predict(X_test)
            stacking_scores.append(accuracy_score(y_test, pred))
        
        ensemble_results['stacking'] = {
            'avg': np.mean(stacking_scores),
            'std': np.std(stacking_scores),
            'scores': stacking_scores
        }
        
        # 4. é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªé‡ã¿ï¼‰
        logger.info("  é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«...")
        weights = [0.7, 0.3]  # LogisticRegressionã«é«˜ã„é‡ã¿
        weighted_scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å–å¾—
            predictions = []
            for name, model in estimators:
                model.fit(X_train, y_train)
                pred_proba = model.predict_proba(X_test)[:, 1]
                predictions.append(pred_proba)
            
            # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            weighted_pred_proba = np.average(predictions, axis=0, weights=weights)
            weighted_pred = (weighted_pred_proba > 0.5).astype(int)
            weighted_scores.append(accuracy_score(y_test, weighted_pred))
        
        ensemble_results['weighted'] = {
            'avg': np.mean(weighted_scores),
            'std': np.std(weighted_scores),
            'scores': weighted_scores,
            'weights': dict(zip(['lr_l2', 'rf_optimized'], weights))
        }
        
        for method, result in ensemble_results.items():
            logger.info(f"    {method}: {result['avg']:.3%} Â± {result['std']:.3%}")
        
        return ensemble_results
    
    def final_validation(self, X, y, best_method, models):
        """æœ€é«˜æ‰‹æ³•ã§ã®æœ€çµ‚æ¤œè¨¼ï¼ˆ5åˆ†å‰²ï¼‰"""
        logger.info("âœ… æœ€é«˜æ‰‹æ³•ã§ã®æœ€çµ‚æ¤œè¨¼ï¼ˆ5åˆ†å‰²ï¼‰...")
        
        X_scaled = self.scaler.fit_transform(X)
        tscv = TimeSeriesSplit(n_splits=5)
        
        if best_method == 'stacking':
            estimators = [
                ('lr_l2', models['lr_l2']),
                ('rf', models['rf_optimized'])
            ]
            meta_learner = LogisticRegression(C=1.0, random_state=42)
            final_model = StackingClassifier(
                estimators=estimators,
                final_estimator=meta_learner,
                cv=2
            )
        elif best_method == 'voting_soft':
            estimators = [
                ('lr_l2', models['lr_l2']),
                ('rf', models['rf_optimized'])
            ]
            final_model = VotingClassifier(estimators=estimators, voting='soft')
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯LogisticRegression L2
            final_model = models['lr_l2']
        
        final_scores = []
        fold_details = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            final_model.fit(X_train, y_train)
            pred = final_model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            final_scores.append(accuracy)
            
            fold_details.append({
                'fold': fold + 1,
                'accuracy': accuracy,
                'train_size': len(X_train),
                'test_size': len(X_test)
            })
            
            logger.info(f"  Fold {fold+1}: {accuracy:.1%} (Train: {len(X_train):,}, Test: {len(X_test):,})")
        
        final_result = {
            'avg': np.mean(final_scores),
            'std': np.std(final_scores),
            'min': np.min(final_scores),
            'max': np.max(final_scores),
            'scores': final_scores,
            'fold_details': fold_details
        }
        
        return final_result

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ é«˜é€Ÿã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ")
    logger.info("ğŸ¯ ç›®æ¨™: 59.4%ã‹ã‚‰62%è¶…ãˆã‚’ç›®æŒ‡ã™")
    
    system = QuickEnsembleImprovement()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X, y = system.load_and_prepare_data()
        
        # 2. æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        models = system.create_optimized_models()
        
        # 3. å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        individual_results = system.evaluate_individual_models(X, y, models)
        
        # 4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•å®Ÿè£…
        ensemble_results = system.implement_ensemble_methods(X, y, models)
        
        # 5. å…¨çµæœã®çµ±åˆã¨æ¯”è¼ƒ
        all_results = {**individual_results, **ensemble_results}
        
        # 6. ãƒ™ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ç‰¹å®š
        best_method = max(all_results.keys(), key=lambda k: all_results[k]['avg'])
        best_score = all_results[best_method]['avg']
        
        # 7. æœ€çµ‚æ¤œè¨¼
        final_result = system.final_validation(X, y, best_method, models)
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*100)
        logger.info("ğŸ† é«˜é€Ÿã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ”¹å–„çµæœ")
        logger.info("="*100)
        
        baseline_score = 59.4  # å‰å›ã®æœ€é«˜ã‚¹ã‚³ã‚¢ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰
        logger.info(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline_score:.1%}")
        
        # å…¨çµæœã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(all_results.items(), key=lambda x: x[1]['avg'], reverse=True)
        
        logger.info(f"\nğŸ“ˆ æ‰‹æ³•åˆ¥çµæœï¼ˆ3åˆ†å‰²æ¤œè¨¼ï¼‰:")
        for i, (method, result) in enumerate(sorted_results, 1):
            improvement = (result['avg'] - baseline_score/100) * 100
            status = "ğŸš€" if improvement > 2.0 else "ğŸ“ˆ" if improvement > 0.5 else "ğŸ“Š"
            logger.info(f"  {i:2d}. {method:20s}: {result['avg']:.3%} ({improvement:+.2f}%) {status}")
        
        # æœ€çµ‚æ¤œè¨¼çµæœ
        logger.info(f"\nğŸ† æœ€çµ‚æ¤œè¨¼çµæœï¼ˆ5åˆ†å‰²ï¼‰:")
        logger.info(f"  æœ€é«˜æ‰‹æ³•: {best_method}")
        logger.info(f"  ç²¾åº¦: {final_result['avg']:.3%} Â± {final_result['std']:.3%}")
        logger.info(f"  ç¯„å›²: {final_result['min']:.1%} - {final_result['max']:.1%}")
        
        final_improvement = (final_result['avg'] - baseline_score/100) * 100
        logger.info(f"  å‘ä¸Š: {final_improvement:+.2f}% (59.4% â†’ {final_result['avg']:.1%})")
        
        # ç›®æ¨™é”æˆç¢ºèª
        target_60 = 0.60
        target_62 = 0.62
        
        if final_result['avg'] >= target_62:
            logger.info(f"ğŸ‰ ç›®æ¨™å¤§å¹…é”æˆï¼ 62%è¶…ãˆ ({final_result['avg']:.1%} >= 62.0%)")
        elif final_result['avg'] >= target_60:
            logger.info(f"âœ… ç›®æ¨™é”æˆï¼ 60%è¶…ãˆ ({final_result['avg']:.1%} >= 60.0%)")
        else:
            logger.info(f"ğŸ“ˆ æ”¹å–„åŠ¹æœç¢ºèª ({final_result['avg']:.1%})")
        
        logger.info(f"\nâš–ï¸ ã“ã®çµæœã¯å…¨ãƒ‡ãƒ¼ã‚¿{len(X):,}ä»¶ã§ã®å³å¯†ãªæ™‚ç³»åˆ—æ¤œè¨¼ã§ã™")
        logger.info(f"âœ… ç¬¬1æ®µéšå®Œäº†: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()