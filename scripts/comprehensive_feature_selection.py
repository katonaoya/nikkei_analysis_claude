#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ  - ã‚ã‚‰ã‚†ã‚‹æ‰‹æ³•ã§æœ€é©ãªç‰¹å¾´é‡ã‚’ç‰¹å®š
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.feature_selection import (
    SelectKBest, f_classif, chi2, mutual_info_classif,
    RFE, RFECV, SelectFromModel
)
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')

class ComprehensiveFeatureSelector:
    """åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ"""
    
    def __init__(self, sample_size=50000):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.sample_size = sample_size
        
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {self.sample_size:,}ï¼‰")
        
        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        df = df.sort_values('Date').tail(self.sample_size)
        logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(df):,}ä»¶")
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'date', 'code',
            'UpperLimit', 'LowerLimit', 'turnover_value', 'adjustment_factor',
            'Return_Direction'
        }
        
        feature_cols = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64', 'int32', 'float32']]
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        X = clean_df[feature_cols].fillna(0)
        y = clean_df['Binary_Direction']
        
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹")
        logger.info(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X):,}ä»¶")
        logger.info(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        return X, y, feature_cols, clean_df
    
    def correlation_analysis(self, X, y):
        """ç›¸é–¢åˆ†æã«ã‚ˆã‚‹ç‰¹å¾´é‡è©•ä¾¡"""
        logger.info("ğŸ“ˆ ç›¸é–¢åˆ†æå®Ÿè¡Œä¸­...")
        
        # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢
        correlations = {}
        for col in X.columns:
            corr = abs(X[col].corr(y))
            correlations[col] = corr
        
        # ç›¸é–¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_corr = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("ä¸Šä½10ç‰¹å¾´é‡ï¼ˆç›¸é–¢ï¼‰:")
        for i, (feature, corr) in enumerate(sorted_corr[:10]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {corr:.4f}")
        
        return dict(sorted_corr)
    
    def statistical_feature_selection(self, X, y):
        """çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠ"""
        logger.info("ğŸ“Š çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠå®Ÿè¡Œä¸­...")
        
        results = {}
        
        # 1. Fçµ±è¨ˆé‡
        try:
            f_selector = SelectKBest(f_classif, k='all')
            f_selector.fit(X, y)
            f_scores = dict(zip(X.columns, f_selector.scores_))
            results['f_statistics'] = sorted(f_scores.items(), key=lambda x: x[1], reverse=True)
            logger.info("âœ… Fçµ±è¨ˆé‡è¨ˆç®—å®Œäº†")
        except Exception as e:
            logger.warning(f"âš ï¸ Fçµ±è¨ˆé‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 2. ç›¸äº’æƒ…å ±é‡
        try:
            mi_selector = SelectKBest(mutual_info_classif, k='all')
            mi_selector.fit(X, y)
            mi_scores = dict(zip(X.columns, mi_selector.scores_))
            results['mutual_info'] = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)
            logger.info("âœ… ç›¸äº’æƒ…å ±é‡è¨ˆç®—å®Œäº†")
        except Exception as e:
            logger.warning(f"âš ï¸ ç›¸äº’æƒ…å ±é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def tree_based_importance(self, X, y):
        """æ¨¹æœ¨ãƒ™ãƒ¼ã‚¹é‡è¦åº¦"""
        logger.info("ğŸŒ³ æ¨¹æœ¨ãƒ™ãƒ¼ã‚¹é‡è¦åº¦è¨ˆç®—ä¸­...")
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=3)
        
        # Random Foresté‡è¦åº¦
        rf_importances = []
        
        for train_idx, test_idx in tscv.split(X):
            X_train = X.iloc[train_idx]
            y_train = y.iloc[train_idx]
            
            rf = RandomForestClassifier(
                n_estimators=100, max_depth=10, 
                class_weight='balanced', random_state=42, n_jobs=-1
            )
            rf.fit(X_train, y_train)
            rf_importances.append(rf.feature_importances_)
        
        # å¹³å‡é‡è¦åº¦
        avg_importance = np.mean(rf_importances, axis=0)
        rf_scores = dict(zip(X.columns, avg_importance))
        sorted_rf = sorted(rf_scores.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("ä¸Šä½10ç‰¹å¾´é‡ï¼ˆRandom Forestï¼‰:")
        for i, (feature, imp) in enumerate(sorted_rf[:10]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {imp:.4f}")
        
        return sorted_rf
    
    def lasso_feature_selection(self, X, y):
        """LASSOæ­£å‰‡åŒ–ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ"""
        logger.info("ğŸ¯ LASSOç‰¹å¾´é‡é¸æŠå®Ÿè¡Œä¸­...")
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # LASSO CV
        lasso_cv = LassoCV(
            alphas=np.logspace(-4, 1, 50),
            cv=TimeSeriesSplit(n_splits=3),
            random_state=42,
            max_iter=2000
        )
        lasso_cv.fit(X_scaled, y)
        
        # ä¿‚æ•°ã®çµ¶å¯¾å€¤ã‚’é‡è¦åº¦ã¨ã™ã‚‹
        lasso_importance = dict(zip(X.columns, abs(lasso_cv.coef_)))
        sorted_lasso = sorted(lasso_importance.items(), key=lambda x: x[1], reverse=True)
        
        # éã‚¼ãƒ­ç‰¹å¾´é‡ã®ã¿
        non_zero_features = [(name, imp) for name, imp in sorted_lasso if imp > 1e-6]
        
        logger.info(f"LASSOé¸æŠç‰¹å¾´é‡æ•°: {len(non_zero_features)}/{len(X.columns)}")
        logger.info("ä¸Šä½10ç‰¹å¾´é‡ï¼ˆLASSOï¼‰:")
        for i, (feature, imp) in enumerate(non_zero_features[:10]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {imp:.4f}")
        
        return non_zero_features
    
    def recursive_feature_elimination(self, X, y):
        """å†å¸°çš„ç‰¹å¾´é‡å‰Šé™¤"""
        logger.info("ğŸ”„ å†å¸°çš„ç‰¹å¾´é‡å‰Šé™¤å®Ÿè¡Œä¸­...")
        
        # ãƒ™ãƒ¼ã‚¹ã‚¨ã‚¹ãƒ†ã‚£ãƒ¡ãƒ¼ã‚¿ãƒ¼
        rf_estimator = RandomForestClassifier(
            n_estimators=50, max_depth=8,
            class_weight='balanced', random_state=42, n_jobs=-1
        )
        
        # RFE with CV
        rfe_cv = RFECV(
            estimator=rf_estimator,
            step=1,
            cv=TimeSeriesSplit(n_splits=3),
            scoring='accuracy',
            min_features_to_select=5
        )
        
        rfe_cv.fit(X, y)
        
        # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡
        selected_features = X.columns[rfe_cv.support_]
        feature_rankings = dict(zip(X.columns, rfe_cv.ranking_))
        
        logger.info(f"RFEé¸æŠç‰¹å¾´é‡æ•°: {len(selected_features)}/{len(X.columns)}")
        logger.info(f"æœ€é©ç‰¹å¾´é‡æ•°: {rfe_cv.n_features_}")
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_rankings = sorted(feature_rankings.items(), key=lambda x: x[1])
        
        return selected_features.tolist(), sorted_rankings
    
    def permutation_importance_analysis(self, X, y):
        """Permutation Importance"""
        logger.info("ğŸ”€ Permutation Importanceè¨ˆç®—ä¸­...")
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ–
        if len(X) > 20000:
            sample_idx = np.random.choice(len(X), 20000, replace=False)
            X_sample = X.iloc[sample_idx]
            y_sample = y.iloc[sample_idx]
        else:
            X_sample = X
            y_sample = y
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=2)
        
        perm_importances = []
        
        for train_idx, test_idx in tscv.split(X_sample):
            X_train = X_sample.iloc[train_idx]
            X_test = X_sample.iloc[test_idx]
            y_train = y_sample.iloc[train_idx]
            y_test = y_sample.iloc[test_idx]
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            rf = RandomForestClassifier(
                n_estimators=50, max_depth=8,
                class_weight='balanced', random_state=42, n_jobs=-1
            )
            rf.fit(X_train, y_train)
            
            # Permutation importance
            perm_imp = permutation_importance(
                rf, X_test, y_test, 
                n_repeats=5, random_state=42, n_jobs=-1
            )
            perm_importances.append(perm_imp.importances_mean)
        
        # å¹³å‡
        avg_perm_imp = np.mean(perm_importances, axis=0)
        perm_scores = dict(zip(X.columns, avg_perm_imp))
        sorted_perm = sorted(perm_scores.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("ä¸Šä½10ç‰¹å¾´é‡ï¼ˆPermutationï¼‰:")
        for i, (feature, imp) in enumerate(sorted_perm[:10]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {imp:.4f}")
        
        return sorted_perm
    
    def ensemble_ranking(self, rankings_dict):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        logger.info("ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦è¨ˆç®—ä¸­...")
        
        # å„æ‰‹æ³•ã®çµæœã‚’æ­£è¦åŒ–ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        ensemble_scores = {}
        
        for method, rankings in rankings_dict.items():
            if rankings:
                # ã‚¹ã‚³ã‚¢ã‚’0-1ã«æ­£è¦åŒ–
                scores = [score for name, score in rankings]
                if len(scores) > 0 and max(scores) > min(scores):
                    min_score, max_score = min(scores), max(scores)
                    for name, score in rankings:
                        normalized_score = (score - min_score) / (max_score - min_score)
                        if name not in ensemble_scores:
                            ensemble_scores[name] = []
                        ensemble_scores[name].append(normalized_score)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢
        final_scores = {}
        for name, scores in ensemble_scores.items():
            final_scores[name] = np.mean(scores)
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        ensemble_ranking = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸Šä½15ç‰¹å¾´é‡:")
        for i, (feature, score) in enumerate(ensemble_ranking[:15]):
            logger.info(f"  {i+1:2d}. {feature:30s}: {score:.4f}")
        
        return ensemble_ranking
    
    def progressive_feature_testing(self, X, y, feature_ranking, max_features=30):
        """æ®µéšçš„ç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆ"""
        logger.info(f"ğŸ“ˆ æ®µéšçš„ç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆï¼ˆæœ€å¤§{max_features}ç‰¹å¾´é‡ï¼‰")
        
        tscv = TimeSeriesSplit(n_splits=3)
        scaler = StandardScaler()
        
        results = []
        
        # ãƒ¢ãƒ‡ãƒ«
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100, max_depth=10,
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'LogisticRegression': LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            )
        }
        
        # ç‰¹å¾´é‡ã‚’æ®µéšçš„ã«è¿½åŠ ã—ã¦ãƒ†ã‚¹ãƒˆ
        for n_features in range(5, min(max_features + 1, len(feature_ranking)), 2):
            selected_features = [name for name, _ in feature_ranking[:n_features]]
            X_selected = X[selected_features]
            
            for model_name, model in models.items():
                fold_scores = []
                
                for train_idx, test_idx in tscv.split(X_selected):
                    X_train, X_test = X_selected.iloc[train_idx], X_selected.iloc[test_idx]
                    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                    
                    # å‰å‡¦ç†
                    if 'Logistic' in model_name:
                        X_train_proc = scaler.fit_transform(X_train)
                        X_test_proc = scaler.transform(X_test)
                    else:
                        X_train_proc = X_train
                        X_test_proc = X_test
                    
                    # å­¦ç¿’ãƒ»äºˆæ¸¬
                    model.fit(X_train_proc, y_train)
                    y_pred = model.predict(X_test_proc)
                    accuracy = accuracy_score(y_test, y_pred)
                    fold_scores.append(accuracy)
                
                avg_accuracy = np.mean(fold_scores)
                std_accuracy = np.std(fold_scores)
                
                results.append({
                    'n_features': n_features,
                    'model': model_name,
                    'accuracy': avg_accuracy,
                    'std': std_accuracy,
                    'features': selected_features
                })
                
                logger.info(f"  {n_features:2d}ç‰¹å¾´é‡ {model_name:18s}: {avg_accuracy:.4f} Â± {std_accuracy:.4f}")
        
        return results
    
    def find_optimal_combination(self, progressive_results):
        """æœ€é©ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ã®ç‰¹å®š"""
        logger.info("ğŸ¯ æœ€é©ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ç‰¹å®šä¸­...")
        
        # æœ€é«˜ç²¾åº¦ã®çµ„ã¿åˆã‚ã›ã‚’ç‰¹å®š
        best_result = max(progressive_results, key=lambda x: x['accuracy'])
        
        logger.info(f"\nğŸ† æœ€é©çµ„ã¿åˆã‚ã›:")
        logger.info(f"   ç‰¹å¾´é‡æ•°: {best_result['n_features']}å€‹")
        logger.info(f"   ãƒ¢ãƒ‡ãƒ«: {best_result['model']}")
        logger.info(f"   ç²¾åº¦: {best_result['accuracy']:.4f} Â± {best_result['std']:.4f}")
        logger.info(f"   é¸æŠç‰¹å¾´é‡:")
        
        for i, feature in enumerate(best_result['features'][:10]):
            logger.info(f"     {i+1:2d}. {feature}")
        
        if len(best_result['features']) > 10:
            logger.info(f"     ... ä»–{len(best_result['features']) - 10}å€‹")
        
        return best_result
    
    def comprehensive_selection(self):
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠé–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data = self.load_and_prepare_data()
        if data is None:
            return None
        
        X, y, feature_cols, clean_df = data
        
        # å„ç¨®æ‰‹æ³•ã§ç‰¹å¾´é‡è©•ä¾¡
        logger.info("\n" + "="*60)
        logger.info("STEP 1: è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")
        logger.info("="*60)
        
        # 1. ç›¸é–¢åˆ†æ
        correlation_ranking = self.correlation_analysis(X, y)
        correlation_list = list(correlation_ranking.items())
        
        # 2. çµ±è¨ˆçš„æ‰‹æ³•
        statistical_results = self.statistical_feature_selection(X, y)
        
        # 3. æ¨¹æœ¨ãƒ™ãƒ¼ã‚¹
        tree_ranking = self.tree_based_importance(X, y)
        
        # 4. LASSO
        lasso_ranking = self.lasso_feature_selection(X, y)
        
        # 5. RFE
        rfe_features, rfe_ranking = self.recursive_feature_elimination(X, y)
        
        # 6. Permutation Importance
        perm_ranking = self.permutation_importance_analysis(X, y)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
        logger.info("\n" + "="*60)
        logger.info("STEP 2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        logger.info("="*60)
        
        rankings_dict = {
            'correlation': correlation_list,
            'f_statistics': statistical_results.get('f_statistics', []),
            'mutual_info': statistical_results.get('mutual_info', []),
            'random_forest': tree_ranking,
            'lasso': lasso_ranking,
            'permutation': perm_ranking
        }
        
        ensemble_ranking = self.ensemble_ranking(rankings_dict)
        
        # æ®µéšçš„ãƒ†ã‚¹ãƒˆ
        logger.info("\n" + "="*60)
        logger.info("STEP 3: æ®µéšçš„ç‰¹å¾´é‡ãƒ†ã‚¹ãƒˆ")
        logger.info("="*60)
        
        progressive_results = self.progressive_feature_testing(X, y, ensemble_ranking)
        
        # æœ€é©çµ„ã¿åˆã‚ã›ç‰¹å®š
        logger.info("\n" + "="*60)
        logger.info("STEP 4: æœ€é©çµ„ã¿åˆã‚ã›ç‰¹å®š")
        logger.info("="*60)
        
        optimal_result = self.find_optimal_combination(progressive_results)
        
        return {
            'optimal_result': optimal_result,
            'ensemble_ranking': ensemble_ranking,
            'progressive_results': progressive_results,
            'individual_rankings': rankings_dict,
            'original_data': (X, y, feature_cols, clean_df)
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        selector = ComprehensiveFeatureSelector(sample_size=50000)
        
        print("ğŸš€ åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print("="*70)
        
        # åŒ…æ‹¬çš„é¸æŠå®Ÿè¡Œ
        results = selector.comprehensive_selection()
        
        if not results:
            print("âŒ ç‰¹å¾´é‡é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸ")
            return 1
        
        # æœ€çµ‚çµæœè¡¨ç¤º
        optimal = results['optimal_result']
        baseline = 0.517
        improvement = optimal['accuracy'] - baseline
        
        print("\n" + "="*70)
        print("ğŸ† åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ æœ€çµ‚çµæœ")
        print("="*70)
        
        print(f"\nğŸ“Š æœ€é©æ§‹æˆ:")
        print(f"   ç‰¹å¾´é‡æ•°: {optimal['n_features']}å€‹")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {optimal['model']}")
        print(f"   ç²¾åº¦: {optimal['accuracy']:.4f} ({optimal['accuracy']:.1%})")
        print(f"   å®‰å®šæ€§: Â±{optimal['std']:.4f}")
        
        print(f"\nğŸ“ˆ æ”¹å–„åŠ¹æœ:")
        print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline:.1%}")
        print(f"   é”æˆç²¾åº¦: {optimal['accuracy']:.1%}")
        print(f"   æ”¹å–„å¹…: {improvement:+.3f} ({improvement:+.1%})")
        
        # ç›®æ¨™é”æˆåˆ¤å®š
        if optimal['accuracy'] >= 0.60:
            print(f"\nğŸ‰ EXCELLENT! 60%é”æˆ!")
            print(f"ğŸš€ è¶…é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ å®Œæˆ")
        elif optimal['accuracy'] >= 0.57:
            print(f"\nğŸ”¥ GREAT! 57%ä»¥ä¸Šé”æˆ")
            print(f"âœ… å®Ÿç”¨é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ ") 
        elif optimal['accuracy'] >= 0.55:
            print(f"\nğŸ‘ GOOD! 55%ä»¥ä¸Šé”æˆ")
            print(f"âœ… å‰å›çµæœã‚’å†ç¾ãƒ»æ”¹å–„")
        elif optimal['accuracy'] >= 0.53:
            print(f"\nğŸ“ˆ ç›®æ¨™53%é”æˆ")
            print(f"âœ… åŸºæœ¬ç›®æ¨™ã‚¯ãƒªã‚¢")
        else:
            print(f"\nğŸ’¡ æ›´ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")
        
        print(f"\nğŸ¯ é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ (ä¸Šä½10å€‹):")
        for i, feature in enumerate(optimal['features'][:10]):
            print(f"   {i+1:2d}. {feature}")
        
        print(f"\nğŸ’° å®Ÿç”¨æ€§è©•ä¾¡:")
        if optimal['accuracy'] >= 0.55:
            print(f"   æœŸå¾…å¹´ç‡: 15-25%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 12-20%")
            print(f"   âœ… é«˜ã„å®Ÿç”¨æ€§")
        elif optimal['accuracy'] >= 0.53:
            print(f"   æœŸå¾…å¹´ç‡: 12-18%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 10-15%")
            print(f"   âœ… å®Ÿç”¨ãƒ¬ãƒ™ãƒ«")
        else:
            print(f"   æœŸå¾…å¹´ç‡: 8-15%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 6-12%")
            print(f"   âš ï¸ è¿½åŠ æœ€é©åŒ–æ¨å¥¨")
        
        print(f"\nğŸ“Š æŠ€è¡“è©³ç´°:")
        print(f"   å…ƒç‰¹å¾´é‡æ•°: {len(results['original_data'][2])}å€‹")
        print(f"   é¸æŠç‰¹å¾´é‡æ•°: {optimal['n_features']}å€‹")
        print(f"   å‰Šæ¸›ç‡: {(1 - optimal['n_features']/len(results['original_data'][2]))*100:.1f}%")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(results['original_data'][0]):,}ä»¶")
        
        return 0 if improvement > 0 else 1
        
    except Exception as e:
        logger.error(f"ç‰¹å¾´é‡é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    exit(main())