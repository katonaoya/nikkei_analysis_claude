#!/usr/bin/env python3
"""
æœ€çµ‚æ¤œè¨¼ - æœ€è‰¯ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ãŸå®Ÿç”¨æ€§èƒ½è©•ä¾¡
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class UltimateValidator:
    """æœ€çµ‚æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
    
    def load_optimized_data(self, filename: str) -> tuple:
        """æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        file_path = self.processed_dir / filename
        df = pd.read_parquet(file_path)
        
        # æœ€æ–°2.5å¹´ï¼ˆã‚ˆã‚Šå®‰å®šã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        df = df.sort_values('Date')
        cutoff_date = df['Date'].max() - pd.DateOffset(days=912)  # ç´„2.5å¹´
        df = df[df['Date'] >= cutoff_date]
        
        # ä¸Šä½ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨ï¼ˆå‰å›žã®çµæžœã‹ã‚‰ï¼‰
        key_features = [
            'CCI', 'RSI_14', 'Stoch_K', 'Williams_R', 'Stoch_D',
            'MACD', 'MACD_Signal', 'BB_Position', 'ROC_10', 'ROC_20',
            'Price_vs_MA10', 'Price_vs_MA20', 'Volatility_20', 'Volume_Ratio',
            'Price_Position', 'Market_Return', 'Market_Volatility', 'Relative_Return'
        ]
        
        available_features = [col for col in key_features if col in df.columns]
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        X = clean_df[available_features].fillna(0)
        y = clean_df['Binary_Direction']
        dates = clean_df['Date']
        
        logger.info(f"Final data: {len(X)} samples, {len(available_features)} features")
        logger.info(f"Period: {dates.min()} to {dates.max()}")
        logger.info(f"Balance: {y.value_counts().to_dict()}")
        
        return X, y, dates, available_features
    
    def final_model_comparison(self, X, y, dates):
        """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        logger.info("ðŸ† Final model comparison...")
        
        models = {
            'Logistic_Optimized': LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'RandomForest_Optimized': RandomForestClassifier(
                n_estimators=300, max_depth=18, min_samples_split=8,
                min_samples_leaf=4, max_features='sqrt',
                class_weight='balanced_subsample', random_state=42, n_jobs=-1
            )
        }
        
        tscv = TimeSeriesSplit(n_splits=4)  # ã‚ˆã‚ŠåŽ³æ ¼ãªæ¤œè¨¼
        scaler = StandardScaler()
        
        results = {}
        all_predictions = {}
        
        for model_name, model in models.items():
            logger.info(f"Testing {model_name}...")
            
            fold_scores = []
            fold_precisions = []
            fold_recalls = []
            fold_predictions = []
            
            for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›žå¸°ã¯æ¨™æº–åŒ–
                if 'Logistic' in model_name:
                    X_train_proc = scaler.fit_transform(X_train)
                    X_test_proc = scaler.transform(X_test)
                else:
                    X_train_proc = X_train
                    X_test_proc = X_test
                
                # è¨“ç·´
                model.fit(X_train_proc, y_train)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test_proc)
                y_proba = model.predict_proba(X_test_proc)[:, 1]
                
                # è©•ä¾¡æŒ‡æ¨™
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                
                fold_scores.append(accuracy)
                fold_precisions.append(precision)
                fold_recalls.append(recall)
                fold_predictions.extend(y_proba)
            
            results[model_name] = {
                'accuracy': np.mean(fold_scores),
                'accuracy_std': np.std(fold_scores),
                'precision': np.mean(fold_precisions),
                'recall': np.mean(fold_recalls),
                'all_scores': fold_scores
            }
            
            all_predictions[model_name] = fold_predictions
            
            logger.info(f"  {model_name}: {np.mean(fold_scores):.3f} Â± {np.std(fold_scores):.3f}")
        
        # ãƒ¡ã‚¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆæœ€è‰¯ã®2ãƒ¢ãƒ‡ãƒ«ï¼‰
        logger.info("Creating meta-ensemble...")
        
        # æ€§èƒ½é‡ã¿ä»˜ã‘
        model_names = list(results.keys())
        accuracies = [results[name]['accuracy'] for name in model_names]
        weights = np.array(accuracies) / np.sum(accuracies)
        
        # äºˆæ¸¬é•·ã•ã‚’æƒãˆã‚‹
        min_length = min(len(all_predictions[name]) for name in model_names)
        y_eval = y.iloc[-min_length:]
        
        ensemble_pred = np.zeros(min_length)
        for i, name in enumerate(model_names):
            ensemble_pred += weights[i] * np.array(all_predictions[name][:min_length])
        
        ensemble_binary = (ensemble_pred >= 0.5).astype(int)
        ensemble_accuracy = accuracy_score(y_eval, ensemble_binary)
        
        results['Meta_Ensemble'] = {
            'accuracy': ensemble_accuracy,
            'weights': dict(zip(model_names, weights))
        }
        
        logger.info(f"Meta-ensemble: {ensemble_accuracy:.3f}")
        
        return results
    
    def production_readiness_check(self, results):
        """ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æº–å‚™åº¦ãƒã‚§ãƒƒã‚¯"""
        logger.info("âœ… Production readiness check...")
        
        best_accuracy = max(
            results['Logistic_Optimized']['accuracy'],
            results['RandomForest_Optimized']['accuracy'],
            results['Meta_Ensemble']['accuracy']
        )
        
        baseline = 0.505
        improvement = best_accuracy - baseline
        
        # å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
        stability_scores = []
        for model_name in ['Logistic_Optimized', 'RandomForest_Optimized']:
            std = results[model_name]['accuracy_std']
            stability_scores.append(std)
        
        avg_stability = np.mean(stability_scores)
        
        readiness = {
            'best_accuracy': best_accuracy,
            'improvement': improvement,
            'stability': avg_stability,
            'status': 'READY' if best_accuracy >= 0.52 and avg_stability < 0.01 else 'NEEDS_WORK'
        }
        
        return readiness

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="Ultimate validation")
    parser.add_argument("--features-file", required=True, help="Features file")
    
    args = parser.parse_args()
    
    try:
        validator = UltimateValidator()
        
        print("ðŸ“Š Loading optimized data...")
        X, y, dates, features = validator.load_optimized_data(args.features_file)
        
        print("ðŸ† Final model comparison...")
        model_results = validator.final_model_comparison(X, y, dates)
        
        print("âœ… Production readiness check...")
        readiness = validator.production_readiness_check(model_results)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        print("\n" + "="*70)
        print("ðŸŽ¯ ULTIMATE VALIDATION RESULTS")
        print("="*70)
        
        print(f"\nðŸ¤– Model Performance:")
        for name, result in model_results.items():
            if name != 'Meta_Ensemble':
                print(f"   {name:25s}: {result['accuracy']:.3f} Â± {result['accuracy_std']:.3f}")
                print(f"   {'':25s}  Precision: {result['precision']:.3f}, Recall: {result['recall']:.3f}")
            else:
                print(f"   {name:25s}: {result['accuracy']:.3f}")
                weights_str = ", ".join([f"{k}={v:.2f}" for k, v in result['weights'].items()])
                print(f"   {'':25s}  Weights: {weights_str}")
        
        print(f"\nðŸŽ¯ FINAL ASSESSMENT:")
        print(f"   ðŸ† Best Accuracy:    {readiness['best_accuracy']:.3f} ({readiness['best_accuracy']:.1%})")
        print(f"   ðŸ“ˆ Total Improvement: +{readiness['improvement']:.3f} (+{readiness['improvement']*100:.1f}%)")
        print(f"   ðŸ“Š Stability:        {readiness['stability']:.4f} (lower is better)")
        print(f"   ðŸš€ Status:           {readiness['status']}")
        
        # ç›®æ¨™é”æˆåˆ¤å®š
        if readiness['best_accuracy'] >= 0.53:
            print(f"\nðŸŽ‰ðŸŽ¯ SUCCESS: TARGET ACHIEVED!")
            print(f"âœ… Accuracy {readiness['best_accuracy']:.1%} exceeds 53% target")
            print(f"ðŸš€ System is ready for production deployment!")
        elif readiness['best_accuracy'] >= 0.525:
            print(f"\nðŸ”¥ EXCELLENT: Very close to 53% target!")
            print(f"âœ… Accuracy {readiness['best_accuracy']:.1%} is practically usable")
            print(f"ðŸ’¡ Consider final tuning or accept current performance")
        elif readiness['best_accuracy'] >= 0.52:
            print(f"\nðŸ‘ GOOD: Significant improvement achieved!")
            print(f"âœ… Accuracy {readiness['best_accuracy']:.1%} is a solid improvement")
            print(f"ðŸ’¡ System is usable, further optimization optional")
        else:
            print(f"\nðŸ“ˆ PROGRESS: Some improvement made")
            print(f"ðŸ’¡ Consider advanced techniques for target achievement")
        
        print(f"\nðŸ“‹ DEVELOPMENT SUMMARY:")
        print(f"   Original Performance:  ~50.0%")
        print(f"   Enhanced Features:     +{(readiness['best_accuracy']-0.50)*100:.1f}%")
        print(f"   ðŸŽ¯ Final Achievement:  {readiness['best_accuracy']:.1%}")
        
        return 0 if readiness['best_accuracy'] >= 0.52 else 1
        
    except Exception as e:
        logger.error(f"Ultimate validation failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())