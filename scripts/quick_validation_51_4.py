#!/usr/bin/env python3
"""
51.4%é”æˆã®å†ç¢ºèªã¨è©³ç´°åˆ†æ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

def main():
    """51.4%é”æˆã®è©³ç´°æ¤œè¨¼"""
    logger.info("ğŸ¯ 51.4%ç²¾åº¦ã®è©³ç´°æ¤œè¨¼")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path("data")
    processed_dir = data_dir / "processed"
    processed_files = list(processed_dir.glob("*.parquet"))
    
    if not processed_files:
        logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
        
    df = pd.read_parquet(processed_files[0])
    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df):,}ä»¶")
    
    # æœ€é©ç‰¹å¾´é‡ï¼ˆç©¶æ¥µã®ãƒ†ã‚¹ãƒˆã§åˆ¤æ˜ï¼‰
    optimal_features = ['Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20']
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    clean_df = df[df['Binary_Direction'].notna()].copy()
    clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
    
    # ç‰¹å¾´é‡å­˜åœ¨ç¢ºèª
    missing = [f for f in optimal_features if f not in clean_df.columns]
    if missing:
        logger.error(f"âŒ ä¸è¶³ç‰¹å¾´é‡: {missing}")
        return
        
    X = clean_df[optimal_features].fillna(0)
    y = clean_df['Binary_Direction'].astype(int)
    
    logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
    logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {optimal_features}")
    
    # æœ€é©æ§‹æˆã§ã®æ¤œè¨¼
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç©¶æ¥µã®ãƒ†ã‚¹ãƒˆã§åˆ¤æ˜ï¼‰
    model = LogisticRegression(
        C=0.001, 
        class_weight='balanced', 
        random_state=42, 
        max_iter=1000,
        solver='lbfgs'
    )
    
    # 5åˆ†å‰²æ™‚ç³»åˆ—è©•ä¾¡
    tscv = TimeSeriesSplit(n_splits=5)
    scores = []
    detailed_results = []
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
        X_train = X_scaled[train_idx]
        X_test = X_scaled[test_idx]
        y_train = y.iloc[train_idx]
        y_test = y.iloc[test_idx]
        
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, pred)
        scores.append(accuracy)
        
        detailed_results.append({
            'fold': fold + 1,
            'accuracy': accuracy,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'class_dist': y_test.value_counts().to_dict()
        })
        
        logger.info(f"  Fold {fold+1}: {accuracy:.1%} (Train: {len(X_train):,}, Test: {len(X_test):,})")
    
    avg_score = np.mean(scores)
    std_score = np.std(scores)
    
    # çµæœè©³ç´°
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ 51.4%ç²¾åº¦æ¤œè¨¼çµæœ")
    logger.info("="*80)
    logger.info(f"å¹³å‡ç²¾åº¦: {avg_score:.3%}")
    logger.info(f"æ¨™æº–åå·®: {std_score:.3%}")
    logger.info(f"ç¯„å›²: {min(scores):.1%} - {max(scores):.1%}")
    
    # å„Foldã®è©³ç´°
    logger.info("\nğŸ“Š Foldåˆ¥è©³ç´°:")
    for result in detailed_results:
        logger.info(f"  Fold {result['fold']}: {result['accuracy']:.1%}")
        logger.info(f"    Train: {result['train_size']:,}, Test: {result['test_size']:,}")
        logger.info(f"    ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {result['class_dist']}")
    
    # ç›®æ¨™é”æˆç¢ºèª
    target = 0.514  # 51.4%
    if avg_score >= target:
        logger.info(f"\nâœ… 51.4%é”æˆç¢ºèªï¼ ({avg_score:.1%} >= {target:.1%})")
    else:
        logger.warning(f"\nâš ï¸ 51.4%æœªé”æˆ ({avg_score:.1%} < {target:.1%})")
        logger.info(f"å·®: {(target - avg_score)*100:.2f}%")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    logger.info("\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦:")
    importances = abs(model.coef_[0])
    feature_importance = list(zip(optimal_features, importances))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    for feature, importance in feature_importance:
        logger.info(f"  {feature:20s}: {importance:.4f}")
    
    logger.info(f"\nâš–ï¸ ã“ã®çµæœã¯å…¨ãƒ‡ãƒ¼ã‚¿{len(clean_df):,}ä»¶ã§ã®å³å¯†ãª5åˆ†å‰²æ™‚ç³»åˆ—æ¤œè¨¼ã§ã™")
    
if __name__ == "__main__":
    main()