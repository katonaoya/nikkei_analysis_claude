#!/usr/bin/env python3
"""
ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã®æ—¥æ¬¡äºˆæ¸¬åˆ†æ
1æ—¥ã‚ãŸã‚Šã®å€™è£œéŠ˜æŸ„æ•°ã¨ç¢ºä¿¡åº¦åˆ†å¸ƒã‚’èª¿æŸ»
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class DailyPredictionAnalysis:
    """æ—¥æ¬¡äºˆæ¸¬å€™è£œæ•°ã®åˆ†æ"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        
        # æœ€é©ç‰¹å¾´é‡
        self.optimal_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20',
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
        
        # å–å¼•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.confidence_threshold = 0.55   # äºˆæ¸¬ç¢ºä¿¡åº¦é–¾å€¤
        
    def load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æº–å‚™"""
        logger.info("ğŸ“Š æ—¥æ¬¡äºˆæ¸¬åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        df = pd.read_parquet(integrated_file)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
        X = clean_df[self.optimal_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(clean_df):,}ä»¶, {len(self.optimal_features)}ç‰¹å¾´é‡")
        
        return clean_df, X, y
    
    def analyze_daily_predictions(self, df, X, y):
        """æ—¥æ¬¡äºˆæ¸¬å€™è£œæ•°ã®åˆ†æ"""
        logger.info("ğŸ” æ—¥æ¬¡äºˆæ¸¬å€™è£œæ•°åˆ†æ...")
        
        # å­¦ç¿’æœŸé–“ã¨ãƒ†ã‚¹ãƒˆæœŸé–“ã®åˆ†å‰²
        dates = sorted(df['Date'].unique())
        split_idx = int(len(dates) * 0.8)
        train_dates = dates[:split_idx]
        test_dates = dates[split_idx:]
        
        logger.info(f"å­¦ç¿’æœŸé–“: {train_dates[0]} - {train_dates[-1]} ({len(train_dates)}æ—¥)")
        logger.info(f"åˆ†ææœŸé–“: {test_dates[0]} - {test_dates[-1]} ({len(test_dates)}æ—¥)")
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        train_mask = df['Date'].isin(train_dates)
        X_train = X[train_mask]
        y_train = y[train_mask]
        
        scaler = StandardScaler()
        model = LogisticRegression(C=0.001, class_weight='balanced', random_state=42, max_iter=1000)
        
        X_train_scaled = scaler.fit_transform(X_train)
        model.fit(X_train_scaled, y_train)
        
        # ãƒ†ã‚¹ãƒˆæœŸé–“ã§ã®æ—¥æ¬¡äºˆæ¸¬åˆ†æ
        daily_stats = []
        
        for i, date in enumerate(test_dates):
            if i % 50 == 0:
                logger.info(f"  åˆ†æé€²è¡Œ: {i+1}/{len(test_dates)} ({date})")
            
            day_data = df[df['Date'] == date]
            if len(day_data) == 0:
                continue
                
            X_day = day_data[self.optimal_features].fillna(0)
            X_day_scaled = scaler.transform(X_day)
            
            # äºˆæ¸¬å®Ÿè¡Œ
            pred_proba = model.predict_proba(X_day_scaled)[:, 1]
            predictions = pred_proba > 0.5
            
            # ç¢ºä¿¡åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            high_confidence_up = pred_proba >= self.confidence_threshold
            high_confidence_down = pred_proba <= (1 - self.confidence_threshold)
            high_confidence_total = high_confidence_up | high_confidence_down
            
            # çµ±è¨ˆè¨ˆç®—
            daily_stat = {
                'date': date,
                'total_stocks': len(day_data),
                'up_predictions': predictions.sum(),
                'down_predictions': (~predictions).sum(),
                'high_conf_up': high_confidence_up.sum(),
                'high_conf_down': high_confidence_down.sum(),
                'high_conf_total': high_confidence_total.sum(),
                'high_conf_ratio': high_confidence_total.sum() / len(day_data) * 100,
                'avg_confidence_up': pred_proba[predictions].mean() if predictions.sum() > 0 else 0,
                'avg_confidence_down': (1 - pred_proba[~predictions]).mean() if (~predictions).sum() > 0 else 0,
                'max_confidence': max(pred_proba.max(), (1 - pred_proba.min())),
                'min_confidence': min(pred_proba.min(), (1 - pred_proba.max())),
                'std_confidence': pred_proba.std()
            }
            
            daily_stats.append(daily_stat)
        
        return pd.DataFrame(daily_stats)
    
    def analyze_prediction_distribution(self, stats_df):
        """äºˆæ¸¬åˆ†å¸ƒã®è©³ç´°åˆ†æ"""
        logger.info("ğŸ“ˆ äºˆæ¸¬åˆ†å¸ƒã®è©³ç´°åˆ†æ...")
        
        # åŸºæœ¬çµ±è¨ˆ
        logger.info("\\n" + "="*100)
        logger.info("ğŸ“Š æ—¥æ¬¡äºˆæ¸¬å€™è£œæ•°ã®çµ±è¨ˆ")
        logger.info("="*100)
        
        # éŠ˜æŸ„æ•°çµ±è¨ˆ
        logger.info(f"\\nğŸ¢ 1æ—¥ã‚ãŸã‚Šã®åˆ†æå¯¾è±¡éŠ˜æŸ„æ•°:")
        logger.info(f"  å¹³å‡: {stats_df['total_stocks'].mean():.1f}éŠ˜æŸ„")
        logger.info(f"  ä¸­å¤®å€¤: {stats_df['total_stocks'].median():.0f}éŠ˜æŸ„")
        logger.info(f"  ç¯„å›²: {stats_df['total_stocks'].min():.0f} - {stats_df['total_stocks'].max():.0f}éŠ˜æŸ„")
        
        # äºˆæ¸¬åˆ†å¸ƒ
        logger.info(f"\\nğŸ“ˆ 1æ—¥ã‚ãŸã‚Šã®äºˆæ¸¬åˆ†å¸ƒ:")
        logger.info(f"  ä¸Šæ˜‡äºˆæ¸¬å¹³å‡: {stats_df['up_predictions'].mean():.1f}éŠ˜æŸ„")
        logger.info(f"  ä¸‹è½äºˆæ¸¬å¹³å‡: {stats_df['down_predictions'].mean():.1f}éŠ˜æŸ„")
        logger.info(f"  ä¸Šæ˜‡äºˆæ¸¬å‰²åˆ: {stats_df['up_predictions'].mean() / stats_df['total_stocks'].mean() * 100:.1f}%")
        
        # é«˜ç¢ºä¿¡åº¦å€™è£œ
        logger.info(f"\\nğŸ¯ 1æ—¥ã‚ãŸã‚Šã®é«˜ç¢ºä¿¡åº¦å€™è£œæ•°ï¼ˆ{self.confidence_threshold*100:.0f}%ä»¥ä¸Šï¼‰:")
        logger.info(f"  é«˜ç¢ºä¿¡åº¦ä¸Šæ˜‡: {stats_df['high_conf_up'].mean():.1f}éŠ˜æŸ„")
        logger.info(f"  é«˜ç¢ºä¿¡åº¦ä¸‹è½: {stats_df['high_conf_down'].mean():.1f}éŠ˜æŸ„")
        logger.info(f"  é«˜ç¢ºä¿¡åº¦åˆè¨ˆ: {stats_df['high_conf_total'].mean():.1f}éŠ˜æŸ„")
        logger.info(f"  é«˜ç¢ºä¿¡åº¦å‰²åˆ: {stats_df['high_conf_ratio'].mean():.1f}%")
        
        # ç¢ºä¿¡åº¦çµ±è¨ˆ
        logger.info(f"\\nğŸ“Š äºˆæ¸¬ç¢ºä¿¡åº¦ã®çµ±è¨ˆ:")
        logger.info(f"  ä¸Šæ˜‡äºˆæ¸¬å¹³å‡ç¢ºä¿¡åº¦: {stats_df['avg_confidence_up'].mean():.1%}")
        logger.info(f"  ä¸‹è½äºˆæ¸¬å¹³å‡ç¢ºä¿¡åº¦: {stats_df['avg_confidence_down'].mean():.1%}")
        logger.info(f"  æœ€é«˜ç¢ºä¿¡åº¦ã®å¹³å‡: {stats_df['max_confidence'].mean():.1%}")
        logger.info(f"  ç¢ºä¿¡åº¦æ¨™æº–åå·®: {stats_df['std_confidence'].mean():.3f}")
        
        # å®Ÿç”¨çš„ãªå€™è£œæ•°
        logger.info(f"\\nğŸ’¼ å®Ÿç”¨çš„ãªå–å¼•å€™è£œæ•°:")
        
        # æ§˜ã€…ãªç¢ºä¿¡åº¦é–¾å€¤ã§ã®å€™è£œæ•°
        confidence_levels = [0.52, 0.55, 0.60, 0.65, 0.70]
        for conf_level in confidence_levels:
            high_conf_count = stats_df.apply(
                lambda row: self.count_high_confidence_stocks(row, conf_level), axis=1
            ).mean()
            logger.info(f"  ç¢ºä¿¡åº¦{conf_level*100:.0f}%ä»¥ä¸Š: {high_conf_count:.1f}éŠ˜æŸ„/æ—¥")
        
        # å–å¼•å¯èƒ½æ—¥ã®åˆ†æ
        tradeable_days = (stats_df['high_conf_total'] > 0).sum()
        logger.info(f"\\nğŸ“… å–å¼•æ©Ÿä¼šã®åˆ†æ:")
        logger.info(f"  å–å¼•å€™è£œãŒã‚ã‚‹æ—¥: {tradeable_days}/{len(stats_df)}æ—¥ ({tradeable_days/len(stats_df)*100:.1f}%)")
        logger.info(f"  å–å¼•æ©Ÿä¼šãªã—ã®æ—¥: {len(stats_df) - tradeable_days}æ—¥")
        
        # æœˆæ¬¡ãƒ»é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        logger.info(f"\\nğŸ—“ï¸ æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ:")
        self.analyze_temporal_patterns(stats_df)
        
        logger.info("="*100)
        
        return stats_df
    
    def count_high_confidence_stocks(self, row, confidence_level):
        """æŒ‡å®šç¢ºä¿¡åº¦ãƒ¬ãƒ™ãƒ«ã§ã®å€™è£œæ•°è¨ˆç®—"""
        total_stocks = row['total_stocks']
        high_conf_ratio = row['high_conf_ratio'] / 100
        
        # ç°¡æ˜“è¨ˆç®—ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šè¤‡é›‘ï¼‰
        estimated_count = total_stocks * high_conf_ratio * (self.confidence_threshold / confidence_level)
        return max(0, estimated_count)
    
    def analyze_temporal_patterns(self, stats_df):
        """æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        stats_df = stats_df.copy()
        stats_df['weekday'] = pd.to_datetime(stats_df['date']).dt.dayofweek
        stats_df['month'] = pd.to_datetime(stats_df['date']).dt.month
        
        # æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘']
        weekday_stats = stats_df.groupby('weekday')['high_conf_total'].mean()
        
        logger.info("  æ›œæ—¥åˆ¥é«˜ç¢ºä¿¡åº¦å€™è£œæ•°:")
        for day_idx, avg_candidates in weekday_stats.items():
            if day_idx < 5:  # å¹³æ—¥ã®ã¿
                logger.info(f"    {weekday_names[day_idx]}æ›œæ—¥: {avg_candidates:.1f}éŠ˜æŸ„")
        
        # æœˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        month_stats = stats_df.groupby('month')['high_conf_total'].mean()
        logger.info("  æœˆåˆ¥é«˜ç¢ºä¿¡åº¦å€™è£œæ•°ï¼ˆä¸Šä½3ãƒ¶æœˆï¼‰:")
        top_months = month_stats.nlargest(3)
        for month, avg_candidates in top_months.items():
            logger.info(f"    {month:2d}æœˆ: {avg_candidates:.1f}éŠ˜æŸ„")
    
    def generate_practical_recommendations(self, stats_df):
        """å®Ÿç”¨çš„ãªæ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        logger.info("\\n" + "="*100)
        logger.info("ğŸ’¡ å®Ÿç”¨çš„ãªé‹ç”¨æ¨å¥¨äº‹é …")
        logger.info("="*100)
        
        avg_high_conf = stats_df['high_conf_total'].mean()
        avg_total_stocks = stats_df['total_stocks'].mean()
        
        logger.info(f"\\nğŸ¯ ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§:")
        logger.info(f"  â€¢ 1æ—¥å¹³å‡{avg_total_stocks:.0f}éŠ˜æŸ„ã‚’åˆ†æ")
        logger.info(f"  â€¢ ãã®ã†ã¡{avg_high_conf:.1f}éŠ˜æŸ„ãŒé«˜ç¢ºä¿¡åº¦å€™è£œ")
        logger.info(f"  â€¢ é¸æŠç‡: {avg_high_conf/avg_total_stocks*100:.1f}%")
        
        logger.info(f"\\nğŸ“‹ é‹ç”¨æˆ¦ç•¥ã®ææ¡ˆ:")
        
        if avg_high_conf >= 20:
            logger.info(f"  ğŸš€ è±Šå¯Œãªå€™è£œ: ä¸Šä½10-15éŠ˜æŸ„ã«çµã£ã¦åˆ†æ•£æŠ•è³‡")
        elif avg_high_conf >= 10:
            logger.info(f"  âœ… é©åˆ‡ãªå€™è£œæ•°: 5-10éŠ˜æŸ„ã§ã®é›†ä¸­æŠ•è³‡")
        elif avg_high_conf >= 5:
            logger.info(f"  âš ï¸ é™å®šçš„å€™è£œ: 2-5éŠ˜æŸ„ã§ã®æ…é‡æŠ•è³‡")
        else:
            logger.info(f"  ğŸ” å€™è£œå°‘æ•°: ç¢ºä¿¡åº¦é–¾å€¤ã‚’ä¸‹ã’ã‚‹æ¤œè¨ãŒå¿…è¦")
        
        logger.info(f"\\nâš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®ææ¡ˆ:")
        
        # ç¢ºä¿¡åº¦é–¾å€¤ã®æ¨å¥¨
        if avg_high_conf < 5:
            logger.info(f"  â€¢ ç¢ºä¿¡åº¦é–¾å€¤ã‚’52-53%ã«ä¸‹ã’ã¦å€™è£œæ•°å¢—åŠ ")
        elif avg_high_conf > 30:
            logger.info(f"  â€¢ ç¢ºä¿¡åº¦é–¾å€¤ã‚’60-65%ã«ä¸Šã’ã¦ç²¾åº¦å‘ä¸Š")
        else:
            logger.info(f"  â€¢ ç¾åœ¨ã®55%é–¾å€¤ã¯é©åˆ‡")
        
        # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚µã‚¤ã‚º
        recommended_positions = min(10, max(3, int(avg_high_conf * 0.5)))
        logger.info(f"  â€¢ æ¨å¥¨åŒæ™‚ä¿æœ‰éŠ˜æŸ„æ•°: {recommended_positions}éŠ˜æŸ„")
        
        # å–å¼•é »åº¦
        trade_frequency = stats_df['high_conf_total'].sum() / len(stats_df)
        logger.info(f"  â€¢ æƒ³å®šæœˆé–“å–å¼•å›æ•°: {trade_frequency * 21:.0f}å›")
        
        logger.info("="*100)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸ” æ—¥æ¬¡äºˆæ¸¬å€™è£œæ•°åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    
    analyzer = DailyPredictionAnalysis()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df, X, y = analyzer.load_and_prepare_data()
        
        # æ—¥æ¬¡äºˆæ¸¬åˆ†æ
        stats_df = analyzer.analyze_daily_predictions(df, X, y)
        
        # è©³ç´°åˆ†æ
        analyzer.analyze_prediction_distribution(stats_df)
        
        # å®Ÿç”¨çš„æ¨å¥¨äº‹é …
        analyzer.generate_practical_recommendations(stats_df)
        
        logger.info("\\nâœ… æ—¥æ¬¡äºˆæ¸¬å€™è£œæ•°åˆ†æå®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()