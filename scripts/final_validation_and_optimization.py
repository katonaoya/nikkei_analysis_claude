#!/usr/bin/env python3
"""
æœ€çµ‚æ¤œè¨¼ã¨æœ€é©åŒ– - é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã§ã®è©³ç´°åˆ†æ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class FinalValidator:
    """æœ€çµ‚æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        
        # æœ€é©ç‰¹å¾´é‡ï¼ˆå‰å›ã®çµæœã‹ã‚‰ï¼‰
        self.optimal_features = [
            'Market_Breadth',
            'Market_Return', 
            'Volatility_20',
            'RSI',
            'Price_vs_MA20'
        ]
    
    def load_and_prepare_final_data(self, sample_size=75000):
        """æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        logger.info(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size:,}ï¼‰")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        df = df.sort_values('Date').tail(sample_size)
        logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(df):,}ä»¶")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # æœ€é©ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨
        X = clean_df[self.optimal_features].fillna(0)
        y = clean_df['Binary_Direction']
        dates = clean_df['Date']
        
        logger.info(f"æœ€é©ç‰¹å¾´é‡æ•°: {len(self.optimal_features)}å€‹")
        logger.info(f"æœ€çµ‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X):,}ä»¶")
        logger.info(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        logger.info(f"æœŸé–“: {dates.min()} ï½ {dates.max()}")
        
        return X, y, dates, clean_df
    
    def comprehensive_model_evaluation(self, X, y):
        """åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        logger.info("ğŸ” åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        # è¤‡æ•°ã®æ¤œè¨¼æ‰‹æ³•
        evaluation_results = {}
        
        # 1. æ™‚ç³»åˆ—åˆ†å‰²ã§ã®æ¤œè¨¼ï¼ˆè¤‡æ•°åˆ†å‰²æ•°ï¼‰
        for n_splits in [3, 5, 7]:
            tscv = TimeSeriesSplit(n_splits=n_splits)
            
            # LogisticRegression
            scaler = StandardScaler()
            lr_scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                lr = LogisticRegression(
                    C=0.01, penalty='l1', solver='liblinear',
                    class_weight='balanced', random_state=42, max_iter=1000
                )
                lr.fit(X_train_scaled, y_train)
                y_pred = lr.predict(X_test_scaled)
                accuracy = accuracy_score(y_test, y_pred)
                lr_scores.append(accuracy)
            
            # RandomForest
            rf_scores = []
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                rf = RandomForestClassifier(
                    n_estimators=150, max_depth=12,
                    class_weight='balanced', random_state=42, n_jobs=-1
                )
                rf.fit(X_train, y_train)
                y_pred = rf.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                rf_scores.append(accuracy)
            
            evaluation_results[f'tscv_{n_splits}'] = {
                'LogisticRegression': {'scores': lr_scores, 'mean': np.mean(lr_scores), 'std': np.std(lr_scores)},
                'RandomForest': {'scores': rf_scores, 'mean': np.mean(rf_scores), 'std': np.std(rf_scores)}
            }
            
            logger.info(f"æ™‚ç³»åˆ—åˆ†å‰² {n_splits}-fold:")
            logger.info(f"  LogisticRegression: {np.mean(lr_scores):.4f} Â± {np.std(lr_scores):.4f}")
            logger.info(f"  RandomForest:       {np.mean(rf_scores):.4f} Â± {np.std(rf_scores):.4f}")
        
        return evaluation_results
    
    def stability_analysis(self, X, y, dates):
        """å®‰å®šæ€§åˆ†æ"""
        logger.info("ğŸ“ˆ å®‰å®šæ€§åˆ†æå®Ÿè¡Œä¸­...")
        
        # æœŸé–“åˆ¥æ€§èƒ½
        unique_dates = sorted(dates.unique())
        date_periods = [
            unique_dates[:len(unique_dates)//3],      # å‰æœŸ
            unique_dates[len(unique_dates)//3:2*len(unique_dates)//3],  # ä¸­æœŸ
            unique_dates[2*len(unique_dates)//3:]     # å¾ŒæœŸ
        ]
        
        period_results = {}
        
        for i, period in enumerate(date_periods):
            period_mask = dates.isin(period)
            X_period = X[period_mask]
            y_period = y[period_mask]
            
            if len(X_period) < 1000:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            # ç°¡å˜ãªåˆ†å‰²è©•ä¾¡
            split_point = int(len(X_period) * 0.7)
            X_train = X_period.iloc[:split_point]
            X_test = X_period.iloc[split_point:]
            y_train = y_period.iloc[:split_point]
            y_test = y_period.iloc[split_point:]
            
            # LogisticRegressionè©•ä¾¡
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            lr = LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            )
            lr.fit(X_train_scaled, y_train)
            y_pred = lr.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            period_name = ['å‰æœŸ', 'ä¸­æœŸ', 'å¾ŒæœŸ'][i]
            period_results[period_name] = {
                'accuracy': accuracy,
                'samples': len(X_period),
                'date_range': f"{period[0]} ï½ {period[-1]}"
            }
            
            logger.info(f"{period_name}({period[0]} ï½ {period[-1]}): {accuracy:.4f} ({len(X_period):,}ä»¶)")
        
        return period_results
    
    def hyperparameter_optimization(self, X, y):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        logger.info("âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè¡Œä¸­...")
        
        tscv = TimeSeriesSplit(n_splits=3)
        
        # LogisticRegression ã® C ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        c_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2]
        best_c = None
        best_lr_score = 0
        
        scaler = StandardScaler()
        
        for c_val in c_values:
            scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                lr = LogisticRegression(
                    C=c_val, penalty='l1', solver='liblinear',
                    class_weight='balanced', random_state=42, max_iter=1000
                )
                lr.fit(X_train_scaled, y_train)
                y_pred = lr.predict(X_test_scaled)
                accuracy = accuracy_score(y_test, y_pred)
                scores.append(accuracy)
            
            avg_score = np.mean(scores)
            logger.info(f"  C={c_val:5.3f}: {avg_score:.4f} Â± {np.std(scores):.4f}")
            
            if avg_score > best_lr_score:
                best_lr_score = avg_score
                best_c = c_val
        
        # RandomForest ã® max_depth æœ€é©åŒ–
        depth_values = [8, 10, 12, 15, 18, 20]
        best_depth = None
        best_rf_score = 0
        
        for depth in depth_values:
            scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                rf = RandomForestClassifier(
                    n_estimators=150, max_depth=depth,
                    class_weight='balanced', random_state=42, n_jobs=-1
                )
                rf.fit(X_train, y_train)
                y_pred = rf.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                scores.append(accuracy)
            
            avg_score = np.mean(scores)
            logger.info(f"  max_depth={depth:2d}: {avg_score:.4f} Â± {np.std(scores):.4f}")
            
            if avg_score > best_rf_score:
                best_rf_score = avg_score
                best_depth = depth
        
        optimization_results = {
            'best_c': best_c,
            'best_c_score': best_lr_score,
            'best_depth': best_depth, 
            'best_depth_score': best_rf_score
        }
        
        logger.info(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        logger.info(f"  LogisticRegression C: {best_c} -> {best_lr_score:.4f}")
        logger.info(f"  RandomForest max_depth: {best_depth} -> {best_rf_score:.4f}")
        
        return optimization_results
    
    def final_performance_test(self, X, y, optimization_results):
        """æœ€çµ‚æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ† æœ€çµ‚æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=5)  # ã‚ˆã‚Šå³å¯†ãªæ¤œè¨¼
        
        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        optimized_lr = LogisticRegression(
            C=optimization_results['best_c'], 
            penalty='l1', solver='liblinear',
            class_weight='balanced', random_state=42, max_iter=1000
        )
        
        optimized_rf = RandomForestClassifier(
            n_estimators=200, max_depth=optimization_results['best_depth'],
            min_samples_split=8, min_samples_leaf=4,
            class_weight='balanced', random_state=42, n_jobs=-1
        )
        
        scaler = StandardScaler()
        
        # è©³ç´°è©•ä¾¡
        lr_scores = []
        rf_scores = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # LogisticRegression
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            optimized_lr.fit(X_train_scaled, y_train)
            y_pred_lr = optimized_lr.predict(X_test_scaled)
            lr_accuracy = accuracy_score(y_test, y_pred_lr)
            lr_scores.append(lr_accuracy)
            
            # RandomForest
            optimized_rf.fit(X_train, y_train)
            y_pred_rf = optimized_rf.predict(X_test)
            rf_accuracy = accuracy_score(y_test, y_pred_rf)
            rf_scores.append(rf_accuracy)
            
            logger.info(f"  Fold {fold+1}: LR={lr_accuracy:.4f}, RF={rf_accuracy:.4f}")
        
        final_results = {
            'LogisticRegression': {
                'mean': np.mean(lr_scores),
                'std': np.std(lr_scores),
                'scores': lr_scores,
                'params': {'C': optimization_results['best_c']}
            },
            'RandomForest': {
                'mean': np.mean(rf_scores),
                'std': np.std(rf_scores), 
                'scores': rf_scores,
                'params': {'max_depth': optimization_results['best_depth']}
            }
        }
        
        return final_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        validator = FinalValidator()
        
        print("ğŸ æœ€çµ‚æ¤œè¨¼ã¨æœ€é©åŒ–é–‹å§‹")
        print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data = validator.load_and_prepare_final_data()
        if data is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™å¤±æ•—")
            return 1
        
        X, y, dates, clean_df = data
        
        # åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        print("\nğŸ“Š åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
        evaluation_results = validator.comprehensive_model_evaluation(X, y)
        
        # å®‰å®šæ€§åˆ†æ
        print("\nğŸ“ˆ å®‰å®šæ€§åˆ†æ...")
        stability_results = validator.stability_analysis(X, y, dates)
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        print("\nâš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–...")
        optimization_results = validator.hyperparameter_optimization(X, y)
        
        # æœ€çµ‚æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        print("\nğŸ† æœ€çµ‚æ€§èƒ½ãƒ†ã‚¹ãƒˆ...")
        final_results = validator.final_performance_test(X, y, optimization_results)
        
        # æœ€çµ‚çµæœè¡¨ç¤º
        print("\n" + "="*70)
        print("ğŸ¯ æœ€çµ‚æ¤œè¨¼çµæœ")
        print("="*70)
        
        best_model = 'LogisticRegression' if final_results['LogisticRegression']['mean'] > final_results['RandomForest']['mean'] else 'RandomForest'
        best_score = final_results[best_model]['mean']
        best_std = final_results[best_model]['std']
        
        baseline = 0.517
        improvement = best_score - baseline
        
        print(f"\nğŸ† æœ€çµ‚æœ€é«˜æ€§èƒ½:")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {best_model}")
        print(f"   ç²¾åº¦: {best_score:.4f} ({best_score:.1%})")
        print(f"   å®‰å®šæ€§: Â±{best_std:.4f}")
        print(f"   ç‰¹å¾´é‡: {len(validator.optimal_features)}å€‹")
        
        print(f"\nğŸ“ˆ æ”¹å–„åŠ¹æœ:")
        print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline:.1%}")
        print(f"   é”æˆç²¾åº¦: {best_score:.1%}")
        print(f"   æ”¹å–„å¹…: {improvement:+.3f} ({improvement:+.1%})")
        
        print(f"\nğŸ¯ ç›®æ¨™é”æˆè©•ä¾¡:")
        if best_score >= 0.60:
            print(f"   ğŸ‰ EXCELLENT! 60%é”æˆ!")
            print(f"   ğŸš€ è¶…é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ å®Œæˆ")
        elif best_score >= 0.57:
            print(f"   ğŸ”¥ GREAT! 57%ä»¥ä¸Šé”æˆ")
            print(f"   âœ… å®Ÿç”¨é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ ")
        elif best_score >= 0.55:
            print(f"   ğŸ‘ GOOD! 55%ä»¥ä¸Šé”æˆ")
            print(f"   âœ… é«˜ã„å®Ÿç”¨æ€§")
        elif best_score >= 0.53:
            print(f"   ğŸ“ˆ ç›®æ¨™53%é”æˆ!")
            print(f"   âœ… åŸºæœ¬ç›®æ¨™ã‚¯ãƒªã‚¢")
        else:
            print(f"   ğŸ’¡ æ›´ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")
        
        print(f"\nğŸ”§ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        for model_name, result in final_results.items():
            print(f"   {model_name}:")
            print(f"     ç²¾åº¦: {result['mean']:.4f} Â± {result['std']:.4f}")
            print(f"     ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {result['params']}")
        
        print(f"\nğŸ“… å®‰å®šæ€§åˆ†æ:")
        for period, result in stability_results.items():
            print(f"   {period}: {result['accuracy']:.4f} ({result['samples']:,}ä»¶)")
            print(f"     æœŸé–“: {result['date_range']}")
        
        print(f"\nğŸ’° åç›Šäºˆæƒ³:")
        if best_score >= 0.55:
            print(f"   æœŸå¾…å¹´ç‡: 15-25%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 12-20%")
        elif best_score >= 0.53:
            print(f"   æœŸå¾…å¹´ç‡: 12-18%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 10-15%")
        else:
            print(f"   æœŸå¾…å¹´ç‡: 8-15%")
            print(f"   ãƒªã‚¹ã‚¯èª¿æ•´å¾Œ: 6-12%")
        
        print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        if best_score >= 0.57:
            print(f"   ã‚·ã‚¹ãƒ†ãƒ ã¯å®Ÿç”¨æº–å‚™å®Œäº†")
            print(f"   ãƒªã‚¹ã‚¯ç®¡ç†ã§ã®é‹ç”¨é–‹å§‹æ¨å¥¨")
        elif best_score >= 0.53:
            print(f"   å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆ")
            print(f"   æ¿æƒ…å ±è¿½åŠ ã§æ›´ãªã‚‹å‘ä¸ŠæœŸå¾…")
        else:
            print(f"   å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¿æƒ…å ±ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰ãŒå¿…è¦")
        
        return 0 if improvement > 0 else 1
        
    except Exception as e:
        logger.error(f"æœ€çµ‚æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    exit(main())