#!/usr/bin/env python3
"""
å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ  - Yahoo Finance API
ç„¡æ–™ã§ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import yfinance as yf
import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class ExternalDataCollector:
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.external_dir = self.data_dir / "external"
        self.external_dir.mkdir(parents=True, exist_ok=True)
        
        # Yahoo Finance ãƒ†ã‚£ãƒƒã‚«ãƒ¼è¨­å®š
        self.tickers = {
            "us_10y": "^TNX",        # ç±³å›½10å¹´å›½å‚µåˆ©å›ã‚Š
            "jp_10y": "^TNX-JP",     # æ—¥æœ¬10å¹´å›½å‚µåˆ©å›ã‚Šï¼ˆè©¦è¡Œç”¨ï¼‰
            "sp500": "^GSPC",        # S&P500æŒ‡æ•°
            "usd_jpy": "JPY=X",      # ãƒ‰ãƒ«å††
            "nikkei": "^N225",       # æ—¥çµŒå¹³å‡ï¼ˆå‚è€ƒç”¨ï¼‰
            "vix": "^VIX"           # VIXææ€–æŒ‡æ•°
        }
        
        # ä»£æ›¿ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆæ—¥æœ¬å›½å‚µç”¨ï¼‰
        self.jp_bond_alternatives = ["^TNX-JP", "JP10Y.JP", "JP10Y:U.S."]
        
    def test_data_availability(self):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ” å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿å–å¾—å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ...")
        
        results = {}
        for name, ticker in self.tickers.items():
            try:
                logger.info(f"  {name} ({ticker}) ãƒ†ã‚¹ãƒˆä¸­...")
                data = yf.download(ticker, period="5d", interval="1d", progress=False)
                
                if not data.empty and len(data) > 0:
                    latest_date = data.index[-1].strftime('%Y-%m-%d')
                    latest_value = data['Close'].iloc[-1]
                    results[name] = {
                        'status': 'OK',
                        'ticker': ticker,
                        'latest_date': latest_date,
                        'latest_value': latest_value,
                        'data_points': len(data)
                    }
                    logger.info(f"    âœ… OK: {latest_date} = {latest_value:.2f}")
                else:
                    results[name] = {'status': 'EMPTY', 'ticker': ticker}
                    logger.warning(f"    âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—")
                    
            except Exception as e:
                results[name] = {'status': 'ERROR', 'ticker': ticker, 'error': str(e)}
                logger.error(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ—¥æœ¬å›½å‚µã®ä»£æ›¿ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒ†ã‚¹ãƒˆ
        if results.get('jp_10y', {}).get('status') != 'OK':
            logger.info("  æ—¥æœ¬å›½å‚µä»£æ›¿ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒ†ã‚¹ãƒˆ...")
            for alt_ticker in self.jp_bond_alternatives:
                try:
                    logger.info(f"    {alt_ticker} ãƒ†ã‚¹ãƒˆä¸­...")
                    data = yf.download(alt_ticker, period="5d", interval="1d", progress=False)
                    if not data.empty and len(data) > 0:
                        latest_date = data.index[-1].strftime('%Y-%m-%d')
                        latest_value = data['Close'].iloc[-1]
                        results['jp_10y'] = {
                            'status': 'OK',
                            'ticker': alt_ticker,
                            'latest_date': latest_date,
                            'latest_value': latest_value,
                            'data_points': len(data)
                        }
                        self.tickers['jp_10y'] = alt_ticker
                        logger.info(f"      âœ… ä»£æ›¿æˆåŠŸ: {alt_ticker}")
                        break
                except:
                    continue
        
        return results
    
    def collect_historical_data(self, start_date="2016-01-01"):
        """éå»ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬åé›†"""
        logger.info(f"ğŸ“Š éå»ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ (from {start_date})...")
        
        all_external_data = {}
        successful_tickers = {}
        
        for name, ticker in self.tickers.items():
            try:
                logger.info(f"  {name} ({ticker}) åé›†ä¸­...")
                
                # é•·æœŸãƒ‡ãƒ¼ã‚¿å–å¾—
                data = yf.download(ticker, start=start_date, end=None, interval="1d", progress=False)
                
                if not data.empty and len(data) > 0:
                    # åŸºæœ¬çš„ãªå‰å‡¦ç†
                    processed_data = data.copy()
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’Dateã‚«ãƒ©ãƒ ã«å¤‰æ›
                    processed_data.reset_index(inplace=True)
                    processed_data['Date'] = pd.to_datetime(processed_data['Date'])
                    
                    # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€
                    if 'Close' in processed_data.columns:
                        processed_data[f'{name}_value'] = processed_data['Close']
                        processed_data[f'{name}_change'] = processed_data['Close'].pct_change()
                        processed_data[f'{name}_volatility'] = processed_data['Close'].rolling(20).std()
                    
                    # å¿…è¦ã‚«ãƒ©ãƒ ã®ã¿ä¿æŒ
                    keep_cols = ['Date', f'{name}_value', f'{name}_change', f'{name}_volatility']
                    processed_data = processed_data[keep_cols].copy()
                    
                    all_external_data[name] = processed_data
                    successful_tickers[name] = ticker
                    
                    logger.info(f"    âœ… {name}: {len(processed_data):,}ä»¶ ({processed_data['Date'].min().date()} - {processed_data['Date'].max().date()})")
                else:
                    logger.warning(f"    âš ï¸ {name}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    
            except Exception as e:
                logger.error(f"    âŒ {name}: ã‚¨ãƒ©ãƒ¼ - {e}")
        
        return all_external_data, successful_tickers
    
    def merge_external_data(self, external_data_dict):
        """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ"""
        logger.info("ğŸ”— å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ...")
        
        if not external_data_dict:
            logger.error("âŒ çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«é †æ¬¡çµåˆ
        merged_data = None
        for name, data in external_data_dict.items():
            if merged_data is None:
                merged_data = data.copy()
                logger.info(f"  ãƒ™ãƒ¼ã‚¹: {name} ({len(data):,}ä»¶)")
            else:
                merged_data = merged_data.merge(data, on='Date', how='outer')
                logger.info(f"  çµåˆ: {name} â†’ åˆè¨ˆ {len(merged_data):,}ä»¶")
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        merged_data = merged_data.sort_values('Date').reset_index(drop=True)
        
        # æ¬ æå€¤ã®å‰åŸ‹ã‚ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¯é€±æœ«ç­‰ã§æ¬ æãŒå¤šã„ãŸã‚ï¼‰
        numeric_cols = merged_data.select_dtypes(include=[np.number]).columns
        merged_data[numeric_cols] = merged_data[numeric_cols].fillna(method='ffill')
        
        logger.info(f"âœ… å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(merged_data):,}ä»¶")
        logger.info(f"æœŸé–“: {merged_data['Date'].min().date()} - {merged_data['Date'].max().date()}")
        
        return merged_data
    
    def save_external_data(self, merged_data, filename="external_macro_data.parquet"):
        """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        if merged_data is None:
            return
            
        filepath = self.external_dir / filename
        merged_data.to_parquet(filepath, index=False)
        logger.info(f"ğŸ’¾ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {filepath}")
        logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {filepath.stat().st_size / 1024:.1f} KB")
        
        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        logger.info(f"\nğŸ“‹ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
        logger.info(f"ã‚«ãƒ©ãƒ æ•°: {len(merged_data.columns)}")
        logger.info(f"ã‚«ãƒ©ãƒ : {list(merged_data.columns)}")
        logger.info(f"\næœ€æ–°5æ—¥åˆ†:")
        print(merged_data.tail().to_string(index=False))
    
    def integrate_with_existing_data(self):
        """æ—¢å­˜ã®J-Quantsãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ"""
        logger.info("ğŸ”„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ...")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        processed_dir = self.data_dir / "processed"
        existing_files = list(processed_dir.glob("*.parquet"))
        
        if not existing_files:
            logger.error("âŒ æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        existing_data = pd.read_parquet(existing_files[0])
        existing_data['Date'] = pd.to_datetime(existing_data['Date'])
        logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data):,}ä»¶")
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        external_file = self.external_dir / "external_macro_data.parquet"
        if not external_file.exists():
            logger.error("âŒ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        external_data = pd.read_parquet(external_file)
        external_data['Date'] = pd.to_datetime(external_data['Date'])
        logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_data):,}ä»¶")
        
        # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã§çµ±åˆ
        integrated_data = existing_data.merge(external_data, on='Date', how='left')
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®å‰åŸ‹ã‚ï¼ˆæ ªå¼å–å¼•æ—¥ã¨å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾—æ—¥ã®ã‚ºãƒ¬ã‚’èª¿æ•´ï¼‰
        external_cols = [col for col in integrated_data.columns if any(prefix in col for prefix in ['us_10y', 'jp_10y', 'sp500', 'usd_jpy', 'nikkei', 'vix'])]
        integrated_data[external_cols] = integrated_data[external_cols].fillna(method='ffill')
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_data):,}ä»¶")
        logger.info(f"æ–°è¦ã‚«ãƒ©ãƒ æ•°: {len(external_cols)}")
        logger.info(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(integrated_data.columns)}")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
        integrated_file = processed_dir / "integrated_with_external.parquet"
        integrated_data.to_parquet(integrated_file, index=False)
        logger.info(f"ğŸ’¾ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {integrated_file}")
        
        return integrated_data

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    logger.info("ğŸ¯ ç›®æ¨™: Yahoo Finance APIã§ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã‚’ç„¡æ–™å–å¾—")
    
    collector = ExternalDataCollector()
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ
        test_results = collector.test_data_availability()
        
        # 2. æˆåŠŸã—ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ã§éå»ãƒ‡ãƒ¼ã‚¿åé›†
        external_data_dict, successful_tickers = collector.collect_historical_data()
        
        if not external_data_dict:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # 3. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        merged_external = collector.merge_external_data(external_data_dict)
        
        # 4. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        collector.save_external_data(merged_external)
        
        # 5. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
        integrated_data = collector.integrate_with_existing_data()
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
        logger.info("="*80)
        
        logger.info(f"âœ… å–å¾—æˆåŠŸãƒ†ã‚£ãƒƒã‚«ãƒ¼: {len(successful_tickers)}")
        for name, ticker in successful_tickers.items():
            logger.info(f"  {name}: {ticker}")
        
        if integrated_data is not None:
            logger.info(f"\nğŸ“Š æœ€çµ‚çµ±åˆãƒ‡ãƒ¼ã‚¿:")
            logger.info(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(integrated_data):,}ä»¶")
            logger.info(f"  ã‚«ãƒ©ãƒ æ•°: {len(integrated_data.columns)}")
            logger.info(f"  æœŸé–“: {integrated_data['Date'].min().date()} - {integrated_data['Date'].max().date()}")
            
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã®çµ±è¨ˆ
            external_cols = [col for col in integrated_data.columns if any(prefix in col for prefix in ['us_10y', 'jp_10y', 'sp500', 'usd_jpy', 'nikkei', 'vix'])]
            logger.info(f"  å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ : {len(external_cols)}å€‹")
            
            # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            missing_stats = integrated_data[external_cols].isnull().sum()
            if missing_stats.sum() > 0:
                logger.info(f"  æ¬ æå€¤ã‚ã‚Š:")
                for col, missing_count in missing_stats[missing_stats > 0].items():
                    logger.info(f"    {col}: {missing_count:,}ä»¶ ({missing_count/len(integrated_data)*100:.1f}%)")
            else:
                logger.info(f"  âœ… æ¬ æå€¤ãªã—")
        
        logger.info(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ–°ã—ã„å¤–éƒ¨ç‰¹å¾´é‡ã§ç²¾åº¦è©•ä¾¡ã‚’å®Ÿè¡Œ")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()