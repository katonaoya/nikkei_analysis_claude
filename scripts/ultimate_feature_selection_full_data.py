#!/usr/bin/env python3
"""
ç©¶æ¥µã®ç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ394,102ä»¶ï¼‰ç‰ˆ
ã‚ã‚‰ã‚†ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œè¨¼ã—ã¦æœ€é«˜ç²¾åº¦é”æˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
import itertools
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from sklearn.feature_selection import (
    SelectKBest, f_classif, mutual_info_classif, RFE, RFECV,
    SelectFromModel, VarianceThreshold
)
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class UltimateFeatureSelector:
    """ç©¶æ¥µã®ç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.results = {}
        
        # è¤‡æ•°ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
        self.scalers = {
            'standard': StandardScaler(),
            'minmax': MinMaxScaler(),
            'robust': RobustScaler()
        }
        
        # è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«
        self.models = {
            'LogisticRegression': LogisticRegression(max_iter=2000, random_state=42),
            'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42),
            'RidgeClassifier': RidgeClassifier(random_state=42)
        }
        
    def load_full_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ394,102ä»¶ï¼‰")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ç¢ºèª
        df['Date'] = pd.to_datetime(df['Date'])
        min_date = df['Date'].min()
        max_date = df['Date'].max()
        years = (max_date - min_date).days / 365.25
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {min_date.date()} ~ {max_date.date()} ({years:.1f}å¹´é–“)")
        logger.info("âš ï¸ å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
        
        return df
    
    def create_comprehensive_features(self, df):
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ”§ åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆä¸­...")
        logger.info("1/6: åŸºæœ¬ç‰¹å¾´é‡ç¢ºèª...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # 2. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã®æ‹¡å¼µ
        logger.info("2/6: æ‹¡å¼µãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ä½œæˆ...")
        
        # ã‚ˆã‚Šå¤šãã®æœŸé–“ã§ã®ç§»å‹•å¹³å‡
        for period in [5, 10, 25, 50, 75, 100]:
            df[f'MA_{period}'] = df.groupby('Code')['Close'].rolling(period, min_periods=1).mean().reset_index(0, drop=True)
            df[f'Price_vs_MA{period}'] = (df['Close'] - df[f'MA_{period}']) / (df[f'MA_{period}'] + 1e-6)
        
        # EMAã®è¿½åŠ 
        for period in [12, 26, 50]:
            df[f'EMA_{period}'] = df.groupby('Code')['Close'].ewm(span=period).mean().reset_index(0, drop=True)
            df[f'Price_vs_EMA{period}'] = (df['Close'] - df[f'EMA_{period}']) / (df[f'EMA_{period}'] + 1e-6)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ã®æ‹¡å¼µ
        for period in [5, 10, 20, 30, 60]:
            df[f'Volatility_{period}'] = df.groupby('Code')['Close'].rolling(period, min_periods=1).std().reset_index(0, drop=True)
            df[f'VolatilityRank_{period}'] = df.groupby('Date')[f'Volatility_{period}'].rank(pct=True)
        
        # RSIã®è¤‡æ•°æœŸé–“
        for period in [7, 14, 21, 28]:
            df[f'RSI_{period}'] = self._calculate_rsi(df, period)
        
        # MACDç³»æŒ‡æ¨™
        df['MACD_12_26'] = df.groupby('Code').apply(lambda x: self._calculate_macd(x, 12, 26)).reset_index(0, drop=True)
        df['MACD_Signal'] = df.groupby('Code')['MACD_12_26'].ewm(span=9).mean().reset_index(0, drop=True)
        df['MACD_Histogram'] = df['MACD_12_26'] - df['MACD_Signal']
        
        # 3. å¸‚å ´æ§‹é€ æŒ‡æ¨™ã®è©³ç´°åŒ–
        logger.info("3/6: è©³ç´°å¸‚å ´æ§‹é€ æŒ‡æ¨™ä½œæˆ...")
        
        # æ—¥æ¬¡å¸‚å ´çµ±è¨ˆã®è©³ç´°åŒ–
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std', 'min', 'max', 'median'],
            'Volume': ['mean', 'std', 'min', 'max', 'median'],
            'Returns': ['mean', 'std', 'skew', 'min', 'max'],
            'High': 'mean',
            'Low': 'mean'
        })
        
        daily_market.columns = [f'Market_{stat}_{col}' for col, stat in daily_market.columns]
        daily_market = daily_market.reset_index()
        
        # å¸‚å ´å¹…æŒ‡æ¨™ã®è©³ç´°åŒ–
        daily_breadth = df.groupby('Date').agg({
            'Returns': lambda x: (x > 0).sum() / len(x),  # ä¸Šæ˜‡éŠ˜æŸ„æ¯”ç‡
            'Close': lambda x: len(x)  # å–å¼•éŠ˜æŸ„æ•°
        })
        daily_breadth.columns = ['Market_Breadth_Ratio', 'Market_Stock_Count']
        daily_breadth = daily_breadth.reset_index()
        
        # 4. ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã®é«˜åº¦åŒ–
        logger.info("4/6: é«˜åº¦ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æä½œæˆ...")
        
        df['Sector_Code'] = df['Code'].astype(str).str[:2]
        
        # ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥çµ±è¨ˆ
        sector_stats = df.groupby(['Date', 'Sector_Code']).agg({
            'Close': ['mean', 'std', 'count'],
            'Volume': 'mean',
            'Returns': ['mean', 'std']
        })
        sector_stats.columns = [f'Sector_{stat}_{col}' for col, stat in sector_stats.columns]
        sector_stats = sector_stats.reset_index()
        
        # ã‚»ã‚¯ã‚¿ãƒ¼ç›¸å¯¾ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        sector_performance = df.groupby(['Date', 'Sector_Code'])['Returns'].mean().reset_index()
        sector_performance.columns = ['Date', 'Sector_Code', 'Sector_Return']
        
        # 5. å€‹åˆ¥éŠ˜æŸ„æŒ‡æ¨™
        logger.info("5/6: å€‹åˆ¥éŠ˜æŸ„è©³ç´°æŒ‡æ¨™ä½œæˆ...")
        
        # ä¾¡æ ¼ãƒ¬ãƒ³ã‚¸æŒ‡æ¨™
        df['Price_Range'] = (df['High'] - df['Low']) / (df['Close'] + 1e-6)
        df['Upper_Shadow'] = (df['High'] - np.maximum(df['Open'], df['Close'])) / (df['Close'] + 1e-6)
        df['Lower_Shadow'] = (np.minimum(df['Open'], df['Close']) - df['Low']) / (df['Close'] + 1e-6)
        
        # å‡ºæ¥é«˜æŒ‡æ¨™ã®è©³ç´°åŒ–
        for period in [5, 10, 20]:
            df[f'Volume_MA_{period}'] = df.groupby('Code')['Volume'].rolling(period, min_periods=1).mean().reset_index(0, drop=True)
            df[f'Volume_Ratio_{period}'] = df['Volume'] / (df[f'Volume_MA_{period}'] + 1e-6)
        
        # ä¾¡æ ¼å‹¢ã„æŒ‡æ¨™
        for period in [3, 5, 10, 20]:
            df[f'Price_Momentum_{period}'] = df.groupby('Code')['Close'].pct_change(period)
            df[f'Return_Momentum_{period}'] = df.groupby('Code')['Returns'].rolling(period, min_periods=1).sum().reset_index(0, drop=True)
        
        # 6. çµ±åˆã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        logger.info("6/6: ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°...")
        
        # å„ç¨®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸
        df = df.merge(daily_market, on='Date', how='left')
        df = df.merge(daily_breadth, on='Date', how='left')
        df = df.merge(sector_stats, on=['Date', 'Sector_Code'], how='left')
        df = df.merge(sector_performance, on=['Date', 'Sector_Code'], how='left')
        
        # å¸‚å ´ç›¸å¯¾æŒ‡æ¨™ã®è¨ˆç®—
        df['Market_Relative_Return'] = df['Returns'] - df['Market_mean_Returns']
        df['Market_Relative_Volume'] = df['Volume'] / (df['Market_mean_Volume'] + 1e-6)
        df['Sector_Relative_Return'] = df['Returns'] - df['Sector_Return']
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        
        logger.info(f"âœ… åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        return df
    
    def _calculate_rsi(self, df, period):
        """RSIè¨ˆç®—"""
        def rsi_calc(group):
            delta = group['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + 1e-6)
            return 100 - (100 / (1 + rs))
        
        return df.groupby('Code', group_keys=False).apply(rsi_calc).reset_index(0, drop=True)
    
    def _calculate_macd(self, group, fast=12, slow=26):
        """MACDè¨ˆç®—"""
        ema_fast = group['Close'].ewm(span=fast).mean()
        ema_slow = group['Close'].ewm(span=slow).mean()
        return ema_fast - ema_slow
    
    def categorize_features(self, df):
        """ç‰¹å¾´é‡ã®è©³ç´°åˆ†é¡"""
        logger.info("ğŸ“Š ç‰¹å¾´é‡åˆ†é¡ä¸­...")
        
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector_Code'
        }
        
        all_features = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        # è©³ç´°åˆ†é¡
        feature_categories = {
            'basic': [col for col in all_features if col in ['Returns', 'Volume_Change']],
            'technical_ma': [col for col in all_features if 'MA_' in col or 'Price_vs_MA' in col or 'EMA' in col or 'Price_vs_EMA' in col],
            'technical_volatility': [col for col in all_features if 'Volatility' in col or 'VolatilityRank' in col],
            'technical_momentum': [col for col in all_features if 'RSI' in col or 'MACD' in col or 'Momentum' in col],
            'technical_volume': [col for col in all_features if 'Volume_' in col and col != 'Volume_Change'],
            'technical_price': [col for col in all_features if any(x in col for x in ['Range', 'Shadow', 'Upper', 'Lower'])],
            'market': [col for col in all_features if col.startswith('Market_') and 'Relative' not in col],
            'sector': [col for col in all_features if col.startswith('Sector_')],
            'relative': [col for col in all_features if 'Relative' in col],
            'breadth': [col for col in all_features if 'Breadth' in col]
        }
        
        # åˆ†é¡çµæœã®è¡¨ç¤º
        for category, features in feature_categories.items():
            logger.info(f"{category:20s}: {len(features):3d}å€‹")
        
        logger.info(f"å…¨ç‰¹å¾´é‡ç·æ•°: {len(all_features)}å€‹")
        
        return feature_categories, all_features
    
    def comprehensive_feature_selection(self, df, feature_categories, all_features):
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ"""
        logger.info("ğŸ” åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠé–‹å§‹...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        
        X = clean_df[all_features]
        y = clean_df['Binary_Direction'].astype(int)
        
        selection_results = {}
        
        # 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥å˜ç‹¬è©•ä¾¡
        logger.info("1/8: ã‚«ãƒ†ã‚´ãƒªåˆ¥å˜ç‹¬è©•ä¾¡...")
        category_scores = self._evaluate_feature_categories(X, y, feature_categories)
        selection_results['category_scores'] = category_scores
        
        # 2. çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠ
        logger.info("2/8: çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠ...")
        statistical_rankings = self._statistical_feature_selection(X, y)
        selection_results['statistical'] = statistical_rankings
        
        # 3. ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡é¸æŠ
        logger.info("3/8: ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡é¸æŠ...")
        model_rankings = self._model_based_feature_selection(X, y)
        selection_results['model_based'] = model_rankings
        
        # 4. å†å¸°çš„ç‰¹å¾´é‡é™¤å»
        logger.info("4/8: å†å¸°çš„ç‰¹å¾´é‡é™¤å»...")
        rfe_features = self._recursive_feature_elimination(X, y)
        selection_results['rfe'] = rfe_features
        
        # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦
        logger.info("5/8: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦è¨ˆç®—...")
        ensemble_ranking = self._create_ensemble_ranking(selection_results)
        
        # 6. æ®µéšçš„æœ€é©åŒ–
        logger.info("6/8: æ®µéšçš„æœ€é©åŒ–...")
        progressive_results = self._progressive_optimization(X, y, ensemble_ranking)
        selection_results['progressive'] = progressive_results
        
        # 7. ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›æœ€é©åŒ–
        logger.info("7/8: ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›æœ€é©åŒ–...")
        combination_results = self._category_combination_optimization(X, y, feature_categories)
        selection_results['combination'] = combination_results
        
        # 8. æœ€çµ‚æœ€é©åŒ–
        logger.info("8/8: æœ€çµ‚æœ€é©åŒ–...")
        final_results = self._final_optimization(X, y, selection_results)
        
        return selection_results, final_results
    
    def _evaluate_feature_categories(self, X, y, feature_categories):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥è©•ä¾¡"""
        category_scores = {}
        
        for category, features in feature_categories.items():
            if not features:
                continue
                
            logger.info(f"  {category} ({len(features)}ç‰¹å¾´é‡) è©•ä¾¡ä¸­...")
            
            X_cat = X[features]
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_cat)
            
            # TimeSeriesSplitè©•ä¾¡
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42)
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            category_scores[category] = {
                'score': avg_score,
                'features': features,
                'feature_count': len(features)
            }
            
            logger.info(f"    {category:20s}: {avg_score:.1%}")
        
        return category_scores
    
    def _statistical_feature_selection(self, X, y):
        """çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠ"""
        rankings = {}
        
        # Fçµ±è¨ˆé‡
        f_scores = f_classif(X, y)[0]
        rankings['f_statistic'] = list(zip(X.columns, f_scores))
        
        # ç›¸äº’æƒ…å ±é‡
        mi_scores = mutual_info_classif(X, y, random_state=42)
        rankings['mutual_info'] = list(zip(X.columns, mi_scores))
        
        # åˆ†æ•£ã«ã‚ˆã‚‹é¸æŠ
        var_threshold = VarianceThreshold(threshold=0.01)
        var_threshold.fit(X)
        high_var_features = X.columns[var_threshold.get_support()]
        rankings['high_variance'] = [(f, 1.0) for f in high_var_features]
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚½ãƒ¼ãƒˆ
        for method in rankings:
            rankings[method] = sorted(rankings[method], key=lambda x: x[1], reverse=True)
        
        return rankings
    
    def _model_based_feature_selection(self, X, y):
        """ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡é¸æŠ"""
        rankings = {}
        
        # RandomForesté‡è¦åº¦
        rf = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)
        rf.fit(X, y)
        rankings['random_forest'] = list(zip(X.columns, rf.feature_importances_))
        
        # GradientBoostingé‡è¦åº¦
        gb = GradientBoostingClassifier(n_estimators=50, max_depth=6, random_state=42)
        gb.fit(X, y)
        rankings['gradient_boosting'] = list(zip(X.columns, gb.feature_importances_))
        
        # L1æ­£å‰‡åŒ–
        lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.01, random_state=42)
        lasso.fit(X, y)
        lasso_importance = np.abs(lasso.coef_[0])
        rankings['lasso'] = list(zip(X.columns, lasso_importance))
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚½ãƒ¼ãƒˆ
        for method in rankings:
            rankings[method] = sorted(rankings[method], key=lambda x: x[1], reverse=True)
        
        return rankings
    
    def _recursive_feature_elimination(self, X, y):
        """å†å¸°çš„ç‰¹å¾´é‡é™¤å»"""
        logger.info("    RFEå®Ÿè¡Œä¸­...")
        
        # è¨ˆç®—æ™‚é–“çŸ­ç¸®ã®ãŸã‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(X) > 50000:
            sample_idx = np.random.choice(len(X), 50000, replace=False)
            X_sample = X.iloc[sample_idx]
            y_sample = y.iloc[sample_idx]
        else:
            X_sample = X
            y_sample = y
        
        # RFEå®Ÿè¡Œ
        estimator = LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42)
        rfe = RFE(estimator, n_features_to_select=min(20, len(X.columns)//2))
        rfe.fit(X_sample, y_sample)
        
        selected_features = X.columns[rfe.support_]
        feature_rankings = list(zip(X.columns, rfe.ranking_))
        
        return {
            'selected': selected_features.tolist(),
            'rankings': sorted(feature_rankings, key=lambda x: x[1])
        }
    
    def _create_ensemble_ranking(self, selection_results):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        ensemble_scores = {}
        
        # å„æ‰‹æ³•ã®çµæœã‚’çµ±åˆ
        all_rankings = {}
        
        # çµ±è¨ˆçš„æ‰‹æ³•
        if 'statistical' in selection_results:
            for method, rankings in selection_results['statistical'].items():
                all_rankings[f'stat_{method}'] = rankings
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹æ‰‹æ³•
        if 'model_based' in selection_results:
            for method, rankings in selection_results['model_based'].items():
                all_rankings[f'model_{method}'] = rankings
        
        # RFE
        if 'rfe' in selection_results and 'rankings' in selection_results['rfe']:
            # RFEã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒä½ã„ã»ã©è‰¯ã„ã®ã§é€†è»¢
            rfe_rankings = [(name, 1.0/rank) for name, rank in selection_results['rfe']['rankings']]
            all_rankings['rfe'] = rfe_rankings
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—
        for method, rankings in all_rankings.items():
            if rankings:
                scores = [score for name, score in rankings]
                if len(scores) > 0 and max(scores) > min(scores):
                    min_score, max_score = min(scores), max(scores)
                    for name, score in rankings:
                        normalized_score = (score - min_score) / (max_score - min_score)
                        if name not in ensemble_scores:
                            ensemble_scores[name] = []
                        ensemble_scores[name].append(normalized_score)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        final_scores = {}
        for name, scores in ensemble_scores.items():
            final_scores[name] = np.mean(scores)
        
        ensemble_ranking = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸Šä½20ç‰¹å¾´é‡:")
        for i, (feature, score) in enumerate(ensemble_ranking[:20]):
            logger.info(f"  {i+1:2d}. {feature:40s}: {score:.4f}")
        
        return ensemble_ranking
    
    def _progressive_optimization(self, X, y, ensemble_ranking):
        """æ®µéšçš„æœ€é©åŒ–"""
        logger.info("    æ®µéšçš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
        
        # ç‰¹å¾´é‡æ•°ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
        feature_counts = [5, 10, 15, 20, 25, 30, 40, 50, 75, 100]
        max_features = min(len(ensemble_ranking), 100)
        feature_counts = [n for n in feature_counts if n <= max_features]
        
        results = {}
        
        for n_features in feature_counts:
            selected_features = [name for name, score in ensemble_ranking[:n_features]]
            X_selected = X[selected_features]
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_selected)
            
            # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
            model_scores = {}
            
            for model_name, model in self.models.items():
                tscv = TimeSeriesSplit(n_splits=3)
                scores = []
                
                for train_idx, test_idx in tscv.split(X_scaled):
                    X_train = X_scaled[train_idx]
                    X_test = X_scaled[test_idx]
                    y_train = y.iloc[train_idx]
                    y_test = y.iloc[test_idx]
                    
                    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
                    if model_name == 'LogisticRegression':
                        model.set_params(C=0.1, class_weight='balanced')
                    elif model_name == 'RandomForest':
                        model.set_params(class_weight='balanced')
                    elif model_name == 'GradientBoosting':
                        pass  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_test)
                    scores.append(accuracy_score(y_test, pred))
                
                model_scores[model_name] = np.mean(scores)
            
            # æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ç‰¹å®š
            best_model = max(model_scores, key=model_scores.get)
            best_score = model_scores[best_model]
            
            results[n_features] = {
                'best_model': best_model,
                'best_score': best_score,
                'all_scores': model_scores,
                'features': selected_features
            }
            
            logger.info(f"    {n_features:3d}ç‰¹å¾´é‡: {best_score:.1%} ({best_model})")
        
        # æœ€é«˜æ€§èƒ½ã®ç‰¹å¾´é‡æ•°ç‰¹å®š
        best_n = max(results.keys(), key=lambda k: results[k]['best_score'])
        
        logger.info(f"\n  æœ€é«˜æ€§èƒ½: {best_n}ç‰¹å¾´é‡, {results[best_n]['best_score']:.1%} ({results[best_n]['best_model']})")
        
        return results
    
    def _category_combination_optimization(self, X, y, feature_categories):
        """ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›æœ€é©åŒ–"""
        logger.info("    ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ...")
        
        # æœ‰åŠ¹ãªã‚«ãƒ†ã‚´ãƒªã®ã¿
        valid_categories = {k: v for k, v in feature_categories.items() if v}
        category_names = list(valid_categories.keys())
        
        combination_results = {}
        
        # å˜ä¸€ã‚«ãƒ†ã‚´ãƒª
        for category in category_names[:6]:  # è¨ˆç®—æ™‚é–“çŸ­ç¸®
            features = feature_categories[category]
            if len(features) == 0:
                continue
                
            X_cat = X[features]
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_cat)
            
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42)
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            combination_results[category] = {
                'score': np.mean(scores),
                'categories': [category],
                'feature_count': len(features)
            }
        
        # 2ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ï¼‰
        important_categories = ['technical_ma', 'market', 'technical_volatility', 'relative']
        important_categories = [c for c in important_categories if c in category_names]
        
        for combo in itertools.combinations(important_categories, 2):
            combo_features = []
            for cat in combo:
                combo_features.extend(feature_categories[cat])
            
            if len(combo_features) == 0 or len(combo_features) > 50:  # ç‰¹å¾´é‡æ•°åˆ¶é™
                continue
            
            X_combo = X[combo_features]
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_combo)
            
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42)
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            combo_name = '+'.join(combo)
            combination_results[combo_name] = {
                'score': np.mean(scores),
                'categories': list(combo),
                'feature_count': len(combo_features)
            }
        
        # çµæœè¡¨ç¤º
        sorted_combos = sorted(combination_results.items(), key=lambda x: x[1]['score'], reverse=True)
        
        logger.info("    ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›çµæœ:")
        for i, (combo, result) in enumerate(sorted_combos[:10]):
            logger.info(f"      {i+1:2d}. {combo:30s}: {result['score']:.1%} ({result['feature_count']}ç‰¹å¾´é‡)")
        
        return combination_results
    
    def _final_optimization(self, X, y, selection_results):
        """æœ€çµ‚æœ€é©åŒ–"""
        logger.info("ğŸ¯ æœ€çµ‚æœ€é©åŒ–å®Ÿè¡Œ...")
        
        # å„æ‰‹æ³•ã®æœ€é«˜æ€§èƒ½ã‚’å–å¾—
        best_candidates = []
        
        # Progressiveçµæœã‹ã‚‰æœ€é«˜æ€§èƒ½
        if 'progressive' in selection_results:
            prog_results = selection_results['progressive']
            best_prog = max(prog_results.keys(), key=lambda k: prog_results[k]['best_score'])
            best_candidates.append({
                'name': f'Progressive_{best_prog}features',
                'features': prog_results[best_prog]['features'],
                'model': prog_results[best_prog]['best_model'],
                'score': prog_results[best_prog]['best_score']
            })
        
        # ã‚«ãƒ†ã‚´ãƒªçµ„ã¿åˆã‚ã›ã‹ã‚‰æœ€é«˜æ€§èƒ½
        if 'combination' in selection_results:
            combo_results = selection_results['combination']
            best_combo = max(combo_results.keys(), key=lambda k: combo_results[k]['score'])
            
            # æœ€é«˜ã‚«ãƒ†ã‚´ãƒªã®ç‰¹å¾´é‡ã‚’å–å¾—
            best_combo_info = combo_results[best_combo]
            combo_features = []
            for cat in best_combo_info['categories']:
                if cat in selection_results.get('category_scores', {}):
                    combo_features.extend(selection_results['category_scores'][cat]['features'])
            
            if combo_features:
                best_candidates.append({
                    'name': f'Category_{best_combo}',
                    'features': combo_features,
                    'model': 'LogisticRegression',
                    'score': best_combo_info['score']
                })
        
        # æœ€çµ‚æ¤œè¨¼
        final_results = {}
        
        for candidate in best_candidates:
            logger.info(f"  æœ€çµ‚æ¤œè¨¼: {candidate['name']} ({len(candidate['features'])}ç‰¹å¾´é‡)")
            
            X_final = X[candidate['features']]
            
            # è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
            scaler_results = {}
            
            for scaler_name, scaler in self.scalers.items():
                X_scaled = scaler.fit_transform(X_final)
                
                # 5åˆ†å‰²ã§å³å¯†è©•ä¾¡
                tscv = TimeSeriesSplit(n_splits=5)
                scores = []
                
                for train_idx, test_idx in tscv.split(X_scaled):
                    X_train = X_scaled[train_idx]
                    X_test = X_scaled[test_idx]
                    y_train = y.iloc[train_idx]
                    y_test = y.iloc[test_idx]
                    
                    # æœ€é©ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
                    if candidate['model'] == 'LogisticRegression':
                        model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=2000, random_state=42)
                    elif candidate['model'] == 'RandomForest':
                        model = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)
                    elif candidate['model'] == 'GradientBoosting':
                        model = GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42)
                    else:
                        model = LogisticRegression(C=0.1, class_weight='balanced', max_iter=2000, random_state=42)
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_test)
                    scores.append(accuracy_score(y_test, pred))
                
                avg_score = np.mean(scores)
                std_score = np.std(scores)
                scaler_results[scaler_name] = {'avg': avg_score, 'std': std_score, 'scores': scores}
                
                logger.info(f"    {scaler_name:10s}: {avg_score:.1%} Â± {std_score:.1%}")
            
            # æœ€é«˜ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼é¸æŠ
            best_scaler = max(scaler_results.keys(), key=lambda k: scaler_results[k]['avg'])
            
            final_results[candidate['name']] = {
                'features': candidate['features'],
                'model': candidate['model'],
                'best_scaler': best_scaler,
                'score': scaler_results[best_scaler]['avg'],
                'std': scaler_results[best_scaler]['std'],
                'all_scaler_results': scaler_results
            }
        
        # æœ€é«˜æ€§èƒ½ç‰¹å®š
        if final_results:
            best_final = max(final_results.keys(), key=lambda k: final_results[k]['score'])
            logger.info(f"\nğŸ† æœ€é«˜æ€§èƒ½: {best_final}")
            logger.info(f"ç²¾åº¦: {final_results[best_final]['score']:.1%} Â± {final_results[best_final]['std']:.1%}")
            logger.info(f"ç‰¹å¾´é‡æ•°: {len(final_results[best_final]['features'])}")
            logger.info(f"ãƒ¢ãƒ‡ãƒ«: {final_results[best_final]['model']}")
            logger.info(f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {final_results[best_final]['best_scaler']}")
        
        return final_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ ç©¶æ¥µã®ç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ394,102ä»¶ï¼‰ç‰ˆ")
    logger.info("âš ï¸ ã‚ã‚‰ã‚†ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€éå¸¸ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
    
    selector = UltimateFeatureSelector()
    
    try:
        # 1. å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = selector.load_full_data()
        if df is None:
            return
        
        # 2. åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ
        df = selector.create_comprehensive_features(df)
        
        # 3. ç‰¹å¾´é‡åˆ†é¡
        feature_categories, all_features = selector.categorize_features(df)
        
        # 4. åŒ…æ‹¬çš„ç‰¹å¾´é‡é¸æŠ
        selection_results, final_results = selector.comprehensive_feature_selection(
            df, feature_categories, all_features
        )
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*100)
        logger.info("ğŸ¯ ç©¶æ¥µã®ç‰¹å¾´é‡é¸æŠçµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*100)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(df):,}ä»¶ (å…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼)")
        logger.info(f"ä½œæˆç‰¹å¾´é‡ç·æ•°: {len(all_features)}å€‹")
        
        # æœ€çµ‚çµæœ
        if final_results:
            best_result = max(final_results.keys(), key=lambda k: final_results[k]['score'])
            result = final_results[best_result]
            
            logger.info(f"\nğŸ† æœ€é«˜é”æˆç²¾åº¦: {result['score']:.1%} Â± {result['std']:.1%}")
            logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(result['features'])}")
            logger.info(f"æœ€é©ãƒ¢ãƒ‡ãƒ«: {result['model']}")
            logger.info(f"æœ€é©ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {result['best_scaler']}")
            
            logger.info("\næœ€é©ç‰¹å¾´é‡:")
            for i, feature in enumerate(result['features'][:20], 1):
                logger.info(f"  {i:2d}. {feature}")
            if len(result['features']) > 20:
                logger.info(f"  ... ä»–{len(result['features'])-20}å€‹")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
            if 'category_scores' in selection_results:
                logger.info("\nã‚«ãƒ†ã‚´ãƒªåˆ¥æ€§èƒ½:")
                sorted_categories = sorted(
                    selection_results['category_scores'].items(),
                    key=lambda x: x[1]['score'], reverse=True
                )
                for category, info in sorted_categories[:10]:
                    logger.info(f"  {category:20s}: {info['score']:.1%} ({info['feature_count']}ç‰¹å¾´é‡)")
        
        logger.info(f"\nâš ï¸ ã“ã®çµæœã¯394,102ä»¶ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å³å¯†æ¤œè¨¼ã«ã‚ˆã‚‹ç¾å®Ÿçš„ãªæ€§èƒ½è©•ä¾¡ã§ã™ã€‚")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()