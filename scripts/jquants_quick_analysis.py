#!/usr/bin/env python3
"""
J-Quantsé«˜é€Ÿåˆ†æ - ãƒ‡ãƒ¼ã‚¿ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã®è¿…é€Ÿãªç²¾åº¦è©•ä¾¡
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class JQuantsQuickAnalyzer:
    """J-Quantsé«˜é€Ÿåˆ†æ"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        
    def load_and_sample_data(self, sample_size=50000):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size:,}ï¼‰")
        
        # æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,}ä»¶")
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(df) > sample_size:
            df = df.sort_values('Date').tail(sample_size)
            logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(df):,}ä»¶")
        
        return df
    
    def create_jquants_inspired_features(self, df):
        """J-Quantsãƒ©ã‚¤ã‚¯ãªæ‹¡å¼µç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ğŸ”§ J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        # 1. å¸‚å ´å…¨ä½“æŒ‡æ¨™ï¼ˆæŒ‡æ•°çš„ç‰¹å¾´é‡ï¼‰
        daily_market = df.groupby('Date').agg({
            'Close': ['mean', 'std'],
            'Volume': ['mean', 'std'],
            'Returns': 'mean'
        }).round(6)
        
        daily_market.columns = [
            'Market_Close_Mean', 'Market_Close_Std', 
            'Market_Volume_Mean', 'Market_Volume_Std',
            'Market_Return_Mean'
        ]
        daily_market = daily_market.reset_index()
        
        # 2. ã‚»ã‚¯ã‚¿ãƒ¼æ¨¡æ“¬ï¼ˆã‚³ãƒ¼ãƒ‰å‰2æ¡ã§ã‚»ã‚¯ã‚¿ãƒ¼åˆ†é¡ï¼‰
        df['Sector_Code'] = df['Code'].astype(str).str[:2]
        sector_daily = df.groupby(['Date', 'Sector_Code'])['Close'].mean().reset_index()
        sector_daily.columns = ['Date', 'Sector_Code', 'Sector_Avg_Price']
        
        # 3. ä¿¡ç”¨å–å¼•æ¨¡æ“¬æŒ‡æ¨™
        # å‡ºæ¥é«˜æ€¥å¢—ã‚’ä¿¡ç”¨å–å¼•ã®ä»£ç†æŒ‡æ¨™ã¨ã™ã‚‹
        df['Volume_MA5'] = df.groupby('Code')['Volume'].rolling(5).mean().reset_index(0, drop=True)
        df['Volume_Shock'] = df['Volume'] / (df['Volume_MA5'] + 1e-6)
        
        # ä¾¡æ ¼ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ç©ºå£²ã‚Šåœ§åŠ›ã®ä»£ç†æŒ‡æ¨™ã¨ã™ã‚‹
        df['Price_Volatility_5d'] = df.groupby('Code')['Close'].rolling(5).std().reset_index(0, drop=True)
        df['Volatility_Rank'] = df.groupby('Date')['Price_Volatility_5d'].rank(pct=True)
        
        # 4. å¸‚å ´ç›¸å¯¾æŒ‡æ¨™
        df = df.merge(daily_market, on='Date', how='left')
        df = df.merge(sector_daily, on=['Date', 'Sector_Code'], how='left')
        
        df['Market_Relative_Return'] = df['Returns'] - df['Market_Return_Mean'] 
        df['Market_Relative_Price'] = df['Close'] / (df['Market_Close_Mean'] + 1e-6)
        df['Sector_Relative_Price'] = df['Close'] / (df['Sector_Avg_Price'] + 1e-6)
        df['Market_Volume_Relative'] = df['Volume'] / (df['Market_Volume_Mean'] + 1e-6)
        
        # 5. å¤–å›½äººæŠ•è³‡å®¶æ¨¡æ“¬ï¼ˆå¤§å‹æ ªã§ã®ç‰¹åˆ¥ãªå‹•ãï¼‰
        df['Market_Cap_Proxy'] = df['Close'] * df['Volume']  # ç°¡æ˜“æ™‚ä¾¡ç·é¡
        df['Large_Cap_Flag'] = (df.groupby('Date')['Market_Cap_Proxy'].rank(pct=True) > 0.8).astype(int)
        
        # å¤§å‹æ ªã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ã¨å€‹åˆ¥éŠ˜æŸ„ã®ä¹–é›¢
        large_cap_return = df[df['Large_Cap_Flag'] == 1].groupby('Date')['Returns'].mean()
        large_cap_return = large_cap_return.reset_index()
        large_cap_return.columns = ['Date', 'Large_Cap_Return']
        
        df = df.merge(large_cap_return, on='Date', how='left')
        df['Foreign_Proxy'] = df['Returns'] - df['Large_Cap_Return']
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        logger.info(f"âœ… æ‹¡å¼µç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape}")
        return df
    
    def quick_evaluation(self, df):
        """é«˜é€Ÿè©•ä¾¡"""
        logger.info("âš¡ é«˜é€Ÿè©•ä¾¡é–‹å§‹...")
        
        if 'Binary_Direction' not in df.columns:
            logger.error("âŒ Binary_DirectionãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        # ç‰¹å¾´é‡åˆ†é¡
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction', 'Sector_Code'
        }
        
        all_features = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ç”±æ¥ï¼‰
        basic_features = [col for col in all_features if not any(
            keyword in col for keyword in ['Market', 'Sector', 'Volume_Shock', 'Volatility', 'Foreign', 'Large_Cap']
        )]
        
        # J-Quantsãƒ©ã‚¤ã‚¯æ‹¡å¼µç‰¹å¾´é‡
        jquants_features = [col for col in all_features if any(
            keyword in col for keyword in ['Market', 'Sector', 'Volume_Shock', 'Volatility', 'Foreign', 'Large_Cap']
        )]
        
        logger.info(f"åŸºæœ¬ç‰¹å¾´é‡: {len(basic_features)}å€‹")
        logger.info(f"J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡: {len(jquants_features)}å€‹")
        logger.info(f"å…¨ç‰¹å¾´é‡: {len(all_features)}å€‹")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        logger.info(f"è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = {}
        
        # 1. åŸºæœ¬ç‰¹å¾´é‡
        if basic_features:
            X_basic = clean_df[basic_features]
            y = clean_df['Binary_Direction']
            results['basic'] = self._fast_evaluate(X_basic, y, "åŸºæœ¬ç‰¹å¾´é‡")
        
        # 2. J-Quantsãƒ©ã‚¤ã‚¯ç‰¹å¾´é‡ã®ã¿
        if jquants_features:
            X_jquants = clean_df[jquants_features]
            y = clean_df['Binary_Direction']
            results['jquants_only'] = self._fast_evaluate(X_jquants, y, "J-Quantsãƒ©ã‚¤ã‚¯")
        
        # 3. å…¨ç‰¹å¾´é‡
        X_all = clean_df[all_features]
        y = clean_df['Binary_Direction']
        results['combined'] = self._fast_evaluate(X_all, y, "çµåˆç‰¹å¾´é‡")
        
        return results
    
    def _fast_evaluate(self, X, y, name):
        """é«˜é€Ÿè©•ä¾¡å®Ÿè¡Œ"""
        logger.info(f"ğŸš€ {name}è©•ä¾¡ä¸­...")
        
        # é«˜é€Ÿè¨­å®š
        tscv = TimeSeriesSplit(n_splits=2)  # åˆ†å‰²æ•°å‰Šæ¸›
        scaler = StandardScaler()
        
        # è»½é‡ãƒ¢ãƒ‡ãƒ«
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100, max_depth=10, 
                class_weight='balanced', random_state=42, n_jobs=-1
            ),
            'LogisticRegression': LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=500
            )
        }
        
        model_results = {}
        
        for model_name, model in models.items():
            fold_scores = []
            
            for train_idx, test_idx in tscv.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                # å‰å‡¦ç†
                if 'Logistic' in model_name:
                    X_train_proc = scaler.fit_transform(X_train)
                    X_test_proc = scaler.transform(X_test)
                else:
                    X_train_proc = X_train
                    X_test_proc = X_test
                
                # å­¦ç¿’ãƒ»äºˆæ¸¬
                model.fit(X_train_proc, y_train)
                y_pred = model.predict(X_test_proc)
                accuracy = accuracy_score(y_test, y_pred)
                fold_scores.append(accuracy)
            
            avg_score = np.mean(fold_scores)
            model_results[model_name] = {
                'score': avg_score,
                'std': np.std(fold_scores)
            }
            
            logger.info(f"  {model_name}: {avg_score:.3f} Â± {np.std(fold_scores):.3f}")
        
        return model_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        analyzer = JQuantsQuickAnalyzer()
        
        print("âš¡ J-Quantsé«˜é€Ÿç²¾åº¦åˆ†æ")
        print("="*50)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = analyzer.load_and_sample_data(sample_size=50000)
        if df is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—")
            return 1
        
        # æ‹¡å¼µç‰¹å¾´é‡ä½œæˆ
        df_enhanced = analyzer.create_jquants_inspired_features(df)
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = analyzer.quick_evaluation(df_enhanced)
        
        if not results:
            print("âŒ è©•ä¾¡å¤±æ•—")
            return 1
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*50)
        print("ğŸ“‹ J-QUANTSé«˜é€Ÿåˆ†æçµæœ")
        print("="*50)
        
        baseline = 0.517  # æ—¢å­˜æœ€é«˜ã‚¹ã‚³ã‚¢
        best_score = 0
        best_config = ""
        
        for feature_type, models in results.items():
            print(f"\nğŸ” {feature_type.upper()}:")
            
            for model_name, result in models.items():
                score = result['score']
                std = result['std']
                improvement = score - baseline
                
                print(f"   {model_name:18s}: {score:.3f} Â± {std:.3f} ({improvement:+.3f})")
                
                if score > best_score:
                    best_score = score
                    best_config = f"{feature_type} + {model_name}"
        
        # æœ€çµ‚è©•ä¾¡
        total_improvement = best_score - baseline
        
        print(f"\nğŸ† æœ€é«˜æ€§èƒ½:")
        print(f"   è¨­å®š: {best_config}")
        print(f"   ç²¾åº¦: {best_score:.3f} ({best_score:.1%})")
        print(f"   æ”¹å–„: {total_improvement:+.3f} ({total_improvement:+.1%})")
        
        print(f"\nğŸ’¡ J-Quantsãƒ‡ãƒ¼ã‚¿ã®åŠ¹æœ:")
        if total_improvement > 0.01:
            print(f"   âœ… æœ‰æ„ãªæ”¹å–„ (+{total_improvement:.1%})")
            print(f"   ğŸš€ J-Quantsãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ä¾¡å€¤ã‚ã‚Š")
        elif total_improvement > 0.005:
            print(f"   ğŸ“ˆ å¾®ç´°ãªæ”¹å–„ (+{total_improvement:.1%})")
            print(f"   ğŸ’¡ ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨çµ„ã¿åˆã‚ã›ã§åŠ¹æœçš„")
        else:
            print(f"   â¡ï¸ é™å®šçš„ãªåŠ¹æœ ({total_improvement:+.1%})")
            print(f"   ğŸ’¡ å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ¿æƒ…å ±ãªã©ï¼‰")
        
        # ç›®æ¨™é”æˆåˆ¤å®š
        if best_score >= 0.53:
            print(f"\nğŸ‰ ç›®æ¨™é”æˆ! 53%ã‚’çªç ´!")
        elif best_score >= 0.525:
            print(f"\nğŸ”¥ ç›®æ¨™ã«éå¸¸ã«è¿‘ã„ï¼ˆ52.5%ä»¥ä¸Šï¼‰")
        elif best_score >= 0.52:
            print(f"\nğŸ‘ æœ‰æ„ãªæ”¹å–„ã‚’ç¢ºèªï¼ˆ52%ä»¥ä¸Šï¼‰")
        else:
            print(f"\nğŸ“ˆ æ›´ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦")
        
        print(f"\nğŸ“Š æ¨å¥¨æ¬¡ã‚¹ãƒ†ãƒƒãƒ—:")
        if best_score < 0.53:
            print(f"   1. æ¿æƒ…å ±ï¼ˆkabu APIï¼‰ã®è¿½åŠ ")
            print(f"   2. ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æã®å°å…¥")
            print(f"   3. ã‚»ã‚¯ã‚¿ãƒ¼ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨")
        
        return 0 if total_improvement > 0 else 1
        
    except Exception as e:
        logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    exit(main())