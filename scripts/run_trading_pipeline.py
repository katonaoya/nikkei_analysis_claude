#!/usr/bin/env python
"""
çµ±åˆå®Ÿé‹ç”¨ã‚·ã‚¹ãƒ†ãƒ  - 1ã‚³ãƒãƒ³ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã‹ã‚‰äºˆæ¸¬ã¾ã§å®Ÿè¡Œ
"""

import os
import sys
import subprocess
from pathlib import Path
import pandas as pd
import numpy as np
import argparse
import logging
import json
from datetime import datetime, date, timedelta
from typing import Optional, Dict, List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trading_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class TradingPipeline:
    """çµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, data_dir: Path = None):
        self.data_dir = data_dir or Path("data")
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.models_dir = self.data_dir / "models"
        self.predictions_dir = self.data_dir / "predictions"
        self.reports_dir = self.data_dir / "reports"
        
        # Create directories
        for dir_path in [self.processed_dir, self.models_dir, self.predictions_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        self.scripts_dir = project_root / "scripts"
    
    def run_command(self, command: List[str], description: str) -> Dict:
        """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã¨ãƒ­ã‚°è¨˜éŒ²"""
        logger.info(f"ğŸ”§ {description}")
        logger.info(f"   Command: {' '.join(command)}")
        
        try:
            start_time = datetime.now()
            result = subprocess.run(
                command,
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            duration = datetime.now() - start_time
            
            if result.returncode == 0:
                logger.info(f"âœ… {description} completed in {duration}")
                return {
                    'success': True,
                    'duration': duration,
                    'stdout': result.stdout,
                    'stderr': result.stderr
                }
            else:
                logger.error(f"âŒ {description} failed with return code {result.returncode}")
                logger.error(f"   Error: {result.stderr}")
                return {
                    'success': False,
                    'duration': duration,
                    'stdout': result.stdout,
                    'stderr': result.stderr,
                    'return_code': result.returncode
                }
        
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {description} timed out after 10 minutes")
            return {
                'success': False,
                'error': 'timeout',
                'duration': timedelta(minutes=10)
            }
        except Exception as e:
            logger.error(f"âŒ {description} failed with exception: {e}")
            return {
                'success': False,
                'error': str(e),
                'duration': datetime.now() - start_time
            }
    
    def step1_data_collection(
        self, 
        start_date: Optional[str] = None, 
        end_date: Optional[str] = None,
        batch_size: int = 5,
        delay: float = 1.0
    ) -> Dict:
        """ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿åé›†"""
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥ä»˜è¨­å®šï¼ˆéå»30æ—¥ï¼‰
        if not end_date:
            end_date = date.today().strftime("%Y-%m-%d")
        if not start_date:
            start_date = (date.today() - timedelta(days=30)).strftime("%Y-%m-%d")
        
        command = [
            "python", 
            str(self.scripts_dir / "collect_historical_data.py"),
            "--start-date", start_date,
            "--end-date", end_date,
            "--batch-size", str(batch_size),
            "--delay", str(delay)
        ]
        
        result = self.run_command(command, f"ãƒ‡ãƒ¼ã‚¿åé›† ({start_date} to {end_date})")
        
        # æˆåŠŸã—ãŸå ´åˆã€æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        if result['success']:
            data_files = list(self.raw_dir.glob("nikkei225_historical_*.parquet"))
            if data_files:
                latest_file = max(data_files, key=lambda f: f.stat().st_mtime)
                result['data_file'] = latest_file.name
                logger.info(f"ğŸ“Š Latest data file: {latest_file.name}")
        
        return result
    
    def step2_feature_generation(self, output_filename: Optional[str] = None) -> Dict:
        """ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡ç”Ÿæˆ"""
        
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"features_pipeline_{timestamp}.parquet"
        
        command = [
            "python",
            str(self.scripts_dir / "simple_feature_generation.py"),
            "--output-filename", output_filename
        ]
        
        result = self.run_command(command, "ç‰¹å¾´é‡ç”Ÿæˆ")
        
        if result['success']:
            result['features_file'] = output_filename
            logger.info(f"ğŸ”§ Features file: {output_filename}")
        
        return result
    
    def step3_model_training(
        self, 
        features_file: str, 
        models: List[str] = None,
        train_regression: bool = True,
        train_classification: bool = True
    ) -> Dict:
        """ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        
        results = {}
        
        # å›å¸°ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        if train_regression:
            command = [
                "python",
                str(self.scripts_dir / "simple_model_training.py"),
                "--features-file", features_file,
                "--target", "Next_Day_Return",
                "--model-type", "regression",
                "--save-models"
            ]
            
            reg_result = self.run_command(command, "å›å¸°ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
            results['regression'] = reg_result
            
            # è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            if reg_result['success']:
                model_files = list(self.models_dir.glob("*Next_Day_Return*.joblib"))
                if model_files:
                    latest_reg_model = max(model_files, key=lambda f: f.stat().st_mtime)
                    results['regression']['model_file'] = latest_reg_model.name
        
        # åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        if train_classification:
            command = [
                "python",
                str(self.scripts_dir / "simple_model_training.py"),
                "--features-file", features_file,
                "--target", "Binary_Direction",
                "--model-type", "classification",
                "--save-models"
            ]
            
            clf_result = self.run_command(command, "åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
            results['classification'] = clf_result
            
            # è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            if clf_result['success']:
                model_files = list(self.models_dir.glob("*Binary_Direction*.joblib"))
                if model_files:
                    latest_clf_model = max(model_files, key=lambda f: f.stat().st_mtime)
                    results['classification']['model_file'] = latest_clf_model.name
        
        return results
    
    def step4_prediction(self, model_files: Dict[str, str]) -> Dict:
        """ã‚¹ãƒ†ãƒƒãƒ—4: äºˆæ¸¬å®Ÿè¡Œ"""
        
        results = {}
        
        # å›å¸°äºˆæ¸¬
        if 'regression' in model_files and model_files['regression']:
            command = [
                "python",
                str(self.scripts_dir / "simple_prediction.py"),
                "--model-file", model_files['regression'],
                "--save-predictions"
            ]
            
            reg_pred_result = self.run_command(command, "å›å¸°äºˆæ¸¬å®Ÿè¡Œ")
            results['regression'] = reg_pred_result
        
        # åˆ†é¡äºˆæ¸¬
        if 'classification' in model_files and model_files['classification']:
            command = [
                "python",
                str(self.scripts_dir / "simple_prediction.py"),
                "--model-file", model_files['classification'],
                "--save-predictions"
            ]
            
            clf_pred_result = self.run_command(command, "åˆ†é¡äºˆæ¸¬å®Ÿè¡Œ")
            results['classification'] = clf_pred_result
        
        return results
    
    def generate_summary_report(self, pipeline_results: Dict) -> Path:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.reports_dir / f"pipeline_report_{timestamp}.json"
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ä½œæˆ
        report = {
            'timestamp': timestamp,
            'execution_date': date.today().isoformat(),
            'pipeline_results': pipeline_results,
            'summary': {
                'total_duration': sum(
                    [step.get('duration', timedelta(0)).total_seconds() for step in pipeline_results.values()
                     if isinstance(step, dict) and isinstance(step.get('duration'), timedelta)],
                    0
                ),
                'successful_steps': sum(
                    1 for step in pipeline_results.values()
                    if isinstance(step, dict) and step.get('success', False)
                ),
                'failed_steps': sum(
                    1 for step in pipeline_results.values()
                    if isinstance(step, dict) and not step.get('success', True)
                )
            }
        }
        
        # JSONå½¢å¼ã§ä¿å­˜ï¼ˆdatetime objectã¯æ–‡å­—åˆ—ã«å¤‰æ›ï¼‰
        def json_serializer(obj):
            if isinstance(obj, timedelta):
                return obj.total_seconds()
            elif isinstance(obj, datetime):
                return obj.isoformat()
            return str(obj)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=json_serializer)
        
        logger.info(f"ğŸ“‹ Summary report saved: {report_file}")
        return report_file
    
    def run_full_pipeline(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        skip_data_collection: bool = False,
        train_regression: bool = True,
        train_classification: bool = True
    ) -> Dict:
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        
        logger.info("ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        logger.info("="*60)
        
        pipeline_results = {}
        start_time = datetime.now()
        
        try:
            # Step 1: ãƒ‡ãƒ¼ã‚¿åé›†
            if not skip_data_collection:
                logger.info("ğŸ“Š Step 1: ãƒ‡ãƒ¼ã‚¿åé›†")
                pipeline_results['data_collection'] = self.step1_data_collection(
                    start_date, end_date
                )
                
                if not pipeline_results['data_collection']['success']:
                    logger.error("âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return pipeline_results
            
            # Step 2: ç‰¹å¾´é‡ç”Ÿæˆ
            logger.info("ğŸ”§ Step 2: ç‰¹å¾´é‡ç”Ÿæˆ")
            pipeline_results['feature_generation'] = self.step2_feature_generation()
            
            if not pipeline_results['feature_generation']['success']:
                logger.error("âŒ ç‰¹å¾´é‡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return pipeline_results
            
            features_file = pipeline_results['feature_generation']['features_file']
            
            # Step 3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            logger.info("ğŸ¤– Step 3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
            pipeline_results['model_training'] = self.step3_model_training(
                features_file, 
                train_regression=train_regression,
                train_classification=train_classification
            )
            
            # è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            model_files = {}
            if train_regression and 'regression' in pipeline_results['model_training']:
                if pipeline_results['model_training']['regression'].get('success'):
                    model_files['regression'] = pipeline_results['model_training']['regression'].get('model_file')
            
            if train_classification and 'classification' in pipeline_results['model_training']:
                if pipeline_results['model_training']['classification'].get('success'):
                    model_files['classification'] = pipeline_results['model_training']['classification'].get('model_file')
            
            # Step 4: äºˆæ¸¬å®Ÿè¡Œ
            if model_files:
                logger.info("ğŸ”® Step 4: äºˆæ¸¬å®Ÿè¡Œ")
                pipeline_results['predictions'] = self.step4_prediction(model_files)
            else:
                logger.warning("âš ï¸ è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒãªã„ãŸã‚ã€äºˆæ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            
            # ç·å®Ÿè¡Œæ™‚é–“
            total_duration = datetime.now() - start_time
            pipeline_results['total_duration'] = total_duration
            
            # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_file = self.generate_summary_report(pipeline_results)
            
            logger.info("="*60)
            logger.info(f"âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†! ç·å®Ÿè¡Œæ™‚é–“: {total_duration}")
            logger.info(f"ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
            
            return pipeline_results
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            pipeline_results['error'] = str(e)
            pipeline_results['total_duration'] = datetime.now() - start_time
            return pipeline_results


def print_pipeline_summary(results: Dict):
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
    
    print("\n" + "="*60)
    print("ğŸ“Š TRADING PIPELINE EXECUTION SUMMARY")
    print("="*60)
    
    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼
    total_duration = results.get('total_duration', timedelta(0))
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {total_duration}")
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœ
    steps = [
        ('data_collection', 'ãƒ‡ãƒ¼ã‚¿åé›†'),
        ('feature_generation', 'ç‰¹å¾´é‡ç”Ÿæˆ'),
        ('model_training', 'ãƒ¢ãƒ‡ãƒ«è¨“ç·´'),
        ('predictions', 'äºˆæ¸¬å®Ÿè¡Œ')
    ]
    
    print(f"\nğŸ“‹ å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—:")
    print("-" * 40)
    
    for step_key, step_name in steps:
        if step_key in results:
            step_result = results[step_key]
            if isinstance(step_result, dict):
                if step_result.get('success'):
                    print(f"âœ… {step_name}: æˆåŠŸ")
                else:
                    print(f"âŒ {step_name}: å¤±æ•—")
            else:
                print(f"ğŸ“ {step_name}: å®Ÿè¡Œæ¸ˆã¿")
        else:
            print(f"â­ï¸  {step_name}: ã‚¹ã‚­ãƒƒãƒ—")
    
    # æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°è¡¨ç¤º
    if 'model_training' in results:
        training_results = results['model_training']
        if isinstance(training_results, dict):
            print(f"\nğŸ¤– è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«:")
            print("-" * 30)
            
            for model_type in ['regression', 'classification']:
                if model_type in training_results:
                    model_result = training_results[model_type]
                    if model_result.get('success') and 'model_file' in model_result:
                        print(f"  âœ… {model_type}: {model_result['model_file']}")
                    else:
                        print(f"  âŒ {model_type}: è¨“ç·´å¤±æ•—")
    
    print("\n" + "="*60)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="çµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ")
    
    # ãƒ‡ãƒ¼ã‚¿åé›†é–¢é€£
    parser.add_argument(
        "--start-date",
        type=str,
        help="ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹æ—¥ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--end-date", 
        type=str,
        help="ãƒ‡ãƒ¼ã‚¿å–å¾—çµ‚äº†æ—¥ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--skip-data-collection",
        action="store_true",
        help="ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    
    # ãƒ¢ãƒ‡ãƒ«é–¢é€£
    parser.add_argument(
        "--no-regression",
        action="store_true", 
        help="å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    parser.add_argument(
        "--no-classification",
        action="store_true",
        help="åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    
    # ãã®ä»–
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="è©³ç´°å‡ºåŠ›ã‚’æŠ‘åˆ¶"
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´
    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        pipeline = TradingPipeline()
        
        results = pipeline.run_full_pipeline(
            start_date=args.start_date,
            end_date=args.end_date,
            skip_data_collection=args.skip_data_collection,
            train_regression=not args.no_regression,
            train_classification=not args.no_classification
        )
        
        # çµæœè¡¨ç¤º
        if not args.quiet:
            print_pipeline_summary(results)
        
        # æˆåŠŸ/å¤±æ•—ã®åˆ¤å®š
        if 'error' in results:
            return 1
        
        # ä¸»è¦ã‚¹ãƒ†ãƒƒãƒ—ã®æˆåŠŸç¢ºèª
        critical_steps = ['feature_generation']
        for step in critical_steps:
            if step in results and isinstance(results[step], dict):
                if not results[step].get('success', False):
                    return 1
        
        return 0
        
    except Exception as e:
        logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())