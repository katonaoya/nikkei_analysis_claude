#!/usr/bin/env python3
"""
å³å¯†ãª10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦æ¤œè¨¼
2014-2024ã®å®Œå…¨10å¹´é–“ã§ã®æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from datetime import datetime, date
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class Strict10YearValidator:
    """å³å¯†ãª10å¹´é–“æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡
        self.baseline_features = [
            'Market_Breadth', 'Market_Return', 'Volatility_20', 'Price_vs_MA20'
        ]
        
        # æœ€é©å¤–éƒ¨ç‰¹å¾´é‡ï¼ˆå‰å›ã®çµæœã‹ã‚‰ï¼‰
        self.optimal_external_features = [
            'sp500_change', 'vix_change', 'nikkei_change', 'us_10y_change', 'usd_jpy_change'
        ]
    
    def load_and_filter_10_years(self):
        """10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã®å³å¯†æŠ½å‡º"""
        logger.info("ğŸ“Š 10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã®å³å¯†æŠ½å‡º...")
        
        integrated_file = self.processed_dir / "integrated_with_external.parquet"
        if not integrated_file.exists():
            logger.error("âŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(integrated_file)
        df['Date'] = pd.to_datetime(df['Date'])
        
        # ç¾åœ¨ã®åˆ©ç”¨å¯èƒ½ãªå®Œå…¨å¹´åº¦ã‚’ç¢ºèª
        available_years = sorted(df['Date'].dt.year.unique())
        logger.info(f"åˆ©ç”¨å¯èƒ½å¹´åº¦: {available_years}")
        
        # æœ€æ–°ã®å®Œå…¨10å¹´é–“ã‚’æ±ºå®š
        # 2025å¹´ã¯8æœˆã¾ã§ãªã®ã§ã€2024å¹´ã‚’æœ€çµ‚å¹´ã¨ã—ã¦2015-2024ã®10å¹´é–“ã‚’ä½¿ç”¨
        end_year = 2024
        start_year = end_year - 9  # 10å¹´é–“ãªã®ã§9å¹´å·®
        
        logger.info(f"æ¤œè¨¼æœŸé–“è¨­å®š: {start_year}å¹´ï½{end_year}å¹´ (å®Œå…¨10å¹´é–“)")
        
        # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        start_date = f"{start_year}-01-01"
        end_date = f"{end_year}-12-31"
        
        filtered_df = df[
            (df['Date'] >= start_date) & 
            (df['Date'] <= end_date)
        ].copy()
        
        logger.info(f"âœ… 10å¹´é–“ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†:")
        logger.info(f"  æœŸé–“: {filtered_df['Date'].min().date()} - {filtered_df['Date'].max().date()}")
        logger.info(f"  ãƒ‡ãƒ¼ã‚¿æ•°: {len(filtered_df):,}ä»¶")
        
        # å¹´åˆ¥åˆ†å¸ƒç¢ºèª
        yearly_dist = filtered_df.groupby(filtered_df['Date'].dt.year).size()
        logger.info(f"  å¹´åˆ¥åˆ†å¸ƒ:")
        for year, count in yearly_dist.items():
            logger.info(f"    {year}å¹´: {count:,}ä»¶")
        
        return filtered_df
    
    def validate_data_quality(self, df):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
        logger.info("ğŸ” 10å¹´é–“ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼...")
        
        # äºˆæ¸¬å¯¾è±¡ã®ç¢ºèª
        valid_target = df['Binary_Direction'].notna().sum()
        logger.info(f"äºˆæ¸¬å¯¾è±¡æ•°: {valid_target:,}ä»¶")
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®å……å®Ÿåº¦ç¢ºèª
        external_cols = [col for col in df.columns if any(pattern in col for pattern in ['us_10y', 'sp500', 'usd_jpy', 'nikkei', 'vix'])]
        logger.info(f"å¤–éƒ¨ç‰¹å¾´é‡æ•°: {len(external_cols)}å€‹")
        
        # æœ€é©å¤–éƒ¨ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèª
        missing_optimal = [f for f in self.optimal_external_features if f not in df.columns]
        if missing_optimal:
            logger.error(f"âŒ æœ€é©å¤–éƒ¨ç‰¹å¾´é‡ä¸è¶³: {missing_optimal}")
            return False
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®æ¬ æçŠ¶æ³
        logger.info("å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿æ¬ æçŠ¶æ³:")
        for col in self.optimal_external_features:
            missing_rate = df[col].isnull().mean()
            logger.info(f"  {col}: {missing_rate*100:.1f}%æ¬ æ")
        
        return True
    
    def baseline_10_year_test(self, df):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³10å¹´é–“ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³10å¹´é–“ãƒ†ã‚¹ãƒˆ...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèª
        missing_baseline = [f for f in self.baseline_features if f not in clean_df.columns]
        if missing_baseline:
            logger.error(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡ä¸è¶³: {missing_baseline}")
            return None
            
        X = clean_df[self.baseline_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        
        return self._time_series_evaluation(X, y, "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå¾“æ¥4ç‰¹å¾´é‡ãƒ»10å¹´é–“ï¼‰")
    
    def external_10_year_test(self, df):
        """å¤–éƒ¨ç‰¹å¾´é‡10å¹´é–“ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸŒ å¤–éƒ¨ç‰¹å¾´é‡10å¹´é–“ãƒ†ã‚¹ãƒˆ...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        y = clean_df['Binary_Direction'].astype(int)
        
        # æœ€é©çµ„ã¿åˆã‚ã›ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + å¤–éƒ¨å¤‰åŒ–ç‰¹å¾´é‡ï¼‰
        optimal_features = self.baseline_features + self.optimal_external_features
        available_features = [f for f in optimal_features if f in clean_df.columns]
        
        X = clean_df[available_features].fillna(0)
        
        logger.info(f"å¤–éƒ¨ç‰¹å¾´é‡æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {len(available_features)}å€‹")
        
        return self._time_series_evaluation(X, y, f"æœ€é©å¤–éƒ¨ç‰¹å¾´é‡ï¼ˆ{len(available_features)}å€‹ãƒ»10å¹´é–“ï¼‰")
    
    def _time_series_evaluation(self, X, y, description):
        """æ™‚ç³»åˆ—è©•ä¾¡ï¼ˆ10åˆ†å‰²ã§ã‚ˆã‚Šå³å¯†ã«ï¼‰"""
        X_scaled = self.scaler.fit_transform(X)
        
        model = LogisticRegression(
            C=0.001, 
            class_weight='balanced', 
            random_state=42, 
            max_iter=1000,
            solver='lbfgs'
        )
        
        # 10åˆ†å‰²ã§ã‚ˆã‚Šå³å¯†ãªè©•ä¾¡
        tscv = TimeSeriesSplit(n_splits=10)
        scores = []
        fold_details = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train = X_scaled[train_idx]
            X_test = X_scaled[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, pred)
            scores.append(accuracy)
            
            fold_details.append({
                'fold': fold + 1,
                'accuracy': accuracy,
                'train_size': len(X_train),
                'test_size': len(X_test)
            })
        
        avg_score = np.mean(scores)
        std_score = np.std(scores)
        min_score = np.min(scores)
        max_score = np.max(scores)
        
        logger.info(f"  {description}:")
        logger.info(f"    å¹³å‡ç²¾åº¦: {avg_score:.3%} Â± {std_score:.3%}")
        logger.info(f"    ç¯„å›²: {min_score:.1%} - {max_score:.1%}")
        logger.info(f"    10åˆ†å‰²è©³ç´°:")
        for detail in fold_details:
            logger.info(f"      Fold{detail['fold']:2d}: {detail['accuracy']:.1%} (Train:{detail['train_size']:,}, Test:{detail['test_size']:,})")
        
        return {
            'avg': avg_score,
            'std': std_score,
            'min': min_score,
            'max': max_score,
            'scores': scores,
            'description': description,
            'fold_details': fold_details
        }
    
    def compare_results(self, baseline_result, external_result):
        """çµæœæ¯”è¼ƒã¨çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š"""
        logger.info("ğŸ“Š 10å¹´é–“æ¤œè¨¼çµæœæ¯”è¼ƒ...")
        
        baseline_score = baseline_result['avg']
        external_score = external_result['avg']
        improvement = (external_score - baseline_score) * 100
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆä¿¡é ¼åŒºé–“æ¯”è¼ƒï¼‰
        baseline_ci = baseline_result['std'] * 1.96  # 95%ä¿¡é ¼åŒºé–“
        external_ci = external_result['std'] * 1.96
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline_score:.3%} (95%CI: Â±{baseline_ci:.3%})")
        logger.info(f"å¤–éƒ¨ç‰¹å¾´é‡: {external_score:.3%} (95%CI: Â±{external_ci:.3%})")
        logger.info(f"æ”¹å–„åŠ¹æœ: {improvement:+.2f}%")
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤å®š
        if improvement > baseline_ci + external_ci:
            significance = "çµ±è¨ˆçš„ã«æœ‰æ„ãªæ”¹å–„"
        elif improvement > 0:
            significance = "æ”¹å–„å‚¾å‘ï¼ˆæœ‰æ„æ€§è¦æ¤œè¨¼ï¼‰"
        else:
            significance = "æ”¹å–„åŠ¹æœãªã—"
        
        logger.info(f"çµ±è¨ˆçš„è©•ä¾¡: {significance}")
        
        return {
            'improvement_pct': improvement,
            'significance': significance,
            'baseline_score': baseline_score,
            'external_score': external_score
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ å³å¯†ãª10å¹´é–“ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ")
    logger.info("ğŸ¯ ç›®æ¨™: 10å¹´åˆ†ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®æ­£ç¢ºãªç²¾åº¦è©•ä¾¡")
    
    validator = Strict10YearValidator()
    
    try:
        # 1. 10å¹´é–“ãƒ‡ãƒ¼ã‚¿ã®å³å¯†æŠ½å‡º
        df_10years = validator.load_and_filter_10_years()
        if df_10years is None:
            return
        
        # 2. ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
        if not validator.validate_data_quality(df_10years):
            return
        
        # 3. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³10å¹´é–“ãƒ†ã‚¹ãƒˆ
        baseline_result = validator.baseline_10_year_test(df_10years)
        if baseline_result is None:
            return
        
        # 4. å¤–éƒ¨ç‰¹å¾´é‡10å¹´é–“ãƒ†ã‚¹ãƒˆ
        external_result = validator.external_10_year_test(df_10years)
        
        # 5. çµæœæ¯”è¼ƒ
        comparison = validator.compare_results(baseline_result, external_result)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        logger.info("\n" + "="*100)
        logger.info("ğŸ¯ å³å¯†ãª10å¹´é–“æ¤œè¨¼çµæœ")
        logger.info("="*100)
        
        logger.info(f"ğŸ“Š æ¤œè¨¼æœŸé–“: 2015-2024å¹´ (å®Œå…¨10å¹´é–“)")
        logger.info(f"ğŸ“ˆ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_10years):,}ä»¶")
        logger.info(f"ğŸ”¬ è©•ä¾¡æ–¹æ³•: 10åˆ†å‰²æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        
        logger.info(f"\nğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœ:")
        logger.info(f"  ç²¾åº¦: {baseline_result['avg']:.3%} Â± {baseline_result['std']:.3%}")
        logger.info(f"  ç¯„å›²: {baseline_result['min']:.1%} - {baseline_result['max']:.1%}")
        
        logger.info(f"\nğŸŒ å¤–éƒ¨ç‰¹å¾´é‡çµæœ:")
        logger.info(f"  ç²¾åº¦: {external_result['avg']:.3%} Â± {external_result['std']:.3%}")
        logger.info(f"  ç¯„å›²: {external_result['min']:.1%} - {external_result['max']:.1%}")
        
        logger.info(f"\nğŸ¯ æ”¹å–„åŠ¹æœ:")
        logger.info(f"  ç²¾åº¦å‘ä¸Š: {comparison['improvement_pct']:+.2f}%")
        logger.info(f"  çµ±è¨ˆçš„è©•ä¾¡: {comparison['significance']}")
        
        # çµè«–
        if comparison['external_score'] > 0.55:
            logger.info(f"\nğŸ‰ å„ªç§€ãªçµæœï¼10å¹´é–“æ¤œè¨¼ã§55%è¶…ãˆã‚’é”æˆ")
        elif comparison['external_score'] > 0.52:
            logger.info(f"\nâœ… è‰¯å¥½ãªçµæœï¼10å¹´é–“æ¤œè¨¼ã§52%è¶…ãˆã‚’é”æˆ")
        elif comparison['improvement_pct'] > 1.0:
            logger.info(f"\nğŸ“ˆ æœ‰æ„ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèª")
        else:
            logger.info(f"\nğŸ“Š æ”¹å–„åŠ¹æœã¯é™å®šçš„")
        
        logger.info(f"\nâš–ï¸ ã“ã®çµæœã¯å®Œå…¨ãª10å¹´åˆ†ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®å³å¯†æ¤œè¨¼ã§ã™")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()