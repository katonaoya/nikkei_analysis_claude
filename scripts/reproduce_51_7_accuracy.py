#!/usr/bin/env python3
"""
51.7%ç²¾åº¦ã®å†ç¾ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ç‰ˆ
ä»¥å‰ã®æœ€é©ç‰¹å¾´é‡ã§51.7%ä»¥ä¸Šã‚’ç¢ºå®Ÿã«é”æˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import sys
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logger.remove()
logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="INFO")

class AccuracyReproducer:
    """51.7%ç²¾åº¦ã®å†ç¾ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.scaler = StandardScaler()
        
        # ä»¥å‰ã®æœ€é©ç‰¹å¾´é‡
        self.previous_optimal_features = [
            'Market_Breadth',
            'Market_Return', 
            'Volatility_20',
            'RSI',
            'Price_vs_MA20'
        ]
        
    def load_full_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ394,102ä»¶ï¼‰")
        
        processed_files = list(self.processed_dir.glob("*.parquet"))
        if not processed_files:
            logger.error("âŒ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        df = pd.read_parquet(processed_files[0])
        logger.info(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
        
        return df
    
    def check_existing_features(self, df):
        """æ—¢å­˜ç‰¹å¾´é‡ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ” æ—¢å­˜ç‰¹å¾´é‡ãƒã‚§ãƒƒã‚¯...")
        
        available_features = df.columns.tolist()
        logger.info(f"åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡ç·æ•°: {len(available_features)}")
        
        # ä»¥å‰ã®æœ€é©ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèª
        missing_features = []
        available_optimal = []
        
        for feature in self.previous_optimal_features:
            if feature in available_features:
                available_optimal.append(feature)
                logger.info(f"âœ… {feature}: å­˜åœ¨")
            else:
                missing_features.append(feature)
                logger.info(f"âŒ {feature}: ä¸å­˜åœ¨")
        
        logger.info(f"\nåˆ©ç”¨å¯èƒ½æœ€é©ç‰¹å¾´é‡: {len(available_optimal)}å€‹")
        logger.info(f"ä¸è¶³ç‰¹å¾´é‡: {len(missing_features)}å€‹")
        
        # é¡ä¼¼ç‰¹å¾´é‡æ¤œç´¢
        if missing_features:
            logger.info("\nğŸ” é¡ä¼¼ç‰¹å¾´é‡æ¤œç´¢:")
            for missing in missing_features:
                similar = [f for f in available_features if any(keyword in f.lower() for keyword in missing.lower().split('_'))]
                if similar:
                    logger.info(f"  {missing} â†’ é¡ä¼¼: {similar[:5]}")
                else:
                    logger.info(f"  {missing} â†’ é¡ä¼¼ãªã—")
        
        return available_optimal, missing_features
    
    def create_missing_features(self, df, missing_features):
        """ä¸è¶³ç‰¹å¾´é‡ã®ä½œæˆ"""
        logger.info("ğŸ”§ ä¸è¶³ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df = df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        
        for feature in missing_features:
            if feature == 'Market_Breadth':
                # å¸‚å ´å¹…æŒ‡æ¨™ä½œæˆ
                logger.info("  Market_Breadthä½œæˆä¸­...")
                daily_breadth = df.groupby('Date')['Returns'].apply(
                    lambda x: (x > 0).sum() / len(x) if len(x) > 0 else 0.5
                ).reset_index()
                daily_breadth.columns = ['Date', 'Market_Breadth']
                df = df.merge(daily_breadth, on='Date', how='left')
                
            elif feature == 'Market_Return':
                # å¸‚å ´å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ä½œæˆ
                logger.info("  Market_Returnä½œæˆä¸­...")
                daily_market_return = df.groupby('Date')['Returns'].mean().reset_index()
                daily_market_return.columns = ['Date', 'Market_Return']
                df = df.merge(daily_market_return, on='Date', how='left')
                
            elif feature == 'Volatility_20':
                # 20æ—¥ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ä½œæˆ
                logger.info("  Volatility_20ä½œæˆä¸­...")
                df['Volatility_20'] = df.groupby('Code')['Close'].rolling(20, min_periods=1).std().reset_index(0, drop=True)
                
            elif feature == 'RSI':
                # RSIä½œæˆ
                logger.info("  RSIä½œæˆä¸­...")
                df['RSI'] = self._calculate_rsi(df, 14)
                
            elif feature == 'Price_vs_MA20':
                # ç§»å‹•å¹³å‡ä¹–é›¢ç‡ä½œæˆ
                logger.info("  Price_vs_MA20ä½œæˆä¸­...")
                if 'MA_20' not in df.columns:
                    df['MA_20'] = df.groupby('Code')['Close'].rolling(20, min_periods=1).mean().reset_index(0, drop=True)
                df['Price_vs_MA20'] = (df['Close'] - df['MA_20']) / (df['MA_20'] + 1e-6)
        
        # æ¬ æå€¤å‡¦ç†
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        
        logger.info("âœ… ä¸è¶³ç‰¹å¾´é‡ä½œæˆå®Œäº†")
        return df
    
    def _calculate_rsi(self, df, period=14):
        """RSIè¨ˆç®—"""
        def rsi_calc(group):
            close = group['Close']
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + 1e-8)
            return 100 - (100 / (1 + rs))
        
        return df.groupby('Code', group_keys=False).apply(rsi_calc).reset_index(0, drop=True)
    
    def reproduce_51_7_accuracy(self, df):
        """51.7%ç²¾åº¦ã®å†ç¾"""
        logger.info("ğŸ¯ 51.7%ç²¾åº¦å†ç¾å®Ÿè¡Œ...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # æœ€é©ç‰¹å¾´é‡ãŒå…¨ã¦å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        all_features_exist = all(feature in clean_df.columns for feature in self.previous_optimal_features)
        
        if not all_features_exist:
            missing = [f for f in self.previous_optimal_features if f not in clean_df.columns]
            logger.error(f"âŒ ç‰¹å¾´é‡ä¸è¶³: {missing}")
            return None
        
        X = clean_df[self.previous_optimal_features].fillna(0)
        y = clean_df['Binary_Direction'].astype(int)
        
        logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(clean_df):,}ä»¶")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {self.previous_optimal_features}")
        
        # ä»¥å‰ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†ç¾
        models = {
            'LogisticRegression_L1': LogisticRegression(
                C=0.01, penalty='l1', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'LogisticRegression_L2': LogisticRegression(
                C=0.01, penalty='l2', solver='lbfgs',
                class_weight='balanced', random_state=42, max_iter=1000
            ),
            'RandomForest': RandomForestClassifier(
                n_estimators=100, max_depth=10,
                class_weight='balanced', random_state=42, n_jobs=-1
            )
        }
        
        results = {}
        
        # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
        for model_name, model in models.items():
            logger.info(f"  {model_name}è©•ä¾¡ä¸­...")
            
            if 'LogisticRegression' in model_name:
                X_scaled = self.scaler.fit_transform(X)
            else:
                X_scaled = X.values
            
            # 5åˆ†å‰²æ™‚ç³»åˆ—è©•ä¾¡
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, pred)
                scores.append(accuracy)
                
                # logger.info(f"    Fold {fold+1}: {accuracy:.1%}")
            
            avg_score = np.mean(scores)
            std_score = np.std(scores)
            
            results[model_name] = {
                'avg': avg_score,
                'std': std_score,
                'scores': scores
            }
            
            logger.info(f"  {model_name}: {avg_score:.1%} Â± {std_score:.1%}")
        
        # æœ€é«˜æ€§èƒ½ç‰¹å®š
        best_model = max(results.keys(), key=lambda k: results[k]['avg'])
        best_score = results[best_model]['avg']
        
        logger.info(f"\nğŸ† æœ€é«˜å†ç¾ç²¾åº¦: {best_score:.1%} ({best_model})")
        
        # 51.7%é”æˆç¢ºèª
        target_accuracy = 0.517  # 51.7%
        if best_score >= target_accuracy:
            logger.info(f"âœ… ç›®æ¨™51.7%é”æˆï¼ ({best_score:.1%} >= {target_accuracy:.1%})")
        else:
            logger.warning(f"âš ï¸ ç›®æ¨™51.7%æœªé”æˆ ({best_score:.1%} < {target_accuracy:.1%})")
            logger.info(f"å·®: {(target_accuracy - best_score)*100:.1f}%")
        
        return results, best_model, best_score
    
    def advanced_feature_test(self, df):
        """é«˜åº¦ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ§ª é«˜åº¦ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ...")
        
        clean_df = df[df['Binary_Direction'].notna()].copy()
        clean_df = clean_df.sort_values(['Date', 'Code']).reset_index(drop=True)
        
        # åˆ©ç”¨å¯èƒ½ãªå…¨ç‰¹å¾´é‡
        exclude_cols = {
            'Date', 'Code', 'Close', 'High', 'Low', 'Open', 'Volume',
            'Next_Day_Return', 'Binary_Direction'
        }
        
        all_features = [col for col in clean_df.columns 
                       if col not in exclude_cols and clean_df[col].dtype in ['int64', 'float64']]
        
        logger.info(f"åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {len(all_features)}å€‹")
        
        # ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
        test_patterns = {
            'Previous_Optimal': self.previous_optimal_features,
            'Top_10_Technical': [f for f in all_features if any(x in f for x in ['MA', 'RSI', 'Vol']) and 'Market' not in f][:10],
            'Top_10_Market': [f for f in all_features if 'Market' in f or 'Breadth' in f][:10],
            'Mixed_15': self.previous_optimal_features + [f for f in all_features if f not in self.previous_optimal_features][:10],
        }
        
        pattern_results = {}
        
        for pattern_name, features in test_patterns.items():
            # ç‰¹å¾´é‡å­˜åœ¨ç¢ºèª
            existing_features = [f for f in features if f in clean_df.columns]
            if len(existing_features) < 3:
                logger.info(f"  {pattern_name}: ç‰¹å¾´é‡ä¸è¶³ (ã‚¹ã‚­ãƒƒãƒ—)")
                continue
            
            logger.info(f"  {pattern_name} ({len(existing_features)}ç‰¹å¾´é‡)...")
            
            X = clean_df[existing_features]
            y = clean_df['Binary_Direction'].astype(int)
            
            # é«˜é€Ÿè©•ä¾¡
            X_scaled = self.scaler.fit_transform(X)
            
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, test_idx in tscv.split(X_scaled):
                X_train = X_scaled[train_idx]
                X_test = X_scaled[test_idx]
                y_train = y.iloc[train_idx]
                y_test = y.iloc[test_idx]
                
                model = LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, random_state=42)
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                scores.append(accuracy_score(y_test, pred))
            
            avg_score = np.mean(scores)
            pattern_results[pattern_name] = {
                'score': avg_score,
                'features': existing_features,
                'count': len(existing_features)
            }
            
            logger.info(f"    {pattern_name}: {avg_score:.1%}")
        
        # æœ€é«˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        if pattern_results:
            best_pattern = max(pattern_results.keys(), key=lambda k: pattern_results[k]['score'])
            best_pattern_score = pattern_results[best_pattern]['score']
            
            logger.info(f"\nğŸ† æœ€é«˜ãƒ‘ã‚¿ãƒ¼ãƒ³: {best_pattern} ({best_pattern_score:.1%})")
        
        return pattern_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ 51.7%ç²¾åº¦å†ç¾ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿ç‰ˆ")
    logger.info("ğŸ¯ ç›®æ¨™: ä»¥å‰ã®51.7%ä»¥ä¸Šã®ç²¾åº¦å†ç¾")
    
    reproducer = AccuracyReproducer()
    
    try:
        # 1. å…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = reproducer.load_full_data()
        if df is None:
            return
        
        # 2. æ—¢å­˜ç‰¹å¾´é‡ãƒã‚§ãƒƒã‚¯
        available_optimal, missing_features = reproducer.check_existing_features(df)
        
        # 3. ä¸è¶³ç‰¹å¾´é‡ä½œæˆ
        if missing_features:
            df = reproducer.create_missing_features(df, missing_features)
        
        # 4. 51.7%ç²¾åº¦å†ç¾
        results, best_model, best_score = reproducer.reproduce_51_7_accuracy(df)
        
        # 5. é«˜åº¦ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ
        pattern_results = reproducer.advanced_feature_test(df)
        
        # çµæœã¾ã¨ã‚
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ 51.7%ç²¾åº¦å†ç¾çµæœ")
        logger.info("="*80)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(df):,}ä»¶ (å…¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼)")
        
        # å†ç¾çµæœ
        if results:
            logger.info("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«åˆ¥å†ç¾çµæœ:")
            for model_name, result in results.items():
                logger.info(f"  {model_name:25s}: {result['avg']:.1%} Â± {result['std']:.1%}")
        
        logger.info(f"\nğŸ† æœ€é«˜å†ç¾ç²¾åº¦: {best_score:.1%} ({best_model})")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³çµæœ
        if pattern_results:
            logger.info("\nğŸ§ª ç‰¹å¾´é‡ãƒ‘ã‚¿ãƒ¼ãƒ³çµæœ:")
            sorted_patterns = sorted(pattern_results.items(), key=lambda x: x[1]['score'], reverse=True)
            for pattern, result in sorted_patterns:
                logger.info(f"  {pattern:20s}: {result['score']:.1%} ({result['count']}ç‰¹å¾´é‡)")
        
        # å…¨ä½“ã®æœ€é«˜ç²¾åº¦
        all_scores = [best_score]
        if pattern_results:
            all_scores.extend([result['score'] for result in pattern_results.values()])
        
        max_achieved = max(all_scores)
        logger.info(f"\nğŸ† å…¨ä½“æœ€é«˜ç²¾åº¦: {max_achieved:.1%}")
        
        # 51.7%ã¨ã®æ¯”è¼ƒ
        target = 0.517
        if max_achieved >= target:
            logger.info(f"âœ… 51.7%å†ç¾æˆåŠŸï¼ ({max_achieved:.1%} >= {target:.1%})")
        else:
            logger.warning(f"âš ï¸ 51.7%å†ç¾å¤±æ•— ({max_achieved:.1%} < {target:.1%})")
            logger.info(f"å·®: {(target - max_achieved)*100:.1f}%")
        
        logger.info(f"\nâš ï¸ ã“ã®çµæœã¯394,102ä»¶ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å³å¯†æ¤œè¨¼ã§ã™")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()