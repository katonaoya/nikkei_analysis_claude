#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
æ—¥çµŒ225å…¨éŠ˜æŸ„Ã—10å¹´ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦å‘ä¸Šæ¤œè¨¼
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, precision_score, classification_report
from sklearn.model_selection import TimeSeriesSplit
from pathlib import Path
import warnings
import logging
from datetime import datetime, timedelta
import joblib

warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class EnhancedPrecisionSystem:
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.scaler = RobustScaler()
        self.feature_selector = None
        self.model = None
        self.feature_names = None
        
        # é«˜ç²¾åº¦LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ‹¡å¼µãƒ‡ãƒ¼ã‚¿ç”¨ã«æœ€é©åŒ–ï¼‰
        self.model_params = {
            'n_estimators': 200,      # ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ ã«å¯¾å¿œ
            'max_depth': 6,           # è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            'min_child_samples': 15,  # éå­¦ç¿’é˜²æ­¢å¼·åŒ–
            'subsample': 0.85,        # ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            'colsample_bytree': 0.8,  # ç‰¹å¾´é‡ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            'learning_rate': 0.05,    # ä½å­¦ç¿’ç‡ã§å®‰å®šå­¦ç¿’
            'random_state': 42,
            'verbose': -1,
            'objective': 'binary',
            'metric': 'binary_logloss'
        }
    
    def load_enhanced_data(self):
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        logger.info("ğŸ“¥ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        # ã¾ãšæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
        existing_data_path = Path("data/processed/real_jquants_data.parquet")
        if existing_data_path.exists():
            logger.info("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­...")
            existing_df = pd.read_parquet(existing_data_path)
            logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_df):,}ä»¶, {existing_df['Code'].nunique()}éŠ˜æŸ„")
        
        # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        logger.info("âš ï¸ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™")
        
        # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        logger.warning("âš ï¸ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        if existing_data_path.exists():
            df = pd.read_parquet(existing_data_path)
            return self.preprocess_data(df)
        
        raise FileNotFoundError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    
    def preprocess_data(self, df):
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹...")
        
        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‡¦ç†
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values(['Code', 'Date']).reset_index(drop=True)
        
        # åŸºæœ¬çš„ãªæŠ€è¡“æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        df = self.calculate_technical_indicators(df)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®è¨ˆç®—ï¼ˆç¿Œæ—¥é«˜å€¤1%ä¸Šæ˜‡ï¼‰
        df = self.calculate_target(df)
        
        # NaNå€¤ã®å‡¦ç†
        df = df.dropna(subset=['Target'])
        
        logger.info(f"âœ… å‰å‡¦ç†å®Œäº†: {len(df):,}ä»¶, {df['Code'].nunique()}éŠ˜æŸ„")
        logger.info(f"æœŸé–“: {df['Date'].min().date()} ï½ {df['Date'].max().date()}")
        
        return df
    
    def calculate_technical_indicators(self, df):
        """æŠ€è¡“æŒ‡æ¨™ã®è¨ˆç®—"""
        logger.info("ğŸ“Š æŠ€è¡“æŒ‡æ¨™è¨ˆç®—ä¸­...")
        
        for code in df['Code'].unique():
            mask = df['Code'] == code
            code_data = df[mask].sort_values('Date')
            
            # ç§»å‹•å¹³å‡
            if 'MA_5' not in df.columns:
                df.loc[mask, 'MA_5'] = code_data['Close'].rolling(window=5).mean()
            if 'MA_20' not in df.columns:
                df.loc[mask, 'MA_20'] = code_data['Close'].rolling(window=20).mean()
            
            # RSI
            if 'RSI' not in df.columns:
                delta = code_data['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                rs = gain / loss
                df.loc[mask, 'RSI'] = 100 - (100 / (1 + rs))
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            if 'Volatility' not in df.columns:
                df.loc[mask, 'Volatility'] = code_data['Close'].pct_change().rolling(window=20).std()
            
            # ãƒªã‚¿ãƒ¼ãƒ³
            if 'Returns' not in df.columns:
                df.loc[mask, 'Returns'] = code_data['Close'].pct_change()
        
        # è¿½åŠ ç‰¹å¾´é‡
        df['Price_vs_MA5'] = df['Close'] / df['MA_5'] - 1
        df['Price_vs_MA20'] = df['Close'] / df['MA_20'] - 1
        df['MA5_vs_MA20'] = df['MA_5'] / df['MA_20'] - 1
        df['Volume_MA'] = df.groupby('Code')['Volume'].transform(lambda x: x.rolling(20).mean())
        df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']
        df['High_Low_Ratio'] = (df['High'] - df['Low']) / df['Close']
        
        logger.info("âœ… æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œäº†")
        return df
    
    def calculate_target(self, df):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®è¨ˆç®—"""
        logger.info("ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°è¨ˆç®—ä¸­...")
        
        df = df.sort_values(['Code', 'Date'])
        df['Next_High'] = df.groupby('Code')['High'].shift(-1)
        df['Target'] = ((df['Next_High'] / df['Close']) - 1 >= 0.01).astype(int)
        
        target_counts = df['Target'].value_counts()
        logger.info(f"âœ… ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: ä¸Šæ˜‡{target_counts.get(1, 0):,}ä»¶, éä¸Šæ˜‡{target_counts.get(0, 0):,}ä»¶")
        
        return df
    
    def prepare_features(self, df):
        """ç‰¹å¾´é‡æº–å‚™"""
        logger.info("ğŸ” ç‰¹å¾´é‡æº–å‚™ä¸­...")
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’é¸æŠ
        feature_candidates = [
            'MA_5', 'MA_20', 'RSI', 'Volatility', 'Returns',
            'Price_vs_MA5', 'Price_vs_MA20', 'MA5_vs_MA20',
            'Volume_Ratio', 'High_Low_Ratio'
        ]
        
        # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨
        available_features = [col for col in feature_candidates if col in df.columns]
        
        logger.info(f"åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {len(available_features)}å€‹")
        logger.info(f"ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {available_features}")
        
        return available_features
    
    def time_series_split_validation(self, df, feature_cols):
        """æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹æ¤œè¨¼"""
        logger.info("â° æ™‚ç³»åˆ—åˆ†å‰²ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df.sort_values('Date')
        
        # æœ€å¾Œã®30æ—¥é–“ã‚’ãƒ†ã‚¹ãƒˆæœŸé–“ã¨ã™ã‚‹
        test_start_date = df_sorted['Date'].max() - timedelta(days=30)
        train_df = df_sorted[df_sorted['Date'] < test_start_date]
        test_df = df_sorted[df_sorted['Date'] >= test_start_date]
        
        logger.info(f"è¨“ç·´æœŸé–“: {train_df['Date'].min().date()} ï½ {train_df['Date'].max().date()}")
        logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {test_df['Date'].min().date()} ï½ {test_df['Date'].max().date()}")
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,}ä»¶")
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}ä»¶")
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
        X_train = train_df[feature_cols].fillna(0)
        y_train = train_df['Target']
        X_test = test_df[feature_cols].fillna(0)
        y_test = test_df['Target']
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆä¸Šä½8ç‰¹å¾´é‡ï¼‰
        self.feature_selector = SelectKBest(score_func=f_classif, k=min(8, len(feature_cols)))
        X_train_selected = self.feature_selector.fit_transform(X_train, y_train)
        X_test_selected = self.feature_selector.transform(X_test)
        
        # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡åã‚’å–å¾—
        selected_features = np.array(feature_cols)[self.feature_selector.get_support()]
        logger.info(f"é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {list(selected_features)}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_train_scaled = self.scaler.fit_transform(X_train_selected)
        X_test_scaled = self.scaler.transform(X_test_selected)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        logger.info("ğŸ¤– LightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        self.model = lgb.LGBMClassifier(**self.model_params)
        self.model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        return test_df, y_pred_proba, selected_features
    
    def evaluate_top_k_strategy(self, test_df, y_pred_proba, k=3):
        """ä¸Šä½KéŠ˜æŸ„é¸æŠæˆ¦ç•¥ã®è©•ä¾¡"""
        logger.info(f"ğŸ“Š ä¸Šä½{k}éŠ˜æŸ„æˆ¦ç•¥è©•ä¾¡é–‹å§‹...")
        
        results = []
        
        # æ—¥ä»˜ã”ã¨ã«è©•ä¾¡
        for date in test_df['Date'].unique():
            date_df = test_df[test_df['Date'] == date].copy()
            date_proba = y_pred_proba[test_df['Date'] == date]
            
            if len(date_df) < k:
                continue
            
            # ä¸Šä½KéŠ˜æŸ„ã‚’é¸æŠ
            top_k_indices = np.argsort(date_proba)[-k:]
            selected_targets = date_df.iloc[top_k_indices]['Target'].values
            
            # ç²¾åº¦è¨ˆç®—
            precision = np.mean(selected_targets)
            results.append({
                'date': date,
                'precision': precision,
                'predictions': len(selected_targets),
                'hits': np.sum(selected_targets)
            })
        
        # å…¨ä½“çµ±è¨ˆ
        overall_precision = np.mean([r['precision'] for r in results])
        total_predictions = sum([r['predictions'] for r in results])
        total_hits = sum([r['hits'] for r in results])
        
        logger.info("="*60)
        logger.info("ğŸ¯ ä¸Šä½3éŠ˜æŸ„æˆ¦ç•¥ - æœ€çµ‚çµæœ")
        logger.info("="*60)
        logger.info(f"ğŸ“Š ç·åˆç²¾åº¦: {overall_precision:.4f} ({overall_precision*100:.2f}%)")
        logger.info(f"ğŸ“ˆ ç·äºˆæ¸¬æ•°: {total_predictions}ä»¶")
        logger.info(f"âœ… çš„ä¸­æ•°: {total_hits}ä»¶")
        logger.info(f"ğŸ“… è©•ä¾¡æ—¥æ•°: {len(results)}æ—¥")
        logger.info(f"ğŸ¯ æ—¥å¹³å‡ç²¾åº¦: {np.mean([r['precision'] for r in results]):.4f}")
        logger.info(f"ğŸ“Š ç²¾åº¦æ¨™æº–åå·®: {np.std([r['precision'] for r in results]):.4f}")
        
        # ç›®æ¨™é”æˆç¢ºèª
        if overall_precision >= 0.60:
            logger.info("ğŸ‰ ç›®æ¨™ç²¾åº¦60%ã‚’é”æˆã—ã¾ã—ãŸï¼")
        else:
            logger.info(f"âš ï¸ ç›®æ¨™ç²¾åº¦60%ã«ã¯{0.60 - overall_precision:.4f}ãƒã‚¤ãƒ³ãƒˆä¸è¶³")
        
        return overall_precision, results
    
    def save_model_and_results(self, precision, selected_features, results):
        """ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜"""
        logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_dir = Path("data/models")
        model_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = model_dir / f"enhanced_precision_model_{timestamp}.joblib"
        
        model_package = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_selector': self.feature_selector,
            'selected_features': selected_features,
            'precision': precision,
            'timestamp': timestamp
        }
        
        joblib.dump(model_package, model_path)
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        
        # çµæœä¿å­˜
        results_dir = Path("results/enhanced_precision")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        results_path = results_dir / f"enhanced_precision_results_{timestamp}.json"
        import json
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump({
                'overall_precision': precision,
                'selected_features': list(selected_features),
                'daily_results': results,
                'timestamp': timestamp
            }, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"âœ… çµæœä¿å­˜å®Œäº†: {results_path}")
        
        return model_path, results_path
    
    def run_enhanced_precision_test(self):
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸç²¾åº¦ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ç²¾åº¦ãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info("="*60)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.load_enhanced_data()
            
            # ç‰¹å¾´é‡æº–å‚™
            feature_cols = self.prepare_features(df)
            
            # æ™‚ç³»åˆ—åˆ†å‰²æ¤œè¨¼
            test_df, y_pred_proba, selected_features = self.time_series_split_validation(df, feature_cols)
            
            # ä¸Šä½3éŠ˜æŸ„æˆ¦ç•¥è©•ä¾¡
            precision, results = self.evaluate_top_k_strategy(test_df, y_pred_proba, k=3)
            
            # çµæœä¿å­˜
            model_path, results_path = self.save_model_and_results(precision, selected_features, results)
            
            logger.info("="*60)
            logger.info("ğŸ‰ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ç²¾åº¦ãƒ†ã‚¹ãƒˆå®Œäº†")
            logger.info("="*60)
            
            return precision, model_path, results_path
            
        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹é«˜ç²¾åº¦AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    system = EnhancedPrecisionSystem()
    precision, model_path, results_path = system.run_enhanced_precision_test()
    
    logger.info(f"ğŸ¯ æœ€çµ‚ç²¾åº¦: {precision:.4f} ({precision*100:.2f}%)")
    logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«: {model_path}")
    logger.info(f"ğŸ“Š çµæœ: {results_path}")


if __name__ == "__main__":
    main()