#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
å¤–éƒ¨æŒ‡æ¨™å•é¡Œã‚’å›é¿ã—ã€å®Ÿè£…æ¸ˆã¿æ”¹å–„è¦ç´ ã‚’æ´»ç”¨ã—ãŸå®‰å®šç‰ˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import os
import joblib
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# MLé–¢é€£
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimplifiedEnhancedTrainer:
    """ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, data_file: str = None):
        """åˆæœŸåŒ–"""
        if data_file is None:
            data_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
        
        self.data_file = data_file
        self.df = None
        self.models = {}
        self.feature_cols = None
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file}")
    
    def load_and_enhance_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ‹¡å¼µç‰¹å¾´é‡ä½œæˆ"""
        logger.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ‹¡å¼µç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        try:
            self.df = pd.read_parquet(self.data_file)
            logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df):,}ä»¶, {self.df['Code'].nunique()}éŠ˜æŸ„")
            
            # æ—¥ä»˜å‹å¤‰æ›
            self.df['Date'] = pd.to_datetime(self.df['Date'])
            
            # ğŸ†• æ‹¡å¼µç‰¹å¾´é‡ä½œæˆï¼ˆå¤–éƒ¨æŒ‡æ¨™ãªã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰
            enhanced_df = self.df.copy()
            result_dfs = []
            
            for code in enhanced_df['Code'].unique():
                code_df = enhanced_df[enhanced_df['Code'] == code].copy()
                code_df = code_df.sort_values('Date')
                
                # æ—¢å­˜ç‰¹å¾´é‡ã®æ‹¡å¼µ
                code_df['Returns'] = code_df['Close'].pct_change(fill_method=None)
                code_df['Volume_MA_20'] = code_df['Volume'].rolling(20).mean()
                code_df['Price_Volume_Trend'] = code_df['Returns'] * code_df['Volume']
                
                # ğŸ†• è¿½åŠ ç§»å‹•å¹³å‡
                for window in [3, 7, 10, 25, 50, 75, 100]:
                    code_df[f'MA_{window}'] = code_df['Close'].rolling(window).mean()
                    code_df[f'MA_{window}_ratio'] = code_df['Close'] / code_df[f'MA_{window}']
                    code_df[f'MA_{window}_slope'] = code_df[f'MA_{window}'].diff(5)
                
                # ğŸ†• æ‹¡å¼µãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                for window in [3, 7, 10, 15, 30]:
                    code_df[f'Volatility_{window}'] = code_df['Returns'].rolling(window).std()
                    code_df[f'Returns_zscore_{window}'] = (code_df['Returns'] - code_df['Returns'].rolling(window).mean()) / code_df['Returns'].rolling(window).std()
                
                # ğŸ†• æ‹¡å¼µRSI
                for window in [5, 9, 14, 21, 28]:
                    delta = code_df['Close'].diff()
                    gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                    rs = gain / loss
                    code_df[f'RSI_{window}'] = 100 - (100 / (1 + rs))
                
                # ğŸ†• è¤‡æ•°ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
                for window in [10, 20, 30]:
                    for std_mult in [1, 2, 2.5]:
                        rolling_mean = code_df['Close'].rolling(window).mean()
                        rolling_std = code_df['Close'].rolling(window).std()
                        code_df[f'BB_upper_{window}_{std_mult}'] = rolling_mean + (rolling_std * std_mult)
                        code_df[f'BB_lower_{window}_{std_mult}'] = rolling_mean - (rolling_std * std_mult)
                        code_df[f'BB_ratio_{window}_{std_mult}'] = (code_df['Close'] - code_df[f'BB_lower_{window}_{std_mult}']) / (code_df[f'BB_upper_{window}_{std_mult}'] - code_df[f'BB_lower_{window}_{std_mult}'])
                
                # ğŸ†• MACDå¤‰ç¨®
                for fast, slow, signal in [(8, 21, 5), (12, 26, 9), (19, 39, 9)]:
                    exp1 = code_df['Close'].ewm(span=fast, adjust=False).mean()
                    exp2 = code_df['Close'].ewm(span=slow, adjust=False).mean()
                    code_df[f'MACD_{fast}_{slow}'] = exp1 - exp2
                    code_df[f'MACD_signal_{fast}_{slow}_{signal}'] = code_df[f'MACD_{fast}_{slow}'].ewm(span=signal, adjust=False).mean()
                    code_df[f'MACD_histogram_{fast}_{slow}_{signal}'] = code_df[f'MACD_{fast}_{slow}'] - code_df[f'MACD_signal_{fast}_{slow}_{signal}']
                
                # ğŸ†• OBVå¤‰ç¨®
                obv_volume = code_df['Volume'] * np.where(code_df['Close'] > code_df['Close'].shift(1), 1, 
                                                         np.where(code_df['Close'] < code_df['Close'].shift(1), -1, 0))
                code_df['OBV'] = obv_volume.cumsum()
                code_df['OBV_MA_10'] = code_df['OBV'].rolling(10).mean()
                code_df['OBV_ratio'] = code_df['OBV'] / code_df['OBV_MA_10']
                
                # ğŸ†• ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹å¤‰ç¨®
                for window in [9, 14, 21]:
                    low_min = code_df['Low'].rolling(window).min()
                    high_max = code_df['High'].rolling(window).max()
                    code_df[f'Stoch_K_{window}'] = 100 * (code_df['Close'] - low_min) / (high_max - low_min)
                    code_df[f'Stoch_D_{window}'] = code_df[f'Stoch_K_{window}'].rolling(3).mean()
                
                # ğŸ†• ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
                code_df['High_Low_ratio'] = code_df['High'] / code_df['Low']
                code_df['Open_Close_ratio'] = code_df['Open'] / code_df['Close']
                code_df['Volume_price_ratio'] = code_df['Volume'] / code_df['Close']
                
                # ğŸ†• ãƒªã‚¿ãƒ¼ãƒ³ç³»ç‰¹å¾´é‡
                for period in [2, 3, 5, 10, 20]:
                    code_df[f'Returns_{period}d'] = code_df['Close'].pct_change(period)
                    code_df[f'Max_return_{period}d'] = code_df['Returns'].rolling(period).max()
                    code_df[f'Min_return_{period}d'] = code_df['Returns'].rolling(period).min()
                
                result_dfs.append(code_df)
            
            # çµåˆ
            enhanced_df = pd.concat(result_dfs, ignore_index=True)
            
            # ç›®çš„å¤‰æ•°ä½œæˆ
            logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
            enhanced_df['Target'] = 0
            
            for code in enhanced_df['Code'].unique():
                mask = enhanced_df['Code'] == code
                code_data = enhanced_df[mask].copy()
                next_high = code_data['High'].shift(-1)
                prev_close = code_data['Close'].shift(1)
                enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
            
            # æ¬ æå€¤å‡¦ç†
            logger.info("æ¬ æå€¤å‡¦ç†...")
            enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
            enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
            enhanced_df = enhanced_df.fillna(method='ffill').fillna(method='bfill')
            enhanced_df = enhanced_df.dropna()
            
            self.df = enhanced_df
            logger.info(f"æ‹¡å¼µç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶")
            
            positive_rate = enhanced_df['Target'].mean()
            logger.info(f"æ­£ä¾‹ç‡: {positive_rate:.3f} ({positive_rate:.1%})")
            
            return True
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def select_optimal_validation_period(self):
        """ğŸ†• å­£ç¯€æ€§è€ƒæ…®ã—ãŸæœ€é©æ¤œè¨¼æœŸé–“é¸æŠ"""
        logger.info("å­£ç¯€æ€§è€ƒæ…®ã—ãŸæœ€é©æ¤œè¨¼æœŸé–“é¸æŠ...")
        
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        
        # å€™è£œæœŸé–“å®šç¾©
        validation_periods = [
            {
                'name': '7æœˆå®‰å®šæœŸ',
                'start_days': 70,
                'end_days': 35,
                'description': 'å¤æ¯ã‚Œå‰ã®å®‰å®šæœŸé–“'
            },
            {
                'name': '10æœˆå®‰å®šæœŸ',
                'start_days': 120,
                'end_days': 90,
                'description': 'ç§‹ã®å®‰å®šã—ãŸå–å¼•æœŸé–“'
            },
            {
                'name': '1æœˆæ–°å¹´æœŸ',
                'start_days': 250,
                'end_days': 220,
                'description': 'æ–°å¹´ã®æ´»ç™ºãªå–å¼•æœŸé–“'
            }
        ]
        
        best_period = None
        best_score = 0
        
        for period in validation_periods:
            test_start = latest_date - timedelta(days=period['start_days'])
            test_end = latest_date - timedelta(days=period['end_days'])
            
            period_data = df_sorted[
                (df_sorted['Date'] >= test_start) & 
                (df_sorted['Date'] <= test_end)
            ]
            
            if len(period_data) < 1000:
                continue
            
            period_volatility = period_data.groupby('Date')['Returns'].std().mean()
            positive_rate = period_data['Target'].mean()
            balance_score = 1 - abs(positive_rate - 0.5)
            
            stability_score = 1 / (period_volatility + 0.001)
            data_volume_score = min(len(period_data) / 3000, 1.0)
            total_score = stability_score * balance_score * data_volume_score
            
            logger.info(f"{period['name']}: {test_start.date()} - {test_end.date()}, ã‚¹ã‚³ã‚¢: {total_score:.4f}")
            
            if total_score > best_score:
                best_score = total_score
                best_period = {
                    **period,
                    'start_date': test_start,
                    'end_date': test_end,
                    'score': total_score,
                    'data_count': len(period_data)
                }
        
        if best_period:
            logger.info(f"ğŸ¯ æœ€é©æ¤œè¨¼æœŸé–“: {best_period['name']}")
            logger.info(f"æœŸé–“: {best_period['start_date'].date()} - {best_period['end_date'].date()}")
        
        return best_period
    
    def create_ensemble_models(self):
        """ğŸ†• ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ...")
        
        models = {
            'lightgbm_v1': LGBMClassifier(
                objective='binary',
                n_estimators=400,
                max_depth=8,
                learning_rate=0.03,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                verbose=-1
            ),
            'lightgbm_v2': LGBMClassifier(
                objective='binary',
                n_estimators=600,
                max_depth=10,
                learning_rate=0.02,
                subsample=0.85,
                colsample_bytree=0.7,
                reg_alpha=0.15,
                reg_lambda=0.15,
                random_state=123,
                verbose=-1
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=250,
                max_depth=12,
                min_samples_split=8,
                min_samples_leaf=4,
                max_features=0.8,
                random_state=42,
                n_jobs=-1
            ),
            'xgboost': xgb.XGBClassifier(
                objective='binary:logistic',
                n_estimators=350,
                max_depth=7,
                learning_rate=0.04,
                subsample=0.8,
                colsample_bytree=0.75,
                reg_alpha=0.12,
                reg_lambda=0.12,
                random_state=42,
                eval_metric='logloss'
            )
        }
        
        return models
    
    def train_and_validate_ensemble(self, validation_period):
        """ğŸ†• ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ»æ¤œè¨¼"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ»æ¤œè¨¼é–‹å§‹...")
        
        # ç‰¹å¾´é‡æº–å‚™
        exclude_cols = ['Date', 'Code', 'CompanyName', 'MatchMethod', 'ApiCode', 'Target']
        self.feature_cols = [col for col in self.df.columns if col not in exclude_cols]
        numeric_cols = self.df[self.feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        self.feature_cols = numeric_cols
        
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(self.feature_cols)}")
        
        X = self.df[self.feature_cols]
        y = self.df['Target']
        
        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        df_sorted = self.df.sort_values('Date')
        
        if validation_period:
            test_start = validation_period['start_date']
            test_end = validation_period['end_date']
        else:
            test_end = df_sorted['Date'].max()
            test_start = test_end - timedelta(days=30)
        
        logger.info(f"è¨“ç·´æœŸé–“: ã€œ {test_start.date()}")
        logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {test_start.date()} ã€œ {test_end.date()}")
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = (df_sorted['Date'] >= test_start) & (df_sorted['Date'] <= test_end)
        
        X_train = X[train_mask]
        y_train = y[train_mask]
        X_test = X[test_mask]
        y_test = y[test_mask]
        
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ä»¶")
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test):,}ä»¶")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
        base_models = self.create_ensemble_models()
        trained_models = {}
        
        for name, model in base_models.items():
            logger.info(f"{name}å­¦ç¿’é–‹å§‹...")
            
            # ç‰¹å¾´é‡é¸æŠï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
            k_features = {
                'lightgbm_v1': 35,
                'lightgbm_v2': 40, 
                'random_forest': 30,
                'xgboost': 32
            }
            
            selector = SelectKBest(score_func=f_classif, k=k_features[name])
            X_train_selected = selector.fit_transform(X_train, y_train)
            X_test_selected = selector.transform(X_test)
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = RobustScaler() if 'lightgbm' in name or 'xgb' in name else StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_selected)
            X_test_scaled = scaler.transform(X_test_selected)
            
            # å­¦ç¿’
            model.fit(X_train_scaled, y_train)
            
            trained_models[name] = {
                'model': model,
                'scaler': scaler,
                'selector': selector
            }
        
        self.models = trained_models
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡
        return self.evaluate_ensemble(df_sorted[test_mask], X_test, y_test)
    
    def evaluate_ensemble(self, test_df, X_test, y_test):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡"""
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡é–‹å§‹...")
        
        # å„ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        model_predictions = {}
        for name, model_data in self.models.items():
            model = model_data['model']
            scaler = model_data['scaler']
            selector = model_data['selector']
            
            X_selected = selector.transform(X_test)
            X_scaled = scaler.transform(X_selected)
            pred_proba = model.predict_proba(X_scaled)[:, 1]
            model_predictions[name] = pred_proba
        
        # ğŸ†• ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆåŠ é‡å¹³å‡ï¼‰
        weights = {
            'lightgbm_v1': 0.3,
            'lightgbm_v2': 0.3,
            'random_forest': 0.2,
            'xgboost': 0.2
        }
        
        ensemble_proba = np.zeros(len(X_test))
        for name, proba in model_predictions.items():
            ensemble_proba += weights[name] * proba
        
        # æ—¥åˆ¥ç²¾åº¦è©•ä¾¡
        test_df_copy = test_df.copy()
        test_df_copy['EnsemblePredProba'] = ensemble_proba
        
        unique_dates = sorted(test_df_copy['Date'].unique())
        daily_results = []
        ensemble_stats = {'total_correct': 0, 'total_predictions': 0}
        individual_stats = {name: {'total_correct': 0, 'total_predictions': 0} for name in model_predictions.keys()}
        
        logger.info(f"æ¤œè¨¼æœŸé–“: {unique_dates[0].date()} ã€œ {unique_dates[-1].date()} ({len(unique_dates)}å–¶æ¥­æ—¥)")
        
        for test_date in unique_dates:
            daily_data = test_df_copy[test_df_copy['Date'] == test_date]
            
            if len(daily_data) < 3:
                continue
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆä¸Šä½3éŠ˜æŸ„ï¼‰
            top3_ensemble = daily_data['EnsemblePredProba'].nlargest(3).index
            ensemble_results_daily = daily_data.loc[top3_ensemble]['Target'].values
            ensemble_correct = np.sum(ensemble_results_daily)
            ensemble_total = len(ensemble_results_daily)
            
            ensemble_stats['total_correct'] += ensemble_correct
            ensemble_stats['total_predictions'] += ensemble_total
            
            ensemble_precision = ensemble_correct / ensemble_total
            
            daily_results.append({
                'date': test_date,
                'ensemble_correct': ensemble_correct,
                'ensemble_total': ensemble_total,
                'ensemble_precision': ensemble_precision,
                'selected_codes': daily_data.loc[top3_ensemble]['Code'].tolist()
            })
            
            logger.info(f"{test_date.strftime('%Y-%m-%d')}: {ensemble_correct}/{ensemble_total}={ensemble_precision:.1%} "
                       f"[{', '.join(daily_data.loc[top3_ensemble]['Code'].astype(str).tolist())}]")
        
        # ç·åˆç²¾åº¦
        ensemble_overall = ensemble_stats['total_correct'] / ensemble_stats['total_predictions']
        
        logger.info(f"\nğŸ‰ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼çµæœ:")
        logger.info(f"æ¤œè¨¼å–¶æ¥­æ—¥æ•°: {len(daily_results)}æ—¥é–“")
        logger.info(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {ensemble_overall:.4f} ({ensemble_overall:.2%})")
        
        return {
            'ensemble_precision': ensemble_overall,
            'daily_results': daily_results,
            'ensemble_stats': ensemble_stats,
            'n_days': len(daily_results)
        }
    
    def save_models(self, results, validation_period):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        precision_str = f"{results['ensemble_precision']:.4f}".replace('.', '')
        
        os.makedirs("models/simplified_enhanced", exist_ok=True)
        
        model_file = f"models/simplified_enhanced/simplified_enhanced_model_{len(self.df)}records_{precision_str}precision_{timestamp}.joblib"
        
        model_data = {
            'models': self.models,
            'feature_cols': self.feature_cols,
            'ensemble_precision': results['ensemble_precision'],
            'results': results,
            'validation_period': validation_period,
            'improvements': [
                'extended_technical_indicators',
                'seasonal_validation_period_optimization', 
                'ensemble_learning_4models',
                'enhanced_feature_engineering'
            ]
        }
        
        joblib.dump(model_data, model_file)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_file
    
    def run_simplified_enhanced_training(self):
        """ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’å®Ÿè¡Œ"""
        logger.info("ğŸš€ ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            if not self.load_and_enhance_data():
                return None
            
            # æœ€é©æ¤œè¨¼æœŸé–“é¸æŠ
            validation_period = self.select_optimal_validation_period()
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ»æ¤œè¨¼
            results = self.train_and_validate_ensemble(validation_period)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_file = self.save_models(results, validation_period)
            
            # çµæœã‚µãƒãƒªãƒ¼
            logger.info(f"\nğŸ¯ ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’æœ€çµ‚çµæœ:")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(self.df):,}ä»¶ ({self.df['Code'].nunique()}éŠ˜æŸ„)")
            logger.info(f"æ‹¡å¼µç‰¹å¾´é‡æ•°: {len(self.feature_cols)}")
            logger.info(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {results['ensemble_precision']:.4f} ({results['ensemble_precision']:.2%})")
            logger.info(f"æ¤œè¨¼æœŸé–“: {results['n_days']}å–¶æ¥­æ—¥")
            logger.info(f"æœ€é©æœŸé–“: {validation_period['name'] if validation_period else 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}")
            logger.info(f"ä¿å­˜å…ˆ: {model_file}")
            
            return results
            
        except Exception as e:
            logger.error(f"ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    trainer = SimplifiedEnhancedTrainer()
    results = trainer.run_simplified_enhanced_training()
    
    if results:
        print(f"\nâœ… ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’ãƒ»æ¤œè¨¼å®Œäº†!")
        print(f"ğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {results['ensemble_precision']:.2%}")
        print(f"ğŸ“ˆ æ”¹å–„è¦ç´ : æ‹¡å¼µç‰¹å¾´é‡ + å­£ç¯€æ€§æœ€é©åŒ– + ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«4ãƒ¢ãƒ‡ãƒ«")
        print(f"ğŸ“… æ¤œè¨¼æœŸé–“: {results['n_days']}å–¶æ¥­æ—¥é–“")
    else:
        print("\nâŒ ç°¡æ˜“ç‰ˆå¼·åŒ–å­¦ç¿’ãƒ»æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()