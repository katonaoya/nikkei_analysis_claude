#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Precision System V3 Legacy (æ€§èƒ½æ¯”è¼ƒç”¨)
ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å‰ã®æ€§èƒ½ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®æ¤œè¨¼ç‰ˆ
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
from datetime import datetime, timedelta
import logging
import warnings
import os
from pathlib import Path

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LegacyPerformanceTest:
    """æ—§ç‰ˆæ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆå°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ¯”è¼ƒæ¤œè¨¼ï¼‰"""
    
    def __init__(self):
        self.output_dir = Path("models/legacy_test")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def load_and_prepare_test_data(self) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæœ€å°é™ï¼‰"""
        logger.info("ğŸ” ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        stock_file = "data/processed/nikkei225_complete_225stocks_20250915_200849.parquet"
        external_file = "data/processed/enhanced_integrated_data.parquet"
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        stock_df = pd.read_parquet(stock_file)
        external_df = pd.read_parquet(external_file)
        
        # å°è¦æ¨¡ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ„å›³çš„ã«åˆ¶å¾¡ï¼‰
        stock_df = stock_df.head(50000)  # 5ä¸‡ä»¶
        external_df = external_df.head(10000)  # 1ä¸‡ä»¶
        
        logger.info(f"ãƒ†ã‚¹ãƒˆç”¨æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶")
        logger.info(f"ãƒ†ã‚¹ãƒˆç”¨å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_df):,}ä»¶")
        
        return stock_df, external_df
    
    def create_legacy_features(self, stock_df: pd.DataFrame, external_df: pd.DataFrame) -> pd.DataFrame:
        """æ—§ç‰ˆã®ç‰¹å¾´é‡ä½œæˆï¼ˆå…¨ç‰¹å¾´é‡ã€å…¨æœŸé–“ï¼‰"""
        logger.info("ğŸ”§ Legacyç‰¹å¾´é‡ä½œæˆé–‹å§‹...")
        
        # å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªè¨±å¯ç¯„å›²å†…ã§ï¼‰
        enhanced_df = stock_df.copy()
        
        # å…¨éŠ˜æŸ„ã«å¯¾ã—ã¦ãƒ•ãƒ«ç‰¹å¾´é‡ä½œæˆ
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy().sort_values('Date')
            
            if len(code_data) < 20:
                continue
                
            # å…¨ç‰¹å¾´é‡ï¼ˆæ—§ç‰ˆä»•æ§˜ï¼‰
            code_data['Returns'] = code_data['Close'].pct_change()
            code_data['Log_Returns'] = np.log(code_data['Close'] / code_data['Close'].shift(1))
            code_data['High_Low_Ratio'] = code_data['High'] / code_data['Low']
            
            # ç§»å‹•å¹³å‡ï¼ˆå…¨æœŸé–“ï¼‰
            for window in [5, 10, 20, 50]:
                code_data[f'MA_{window}'] = code_data['Close'].rolling(window).mean()
                code_data[f'MA_{window}_ratio'] = code_data['Close'] / code_data[f'MA_{window}']
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå…¨æœŸé–“ï¼‰
            for window in [5, 10, 20]:
                code_data[f'Volatility_{window}'] = code_data['Returns'].rolling(window).std()
            
            # RSIï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            for window in [14, 21, 30]:
                delta = code_data['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_data[f'RSI_{window}'] = 100 - (100 / (1 + rs))
            
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            for window in [10, 20]:
                rolling_mean = code_data['Close'].rolling(window).mean()
                rolling_std = code_data['Close'].rolling(window).std()
                code_data[f'BB_{window}_upper'] = rolling_mean + (rolling_std * 2)
                code_data[f'BB_{window}_lower'] = rolling_mean - (rolling_std * 2)
                code_data[f'BB_{window}_ratio'] = (code_data['Close'] - code_data[f'BB_{window}_lower']) / (code_data[f'BB_{window}_upper'] - code_data[f'BB_{window}_lower'])
            
            # MACD
            exp1 = code_data['Close'].ewm(span=12).mean()
            exp2 = code_data['Close'].ewm(span=26).mean()
            code_data['MACD'] = exp1 - exp2
            code_data['MACD_signal'] = code_data['MACD'].ewm(span=9).mean()
            code_data['MACD_histogram'] = code_data['MACD'] - code_data['MACD_signal']
            
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            for window in [10, 20, 50]:
                code_data[f'Volume_MA_{window}'] = code_data['Volume'].rolling(window).mean()
                code_data[f'Volume_ratio_{window}'] = code_data['Volume'] / code_data[f'Volume_MA_{window}']
            
            enhanced_df.loc[mask] = code_data
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆ
        enhanced_df['Target'] = 0
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # æ¬ æå€¤å‡¦ç†
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
        
        logger.info(f"Legacyç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶, {len(enhanced_df.columns)}ã‚«ãƒ©ãƒ ")
        return enhanced_df
    
    def test_legacy_performance(self, df: pd.DataFrame) -> dict:
        """æ—§ç‰ˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ¯ Legacyæ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ç‰¹å¾´é‡æº–å‚™ï¼ˆå…¨ç‰¹å¾´é‡ä½¿ç”¨ï¼‰
        feature_cols = [col for col in df.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       df[col].dtype in ['int64', 'float64']]
        
        X = df[feature_cols].fillna(method='ffill').fillna(0)
        y = df['Target']
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        split_idx = int(len(df) * 0.8)
        X_train = X[:split_idx]
        X_test = X[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆæ—§ç‰ˆã¯å¤šãã®ç‰¹å¾´é‡ä½¿ç”¨ï¼‰
        selector = SelectKBest(score_func=f_classif, k=min(50, len(feature_cols)))
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆæ—§ç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        model = lgb.LGBMClassifier(
            objective='binary',
            n_estimators=300,  # å¤šã„
            max_depth=8,       # æ·±ã„
            learning_rate=0.03, # ä½ã„
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbose=-1
        )
        
        model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        result = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'feature_count': len(feature_cols),
            'selected_features': X_train_selected.shape[1],
            'train_size': len(X_train),
            'test_size': len(X_test),
            'data_size': len(df)
        }
        
        logger.info(f"ğŸ¯ Legacyæ€§èƒ½çµæœ:")
        logger.info(f"  ç²¾åº¦: {accuracy:.4f}")
        logger.info(f"  é©åˆç‡: {precision:.4f}")
        logger.info(f"  å†ç¾ç‡: {recall:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
        logger.info(f"  å…¨ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
        logger.info(f"  é¸æŠç‰¹å¾´é‡æ•°: {X_train_selected.shape[1]}")
        
        return result

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    logger.info("ğŸ”¬ Legacy vs æœ€é©åŒ–ç‰ˆ æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    tester = LegacyPerformanceTest()
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    stock_df, external_df = tester.load_and_prepare_test_data()
    
    # Legacyç‰¹å¾´é‡ä½œæˆ
    legacy_df = tester.create_legacy_features(stock_df, external_df)
    
    # Legacyæ€§èƒ½ãƒ†ã‚¹ãƒˆ
    legacy_result = tester.test_legacy_performance(legacy_df)
    
    logger.info("ğŸ‰ Legacyæ€§èƒ½ãƒ†ã‚¹ãƒˆå®Œäº†")
    logger.info(f"æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {legacy_result['data_size']:,}ä»¶ã§æ¤œè¨¼")