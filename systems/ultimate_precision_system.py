#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Á©∂Ê•µ„ÅÆÈ´òÁ≤æÂ∫¶ÂÆüÁèæ„Ç∑„Çπ„ÉÜ„É† - „ÅÇ„Çâ„ÇÜ„ÇãË©¶Ë°åÈåØË™§„ÇíÂÆüË£Ö
ÂÆåÂÖ®ÂÆü„Éá„Éº„Çø„Åß„ÅÆÊúÄÈ´òÁ≤æÂ∫¶„ÇíÁõÆÊåá„ÅôÂåÖÊã¨ÁöÑÊ©üÊ¢∞Â≠¶Áøí„Ç∑„Çπ„ÉÜ„É†
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import os
import joblib
import warnings
warnings.filterwarnings('ignore')

# MLÈñ¢ÈÄ£
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel
from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from lightgbm import LGBMClassifier
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier, BaggingClassifier
from sklearn.calibration import CalibratedClassifierCV
import optuna

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UltimatePrecisionSystem:
    """Á©∂Ê•µ„ÅÆÈ´òÁ≤æÂ∫¶ÂÆüÁèæ„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self, data_file: str = None):
        """ÂàùÊúüÂåñ"""
        if data_file is None:
            data_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
        
        self.data_file = data_file
        self.df = None
        self.models = {}
        self.best_model = None
        self.best_score = 0.0
        self.experiment_results = []
        
        logger.info("üéØ Á©∂Ê•µ„ÅÆÈ´òÁ≤æÂ∫¶ÂÆüÁèæ„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñÂÆå‰∫Ü")
        logger.info(f"„Éá„Éº„Çø„Éï„Ç°„Ç§„É´: {data_file}")
    
    def load_data(self):
        """„Éá„Éº„ÇøË™≠„ÅøËæº„Åø"""
        logger.info("„Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÈñãÂßã...")
        
        self.df = pd.read_parquet(self.data_file)
        self.df['Date'] = pd.to_datetime(self.df['Date'])
        
        logger.info(f"‚úÖ „Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÂÆå‰∫Ü: {len(self.df):,}‰ª∂, {self.df['Code'].nunique()}ÈäòÊüÑ")
        return True
    
    def create_ultimate_features(self):
        """üî• Á©∂Ê•µ„ÅÆÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞"""
        logger.info("üî• Á©∂Ê•µ„ÅÆÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞ÈñãÂßã...")
        
        enhanced_df = self.df.copy()
        result_dfs = []
        
        for code in enhanced_df['Code'].unique():
            code_df = enhanced_df[enhanced_df['Code'] == code].copy()
            code_df = code_df.sort_values('Date')
            
            # üÜï Âü∫Êú¨„É™„Çø„Éº„É≥Á≥ªÁâπÂæ¥ÈáèÔºàË§áÊï∞ÊúüÈñìÔºâ
            for period in [1, 2, 3, 5, 10, 20, 30]:
                code_df[f'Returns_{period}d'] = code_df['Close'].pct_change(period)
                code_df[f'LogReturns_{period}d'] = np.log(code_df['Close'] / code_df['Close'].shift(period))
            
            # üÜï Êã°ÂºµÁßªÂãïÂπ≥ÂùáÔºà11Á®ÆÈ°ûÔºâ
            windows = [3, 5, 7, 10, 15, 20, 25, 30, 50, 75, 100]
            for window in windows:
                code_df[f'MA_{window}'] = code_df['Close'].rolling(window).mean()
                code_df[f'MA_{window}_ratio'] = code_df['Close'] / code_df[f'MA_{window}']
                code_df[f'MA_{window}_slope'] = code_df[f'MA_{window}'].diff(5)
                code_df[f'MA_{window}_distance'] = (code_df['Close'] - code_df[f'MA_{window}']) / code_df['Close']
            
            # üÜï MA‰∫§Â∑Æ„Ç∑„Ç∞„Éä„É´
            code_df['MA_5_20_cross'] = np.where(code_df['MA_5'] > code_df['MA_20'], 1, 0)
            code_df['MA_10_30_cross'] = np.where(code_df['MA_10'] > code_df['MA_30'], 1, 0)
            code_df['MA_20_50_cross'] = np.where(code_df['MA_20'] > code_df['MA_50'], 1, 0)
            
            # üÜï Êã°ÂºµEMA
            for window in [5, 10, 20, 30, 50]:
                code_df[f'EMA_{window}'] = code_df['Close'].ewm(span=window).mean()
                code_df[f'EMA_{window}_ratio'] = code_df['Close'] / code_df[f'EMA_{window}']
            
            # üÜï „Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£Á≥ªÔºà7Á®ÆÈ°ûÔºâ
            for window in [5, 10, 15, 20, 30, 60, 120]:
                code_df[f'Volatility_{window}'] = code_df['Returns_1d'].rolling(window).std()
                code_df[f'VolatilityRank_{window}'] = code_df[f'Volatility_{window}'].rolling(252).rank() / 252
            
            # üÜï „É™„Çø„Éº„É≥Áµ±Ë®àÔºàZ-Score, PercentileÔºâ
            for window in [10, 20, 50, 100]:
                returns_rolling = code_df['Returns_1d'].rolling(window)
                code_df[f'Returns_zscore_{window}'] = (code_df['Returns_1d'] - returns_rolling.mean()) / returns_rolling.std()
                code_df[f'Returns_percentile_{window}'] = code_df['Returns_1d'].rolling(window).rank() / window
            
            # üÜï RSIÂ§âÁ®ÆÔºà5Á®ÆÈ°ûÔºâ
            for window in [5, 9, 14, 21, 28]:
                delta = code_df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_df[f'RSI_{window}'] = 100 - (100 / (1 + rs))
                
                # RSIÊ¥æÁîüÊåáÊ®ô
                code_df[f'RSI_{window}_oversold'] = (code_df[f'RSI_{window}'] < 30).astype(int)
                code_df[f'RSI_{window}_overbought'] = (code_df[f'RSI_{window}'] > 70).astype(int)
            
            # üÜï MACDÂ§âÁ®ÆÔºà3Á®ÆÈ°ûÔºâ
            macd_configs = [(8, 21, 5), (12, 26, 9), (19, 39, 9)]
            for fast, slow, signal in macd_configs:
                exp1 = code_df['Close'].ewm(span=fast).mean()
                exp2 = code_df['Close'].ewm(span=slow).mean()
                code_df[f'MACD_{fast}_{slow}'] = exp1 - exp2
                code_df[f'MACD_signal_{fast}_{slow}'] = code_df[f'MACD_{fast}_{slow}'].ewm(span=signal).mean()
                code_df[f'MACD_histogram_{fast}_{slow}'] = code_df[f'MACD_{fast}_{slow}'] - code_df[f'MACD_signal_{fast}_{slow}']
                code_df[f'MACD_cross_{fast}_{slow}'] = np.where(code_df[f'MACD_{fast}_{slow}'] > code_df[f'MACD_signal_{fast}_{slow}'], 1, 0)
            
            # üÜï „Éú„É™„É≥„Ç∏„É£„Éº„Éê„É≥„ÉâÂ§âÁ®Æ
            for window in [10, 20, 30]:
                for std_mult in [1, 1.5, 2, 2.5, 3]:
                    rolling_mean = code_df['Close'].rolling(window).mean()
                    rolling_std = code_df['Close'].rolling(window).std()
                    code_df[f'BB_upper_{window}_{std_mult}'] = rolling_mean + (rolling_std * std_mult)
                    code_df[f'BB_lower_{window}_{std_mult}'] = rolling_mean - (rolling_std * std_mult)
                    code_df[f'BB_ratio_{window}_{std_mult}'] = (code_df['Close'] - code_df[f'BB_lower_{window}_{std_mult}']) / (code_df[f'BB_upper_{window}_{std_mult}'] - code_df[f'BB_lower_{window}_{std_mult}'])
                    code_df[f'BB_squeeze_{window}_{std_mult}'] = ((code_df[f'BB_upper_{window}_{std_mult}'] - code_df[f'BB_lower_{window}_{std_mult}']) / rolling_mean).rolling(20).min()
            
            # üÜï „Çπ„Éà„Ç≠„É£„Çπ„ÉÜ„Ç£„ÇØ„ÇπÂ§âÁ®Æ
            for window in [9, 14, 21, 28]:
                low_min = code_df['Low'].rolling(window).min()
                high_max = code_df['High'].rolling(window).max()
                code_df[f'Stoch_K_{window}'] = 100 * (code_df['Close'] - low_min) / (high_max - low_min)
                code_df[f'Stoch_D_{window}'] = code_df[f'Stoch_K_{window}'].rolling(3).mean()
                code_df[f'Stoch_cross_{window}'] = np.where(code_df[f'Stoch_K_{window}'] > code_df[f'Stoch_D_{window}'], 1, 0)
            
            # üÜï ‰æ°Ê†º„Éë„Çø„Éº„É≥ÁâπÂæ¥Èáè
            code_df['High_Low_ratio'] = code_df['High'] / code_df['Low']
            code_df['Open_Close_ratio'] = code_df['Open'] / code_df['Close']
            code_df['Close_Open_ratio'] = code_df['Close'] / code_df['Open']
            code_df['Upper_shadow'] = (code_df['High'] - np.maximum(code_df['Open'], code_df['Close'])) / code_df['Close']
            code_df['Lower_shadow'] = (np.minimum(code_df['Open'], code_df['Close']) - code_df['Low']) / code_df['Close']
            code_df['Body_size'] = abs(code_df['Close'] - code_df['Open']) / code_df['Close']
            code_df['Doji'] = (abs(code_df['Close'] - code_df['Open']) / code_df['Close'] < 0.01).astype(int)
            
            # üÜï „Éú„É™„É•„Éº„É†ÂàÜÊûê
            code_df['Volume_MA_10'] = code_df['Volume'].rolling(10).mean()
            code_df['Volume_MA_20'] = code_df['Volume'].rolling(20).mean()
            code_df['Volume_ratio_10'] = code_df['Volume'] / code_df['Volume_MA_10']
            code_df['Volume_ratio_20'] = code_df['Volume'] / code_df['Volume_MA_20']
            code_df['Price_Volume_Trend'] = (code_df['Returns_1d'] * code_df['Volume']).rolling(10).sum()
            
            # üÜï OBVÂ§âÁ®Æ
            obv_volume = code_df['Volume'] * np.where(code_df['Close'] > code_df['Close'].shift(1), 1, 
                                                     np.where(code_df['Close'] < code_df['Close'].shift(1), -1, 0))
            code_df['OBV'] = obv_volume.cumsum()
            for window in [10, 20, 30]:
                code_df[f'OBV_MA_{window}'] = code_df['OBV'].rolling(window).mean()
                code_df[f'OBV_ratio_{window}'] = code_df['OBV'] / code_df[f'OBV_MA_{window}']
            
            # üÜï „Çµ„Éù„Éº„Éà„Éª„É¨„Ç∏„Çπ„Çø„É≥„Çπ
            for window in [20, 50, 100]:
                code_df[f'Support_{window}'] = code_df['Low'].rolling(window).min()
                code_df[f'Resistance_{window}'] = code_df['High'].rolling(window).max()
                code_df[f'Support_distance_{window}'] = (code_df['Close'] - code_df[f'Support_{window}']) / code_df['Close']
                code_df[f'Resistance_distance_{window}'] = (code_df[f'Resistance_{window}'] - code_df['Close']) / code_df['Close']
            
            # üÜï „Éï„É©„ÇØ„Çø„É´„Éª„ÉÅ„É£„Éº„Éà„Éë„Çø„Éº„É≥
            for window in [5, 10, 15]:
                code_df[f'Local_max_{window}'] = (code_df['High'] == code_df['High'].rolling(window, center=True).max()).astype(int)
                code_df[f'Local_min_{window}'] = (code_df['Low'] == code_df['Low'].rolling(window, center=True).min()).astype(int)
            
            # üÜï „É¢„É°„É≥„Çø„É†ÊåáÊ®ô
            for period in [5, 10, 20, 30]:
                code_df[f'Momentum_{period}'] = code_df['Close'] / code_df['Close'].shift(period) - 1
                code_df[f'ROC_{period}'] = (code_df['Close'] - code_df['Close'].shift(period)) / code_df['Close'].shift(period)
            
            # üÜï ÊôÇÁ≥ªÂàóÁâπÂæ¥Èáè
            code_df['DayOfWeek'] = code_df['Date'].dt.dayofweek
            code_df['Month'] = code_df['Date'].dt.month
            code_df['Quarter'] = code_df['Date'].dt.quarter
            code_df['IsMonthEnd'] = (code_df['Date'].dt.day > 25).astype(int)
            code_df['IsQuarterEnd'] = ((code_df['Date'].dt.month % 3 == 0) & (code_df['Date'].dt.day > 25)).astype(int)
            
            # üÜï Â∏ÇÂ†¥ÊßãÈÄ†ÁâπÂæ¥Èáè
            code_df['Gap'] = (code_df['Open'] - code_df['Close'].shift(1)) / code_df['Close'].shift(1)
            code_df['Gap_up'] = (code_df['Gap'] > 0.01).astype(int)
            code_df['Gap_down'] = (code_df['Gap'] < -0.01).astype(int)
            
            # üÜï ÈÄ£Á∂öÊÄßÁâπÂæ¥Èáè
            for period in [2, 3, 5]:
                code_df[f'Consecutive_up_{period}'] = (code_df['Returns_1d'] > 0).rolling(period).sum()
                code_df[f'Consecutive_down_{period}'] = (code_df['Returns_1d'] < 0).rolling(period).sum()
            
            result_dfs.append(code_df)
        
        # ÁµêÂêà
        enhanced_df = pd.concat(result_dfs, ignore_index=True)
        
        # ÁõÆÁöÑÂ§âÊï∞‰ΩúÊàêÔºà95.45%Á≤æÂ∫¶„Å®Âêå„ÅòÂÆöÁæ©Ôºâ
        logger.info("ÁõÆÁöÑÂ§âÊï∞‰ΩúÊàê...")
        enhanced_df['Target'] = 0
        
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # „Éá„Éº„Çø„ÇØ„É™„Éº„Éã„É≥„Ç∞
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
        enhanced_df = enhanced_df.fillna(method='ffill').fillna(method='bfill').dropna()
        
        self.df = enhanced_df
        logger.info(f"üî• Á©∂Ê•µÁâπÂæ¥Èáè‰ΩúÊàêÂÆå‰∫Ü: {len(enhanced_df):,}‰ª∂")
        logger.info(f"ÁâπÂæ¥ÈáèÊï∞: {len(enhanced_df.columns)}„Ç´„É©„É†")
        
        positive_rate = enhanced_df['Target'].mean()
        logger.info(f"Ê≠£‰æãÁéá: {positive_rate:.3f} ({positive_rate:.1%})")
        
        return enhanced_df
    
    def get_features_and_target(self):
        """ÁâπÂæ¥Èáè„Å®„Çø„Éº„Ç≤„ÉÉ„ÉàÊ∫ñÂÇô"""
        exclude_cols = ['Date', 'Code', 'CompanyName', 'MatchMethod', 'ApiCode', 'Target']
        feature_cols = [col for col in self.df.columns if col not in exclude_cols]
        numeric_cols = self.df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        X = self.df[numeric_cols].replace([np.inf, -np.inf], np.nan).fillna(0)
        y = self.df['Target']
        
        return X, y, numeric_cols
    
    def experiment_1_advanced_lightgbm(self):
        """ÂÆüÈ®ì1: È´òÂ∫¶LightGBMÊúÄÈÅ©Âåñ"""
        logger.info("üß™ ÂÆüÈ®ì1: È´òÂ∫¶LightGBMÊúÄÈÅ©ÂåñÈñãÂßã...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # ÊôÇÁ≥ªÂàóÂàÜÂâ≤
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # OptunaÊúÄÈÅ©Âåñ
        def objective(trial):
            params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'n_estimators': trial.suggest_int('n_estimators', 500, 1500),
                'max_depth': trial.suggest_int('max_depth', 6, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'subsample': trial.suggest_float('subsample', 0.6, 0.95),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.3),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 0.3),
                'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
                'random_state': 42,
                'verbose': -1
            }
            
            # ÁâπÂæ¥ÈáèÈÅ∏Êäû
            selector = SelectKBest(score_func=f_classif, k=50)
            X_train_selected = selector.fit_transform(X_train, y_train)
            X_test_selected = selector.transform(X_test)
            
            # „É¢„Éá„É´Â≠¶Áøí
            model = LGBMClassifier(**params)
            model.fit(X_train_selected, y_train)
            
            # ‰∫àÊ∏¨
            pred_proba = model.predict_proba(X_test_selected)[:, 1]
            
            # Êó•Âà•Á≤æÂ∫¶Ë©ï‰æ°
            test_df_sample = df_sorted[test_mask].copy()
            test_df_sample['PredProba'] = pred_proba
            
            daily_precisions = []
            for date in test_df_sample['Date'].unique():
                daily_data = test_df_sample[test_df_sample['Date'] == date]
                if len(daily_data) >= 3:
                    top3 = daily_data.nlargest(3, 'PredProba')
                    precision = top3['Target'].mean()
                    daily_precisions.append(precision)
            
            return np.mean(daily_precisions) if daily_precisions else 0.0
        
        # ÊúÄÈÅ©ÂåñÂÆüË°å
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=20)  # 20ÂõûË©¶Ë°å
        
        best_params = study.best_params
        best_score = study.best_value
        
        logger.info(f"üéØ ÂÆüÈ®ì1ÁµêÊûú: ÊúÄÈ´òÁ≤æÂ∫¶ = {best_score:.4f} ({best_score:.2%})")
        
        return {
            'name': 'Advanced LightGBM',
            'score': best_score,
            'params': best_params,
            'model_type': 'lightgbm'
        }
    
    def experiment_2_ensemble_voting(self):
        """ÂÆüÈ®ì2: Â§öÊßòÊÄß„Ç¢„É≥„Çµ„É≥„Éñ„É´ÊäïÁ•®"""
        logger.info("üß™ ÂÆüÈ®ì2: Â§öÊßòÊÄß„Ç¢„É≥„Çµ„É≥„Éñ„É´ÊäïÁ•®ÈñãÂßã...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # ÊôÇÁ≥ªÂàóÂàÜÂâ≤
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # ÁâπÂæ¥ÈáèÈÅ∏Êäû
        selector = SelectKBest(score_func=f_classif, k=40)
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # Â§öÊßò„Å™„Éô„Éº„Çπ„É¢„Éá„É´
        base_models = [
            ('lgbm', LGBMClassifier(n_estimators=400, max_depth=8, learning_rate=0.05, random_state=42, verbose=-1)),
            ('xgb', xgb.XGBClassifier(n_estimators=400, max_depth=7, learning_rate=0.05, random_state=42)),
            ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)),
            ('et', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)),
            ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42))
        ]
        
        # „Ç¢„É≥„Çµ„É≥„Éñ„É´‰ΩúÊàê
        ensemble = VotingClassifier(estimators=base_models, voting='soft')
        
        # „Çπ„Ç±„Éº„É™„É≥„Ç∞
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # Â≠¶Áøí
        ensemble.fit(X_train_scaled, y_train)
        
        # ‰∫àÊ∏¨
        pred_proba = ensemble.predict_proba(X_test_scaled)[:, 1]
        
        # Êó•Âà•Á≤æÂ∫¶Ë©ï‰æ°
        test_df_sample = df_sorted[test_mask].copy()
        test_df_sample['PredProba'] = pred_proba
        
        daily_precisions = []
        for date in test_df_sample['Date'].unique():
            daily_data = test_df_sample[test_df_sample['Date'] == date]
            if len(daily_data) >= 3:
                top3 = daily_data.nlargest(3, 'PredProba')
                precision = top3['Target'].mean()
                daily_precisions.append(precision)
        
        score = np.mean(daily_precisions) if daily_precisions else 0.0
        
        logger.info(f"üéØ ÂÆüÈ®ì2ÁµêÊûú: Á≤æÂ∫¶ = {score:.4f} ({score:.2%})")
        
        return {
            'name': 'Ensemble Voting',
            'score': score,
            'model': ensemble,
            'scaler': scaler,
            'selector': selector
        }
    
    def experiment_3_stacking_ensemble(self):
        """ÂÆüÈ®ì3: „Çπ„Çø„ÉÉ„Ç≠„É≥„Ç∞„Ç¢„É≥„Çµ„É≥„Éñ„É´"""
        logger.info("üß™ ÂÆüÈ®ì3: „Çπ„Çø„ÉÉ„Ç≠„É≥„Ç∞„Ç¢„É≥„Çµ„É≥„Éñ„É´ÈñãÂßã...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # ÊôÇÁ≥ªÂàóÂàÜÂâ≤
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # ÁâπÂæ¥ÈáèÈÅ∏Êäû
        selector = SelectKBest(score_func=f_classif, k=45)
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # „Çπ„Ç±„Éº„É™„É≥„Ç∞
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # „É¨„Éô„É´1„É¢„Éá„É´ÔºàÂ§öÊßò„Å™„Ç¢„É´„Ç¥„É™„Ç∫„É†Ôºâ
        level1_models = {
            'lgbm1': LGBMClassifier(n_estimators=300, max_depth=7, learning_rate=0.05, subsample=0.8, random_state=42, verbose=-1),
            'lgbm2': LGBMClassifier(n_estimators=500, max_depth=9, learning_rate=0.03, subsample=0.9, random_state=123, verbose=-1),
            'xgb': xgb.XGBClassifier(n_estimators=350, max_depth=6, learning_rate=0.04, random_state=42),
            'rf': RandomForestClassifier(n_estimators=250, max_depth=8, min_samples_split=10, random_state=42, n_jobs=-1),
            'et': ExtraTreesClassifier(n_estimators=200, max_depth=12, min_samples_split=8, random_state=42, n_jobs=-1)
        }
        
        # ÊôÇÁ≥ªÂàó„ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Åß„É¨„Éô„É´1‰∫àÊ∏¨„Çí‰ΩúÊàê
        tscv = TimeSeriesSplit(n_splits=3)
        level1_train_preds = np.zeros((len(X_train_scaled), len(level1_models)))
        level1_test_preds = np.zeros((len(X_test_scaled), len(level1_models)))
        
        for i, (name, model) in enumerate(level1_models.items()):
            model_train_preds = np.zeros(len(X_train_scaled))
            
            # „ÇØ„É≠„Çπ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥‰∫àÊ∏¨
            for train_idx, val_idx in tscv.split(X_train_scaled):
                model.fit(X_train_scaled[train_idx], y_train.iloc[train_idx])
                model_train_preds[val_idx] = model.predict_proba(X_train_scaled[val_idx])[:, 1]
            
            level1_train_preds[:, i] = model_train_preds
            
            # „ÉÜ„Çπ„Éà„Éá„Éº„Çø‰∫àÊ∏¨ÔºàÂÖ®„Éá„Éº„Çø„ÅßÂÜçÂ≠¶ÁøíÔºâ
            model.fit(X_train_scaled, y_train)
            level1_test_preds[:, i] = model.predict_proba(X_test_scaled)[:, 1]
        
        # „É¨„Éô„É´2„É°„ÇøÂ≠¶ÁøíÂô®
        meta_learner = LogisticRegression(random_state=42, max_iter=1000)
        meta_learner.fit(level1_train_preds, y_train)
        
        # ÊúÄÁµÇ‰∫àÊ∏¨
        pred_proba = meta_learner.predict_proba(level1_test_preds)[:, 1]
        
        # Êó•Âà•Á≤æÂ∫¶Ë©ï‰æ°
        test_df_sample = df_sorted[test_mask].copy()
        test_df_sample['PredProba'] = pred_proba
        
        daily_precisions = []
        for date in test_df_sample['Date'].unique():
            daily_data = test_df_sample[test_df_sample['Date'] == date]
            if len(daily_data) >= 3:
                top3 = daily_data.nlargest(3, 'PredProba')
                precision = top3['Target'].mean()
                daily_precisions.append(precision)
        
        score = np.mean(daily_precisions) if daily_precisions else 0.0
        
        logger.info(f"üéØ ÂÆüÈ®ì3ÁµêÊûú: Á≤æÂ∫¶ = {score:.4f} ({score:.2%})")
        
        return {
            'name': 'Stacking Ensemble',
            'score': score,
            'level1_models': level1_models,
            'meta_learner': meta_learner,
            'scaler': scaler,
            'selector': selector
        }
    
    def experiment_4_calibrated_models(self):
        """ÂÆüÈ®ì4: Á¢∫ÁéáÊ†°Ê≠£„É¢„Éá„É´"""
        logger.info("üß™ ÂÆüÈ®ì4: Á¢∫ÁéáÊ†°Ê≠£„É¢„Éá„É´ÈñãÂßã...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # ÊôÇÁ≥ªÂàóÂàÜÂâ≤
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # ÁâπÂæ¥ÈáèÈÅ∏Êäû
        selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold='median')
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # „Éô„Éº„Çπ„É¢„Éá„É´
        base_model = LGBMClassifier(
            n_estimators=600,
            max_depth=10,
            learning_rate=0.03,
            subsample=0.85,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbose=-1
        )
        
        # Á¢∫ÁéáÊ†°Ê≠£ÔºàIsotonicÂõûÂ∏∞Ôºâ
        calibrated_model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)
        
        # Â≠¶Áøí
        calibrated_model.fit(X_train_selected, y_train)
        
        # ‰∫àÊ∏¨
        pred_proba = calibrated_model.predict_proba(X_test_selected)[:, 1]
        
        # Êó•Âà•Á≤æÂ∫¶Ë©ï‰æ°
        test_df_sample = df_sorted[test_mask].copy()
        test_df_sample['PredProba'] = pred_proba
        
        daily_precisions = []
        for date in test_df_sample['Date'].unique():
            daily_data = test_df_sample[test_df_sample['Date'] == date]
            if len(daily_data) >= 3:
                top3 = daily_data.nlargest(3, 'PredProba')
                precision = top3['Target'].mean()
                daily_precisions.append(precision)
        
        score = np.mean(daily_precisions) if daily_precisions else 0.0
        
        logger.info(f"üéØ ÂÆüÈ®ì4ÁµêÊûú: Á≤æÂ∫¶ = {score:.4f} ({score:.2%})")
        
        return {
            'name': 'Calibrated Model',
            'score': score,
            'model': calibrated_model,
            'selector': selector
        }
    
    def run_all_experiments(self):
        """ÂÖ®ÂÆüÈ®ìÂÆüË°å"""
        logger.info("üöÄ Á©∂Ê•µ„ÅÆÈ´òÁ≤æÂ∫¶ÂÆüÁèæ: ÂÖ®ÂÆüÈ®ìÈñãÂßã!")
        
        try:
            # „Éá„Éº„ÇøÊ∫ñÂÇô
            self.load_data()
            self.create_ultimate_features()
            
            # ÂÖ®ÂÆüÈ®ìÂÆüË°å
            experiments = [
                self.experiment_1_advanced_lightgbm,
                self.experiment_2_ensemble_voting,
                self.experiment_3_stacking_ensemble,
                self.experiment_4_calibrated_models
            ]
            
            results = []
            for i, experiment in enumerate(experiments, 1):
                logger.info(f"\n{'='*50}")
                logger.info(f"ÂÆüÈ®ì {i}/{len(experiments)} ÂÆüË°å‰∏≠...")
                logger.info(f"{'='*50}")
                
                result = experiment()
                results.append(result)
                
                if result['score'] > self.best_score:
                    self.best_score = result['score']
                    self.best_model = result
                    logger.info(f"üéâ Êñ∞Ë®òÈå≤Êõ¥Êñ∞! ÊúÄÈ´òÁ≤æÂ∫¶: {self.best_score:.4f} ({self.best_score:.2%})")
            
            # ÁµêÊûú„Åæ„Å®„ÇÅ
            logger.info(f"\n{'='*60}")
            logger.info("üèÜ ÂÖ®ÂÆüÈ®ìÁµêÊûú:")
            logger.info(f"{'='*60}")
            
            for result in results:
                logger.info(f"{result['name']}: {result['score']:.4f} ({result['score']:.2%})")
            
            logger.info(f"\nü•á ÊúÄÈ´òÁ≤æÂ∫¶: {self.best_model['name']} = {self.best_score:.4f} ({self.best_score:.2%})")
            
            # „É¢„Éá„É´‰øùÂ≠ò
            self.save_best_model()
            
            return results
            
        except Exception as e:
            logger.error(f"ÂÆüÈ®ìÂÆüË°å„Ç®„É©„Éº: {e}")
            return None
    
    def save_best_model(self):
        """ÊúÄÈ´ò„É¢„Éá„É´‰øùÂ≠ò"""
        if self.best_model is None:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        precision_str = f"{self.best_score:.4f}".replace('.', '')
        
        os.makedirs("models/ultimate", exist_ok=True)
        
        model_file = f"models/ultimate/ultimate_model_{precision_str}precision_{timestamp}.joblib"
        
        model_data = {
            'best_model': self.best_model,
            'best_score': self.best_score,
            'data_info': {
                'total_records': len(self.df),
                'n_companies': self.df['Code'].nunique(),
                'feature_count': len(self.df.columns),
                'date_range': f"{self.df['Date'].min()} - {self.df['Date'].max()}"
            },
            'experiment_type': 'ultimate_precision_system'
        }
        
        joblib.dump(model_data, model_file)
        logger.info(f"üéØ ÊúÄÈ´ò„É¢„Éá„É´‰øùÂ≠òÂÆå‰∫Ü: {model_file}")
        
        return model_file

def main():
    """„É°„Ç§„É≥ÂÆüË°å"""
    system = UltimatePrecisionSystem()
    results = system.run_all_experiments()
    
    if results:
        print(f"\nüéâ Á©∂Ê•µ„ÅÆÈ´òÁ≤æÂ∫¶ÂÆüÁèæ„Ç∑„Çπ„ÉÜ„É†ÂÆå‰∫Ü!")
        print(f"ü•á ÊúÄÈ´òÈÅîÊàêÁ≤æÂ∫¶: {system.best_score:.2%}")
        print(f"üèÜ ÊúÄÂÑ™ÁßÄÊâãÊ≥ï: {system.best_model['name']}")
        print(f"üìä ÂÆüÈ®ìÁ∑èÊï∞: {len(results)}ÂÆüÈ®ì")
    else:
        print("\n‚ùå Á©∂Ê•µ„Ç∑„Çπ„ÉÜ„É†ÂÆüË°åÂ§±Êïó")

if __name__ == "__main__":
    main()