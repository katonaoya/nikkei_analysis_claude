#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Precision System V3
å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ + å³å¯†ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹æ”¹å–„ç‰ˆ

æ”¹å–„ç‚¹ï¼š
1. å¤–éƒ¨çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆï¼ˆUSD/JPY, VIX, æ—¥çµŒ225æŒ‡æ•°ç­‰ï¼‰
2. ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–ã«ã‚ˆã‚‹å³å¯†ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
3. è¤‡é›‘æ€§ã‚’æŠ‘ãˆãŸã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
from datetime import datetime, timedelta
import logging
import warnings
import os
from pathlib import Path

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedPrecisionSystemV3:
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ + å³å¯†ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç‰ˆ"""
    
    def __init__(self, stock_file: str = None, external_file: str = None):
        """åˆæœŸåŒ–"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå‹•çš„ã«æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼‰
        if stock_file is None:
            stock_file = self._find_latest_stock_file()
        if external_file is None:
            external_file = self._find_latest_external_file()
            
        self.stock_file = stock_file
        self.external_file = external_file
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("models/enhanced_v3")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ¯ Enhanced Precision System V3 åˆæœŸåŒ–å®Œäº†")
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {self.stock_file}")
        logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {self.external_file}")
    
    def _find_latest_stock_file(self) -> str:
        """æœ€æ–°ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        import glob
        
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            "data/processed/nikkei225_complete_*.parquet",
            "data/real_jquants_data/nikkei225_real_data_*.pkl",
            "data/processed/nikkei225_*.parquet"
        ]
        
        latest_file = None
        latest_time = 0
        
        for pattern in patterns:
            files = glob.glob(pattern)
            for file in files:
                try:
                    file_time = os.path.getmtime(file)
                    if file_time > latest_time:
                        latest_time = file_time
                        latest_file = file
                except:
                    continue
        
        if latest_file is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«å
            latest_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
            logger.warning(f"æœ€æ–°æ ªä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
        else:
            logger.info(f"æœ€æ–°æ ªä¾¡ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—: {latest_file}")
        
        return latest_file
    
    def _find_latest_external_file(self) -> str:
        """æœ€æ–°ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        import glob
        
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            "data/external_extended/external_integrated_*.parquet",
            "data/processed/enhanced_integrated_data.parquet",
            "data/processed/external_*.parquet"
        ]
        
        latest_file = None
        latest_time = 0
        
        for pattern in patterns:
            files = glob.glob(pattern)
            for file in files:
                try:
                    file_time = os.path.getmtime(file)
                    if file_time > latest_time:
                        latest_time = file_time
                        latest_file = file
                except:
                    continue
        
        if latest_file is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«å
            latest_file = "data/external_extended/external_integrated_10years_20250909_231815.parquet"
            logger.warning(f"æœ€æ–°å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
        else:
            logger.info(f"æœ€æ–°å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—: {latest_file}")
        
        return latest_file
    
    def load_and_integrate_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨çµ±åˆ"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        stock_df = pd.read_parquet(self.stock_file)
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶, {stock_df['Code'].nunique()}éŠ˜æŸ„")
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        external_df = None
        if os.path.exists(self.external_file):
            try:
                external_df = pd.read_parquet(self.external_file)
                logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_df):,}ä»¶, {len(external_df.columns)}ã‚«ãƒ©ãƒ ")
            except Exception as e:
                logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                external_df = None
        else:
            logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.external_file}")
        
        # æ—¥ä»˜å‹çµ±ä¸€
        stock_df['Date'] = pd.to_datetime(stock_df['Date']).dt.tz_localize(None)
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼ˆ78.5%ç²¾åº¦æ™‚ã®çŠ¶æ…‹ã«å¾©å…ƒï¼‰
        if external_df is not None and len(external_df) < 10000:  # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿çµ±åˆ
            try:
                external_df['Date'] = pd.to_datetime(external_df['Date']).dt.tz_localize(None)
                
                # é‡è¦ãªå¤–éƒ¨æŒ‡æ¨™ã®ã¿é¸æŠï¼ˆè¤‡é›‘æ€§ã‚’æŠ‘åˆ¶ï¼‰
                important_external_cols = ['Date']
                for col in external_df.columns:
                    if any(key in col.lower() for key in ['usdjpy', 'vix', 'nikkei225_close', 'sp500_close']):
                        important_external_cols.append(col)
                
                if len(important_external_cols) > 1:
                    external_selected = external_df[important_external_cols].copy()
                    stock_df = pd.merge(stock_df, external_selected, on='Date', how='left')
                    logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(important_external_cols)-1}æŒ‡æ¨™")
                else:
                    logger.info("å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰")
                    
            except Exception as e:
                logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        else:
            logger.info("å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ - 78.5%ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        
        logger.info(f"çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶, {len(stock_df.columns)}ã‚«ãƒ©ãƒ ")
        return stock_df
    
    def create_enhanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‹¡å¼µç‰¹å¾´é‡ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ”¥ æ‹¡å¼µç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        
        # å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆ78.5%ç²¾åº¦æ™‚ã®çŠ¶æ…‹ã«å¾©å…ƒï¼‰
        df_recent = df.copy()
        
        logger.info(f"å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {len(df_recent):,}ä»¶ï¼ˆç´„10å¹´é–“ï¼‰")
        
        # å‡¦ç†ç”¨ã®DataFrameã‚’æº–å‚™ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’é‡è¦–ï¼‰
        enhanced_df = df_recent.copy()
        
        # å…¨éŠ˜æŸ„ã‚’ä¸€æ‹¬å‡¦ç†ï¼ˆ78.5%ç²¾åº¦æ™‚ã®çŠ¶æ…‹ã«å¾©å…ƒï¼‰
        unique_codes = enhanced_df['Code'].unique()
        logger.info(f"å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†: {len(unique_codes)}éŠ˜æŸ„")
        
        for code in unique_codes:
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy().sort_values('Date')
            
            if len(code_data) < 50:  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„éŠ˜æŸ„ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆå¿…è¦æœ€å°é™ï¼‰
            code_data['Returns'] = code_data['Close'].pct_change()
            code_data['High_Low_Ratio'] = code_data['High'] / code_data['Low']
                
            # ç§»å‹•å¹³å‡ï¼ˆé‡è¦ãªæœŸé–“ã®ã¿ï¼‰
            for window in [5, 20]:
                code_data[f'MA_{window}'] = code_data['Close'].rolling(window).mean()
                code_data[f'MA_{window}_ratio'] = code_data['Close'] / code_data[f'MA_{window}']
                
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ1ã¤ã®windowã®ã¿ï¼‰
            code_data['Volatility_20'] = code_data['Returns'].rolling(20).std()
                
            # RSIï¼ˆ1ã¤ã®windowã®ã¿ï¼‰
            window = 14
            delta = code_data['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
            rs = gain / loss
            code_data['RSI_14'] = 100 - (100 / (1 + rs))
                
            # MACDï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
            exp1 = code_data['Close'].ewm(span=12).mean()
            exp2 = code_data['Close'].ewm(span=26).mean()
            code_data['MACD'] = exp1 - exp2
                
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡
            code_data['Volume_MA_20'] = code_data['Volume'].rolling(20).mean()
            code_data['Volume_ratio'] = code_data['Volume'] / code_data['Volume_MA_20']
                
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã¯æœ€å°é™ã«
            for col in code_data.columns:
                if any(key in col.lower() for key in ['usdjpy', 'vix']):
                    if code_data[col].notna().sum() > 50:
                        code_data[f'{col}_change'] = code_data[col].pct_change()
            
            enhanced_df.loc[mask] = code_data
        
        # ç›®çš„å¤‰æ•°ä½œæˆ
        logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
        enhanced_df['Target'] = 0
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # ç„¡é™å€¤ãƒ»æ¬ æå€¤å‡¦ç†
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
        
        logger.info(f"ğŸ”¥ ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶")
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(enhanced_df.columns)}ã‚«ãƒ©ãƒ ")
        logger.info(f"æ­£ä¾‹ç‡: {enhanced_df['Target'].mean():.3f}")
        
        return enhanced_df
    
    def walk_forward_optimization(self, df: pd.DataFrame, initial_train_size: int = 252*2) -> list:
        """ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ“ˆ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–é–‹å§‹...")
        
        # å…¨é‡ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼ï¼ˆå†ç¾æ€§ã¨é™½æ€§ã‚µãƒ³ãƒ—ãƒ«ã‚’æœ€å¤§é™æ´»ç”¨ï¼‰
        df_sampled = df.copy()
        logger.info(f"ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {len(df_sampled):,}ä»¶ï¼ˆå…¨é‡ä½¿ç”¨ï¼‰")
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df_sampled.sort_values(['Date', 'Code']).copy()
        unique_dates = sorted(df_sorted['Date'].unique())
        
        results = []
        step_size = 42  # 2ãƒ¶æœˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼ˆè¨ˆç®—é‡å‰Šæ¸›ï¼‰
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ é¸æŠï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰
        feature_cols = [col for col in df_sorted.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       str(df_sorted[col].dtype) in ['int64', 'float64', 'int32', 'float32']]
        
        # ç‰¹å¾´é‡æ•°åˆ¶é™
        if len(feature_cols) > 30:
            # æ¬ æå€¤ãŒå°‘ãªã„ç‰¹å¾´é‡ã‚’å„ªå…ˆé¸æŠ
            non_null_counts = df_sorted[feature_cols].count()
            top_features = non_null_counts.nlargest(30).index.tolist()
            feature_cols = top_features
        
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
        
        # åˆæœŸã‚µã‚¤ã‚ºã‚’å°ã•ãè¨­å®š
        initial_train_size = min(initial_train_size, len(unique_dates) // 3)
        
        for i in range(initial_train_size, len(unique_dates) - step_size, step_size):
            try:
                # æœŸé–“è¨­å®š
                train_end_idx = i
                test_start_idx = i
                test_end_idx = min(i + step_size, len(unique_dates))
                
                train_dates = unique_dates[:train_end_idx]
                test_dates = unique_dates[test_start_idx:test_end_idx]
                
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
                train_df = df_sorted[df_sorted['Date'].isin(train_dates)]
                test_df = df_sorted[df_sorted['Date'].isin(test_dates)]
                
                if len(train_df) == 0 or len(test_df) == 0:
                    continue
                
                # ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°åˆ†é›¢
                X_train = train_df[feature_cols]
                y_train = train_df['Target']
                X_test = test_df[feature_cols]
                y_test = test_df['Target']
                
                # æ¬ æå€¤å‡¦ç†
                X_train = X_train.fillna(method='ffill').fillna(0)
                X_test = X_test.fillna(method='ffill').fillna(0)
                
                # ç‰¹å¾´é‡é¸æŠï¼ˆ78.5%ç²¾åº¦æ™‚ã«å¾©å…ƒï¼‰
                selector = SelectKBest(score_func=f_classif, k=min(30, len(feature_cols)))
                X_train_selected = selector.fit_transform(X_train, y_train)
                X_test_selected = selector.transform(X_test)
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train_selected)
                X_test_scaled = scaler.transform(X_test_selected)
                
                # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è»½é‡åŒ–ï¼‰
                model = lgb.LGBMClassifier(
                    objective='binary',
                    n_estimators=300,  # å¾©å…ƒ
                    max_depth=8,       # å¾©å…ƒ
                    learning_rate=0.03, # å¾©å…ƒ
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    random_state=42,
                    verbose=-1
                )
                
                model.fit(X_train_scaled, y_train)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                
                # è©•ä¾¡
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                
                result = {
                    'period': f"{train_dates[-1].strftime('%Y-%m-%d')} to {test_dates[-1].strftime('%Y-%m-%d')}",
                    'train_size': len(train_df),
                    'test_size': len(test_df),
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'positive_rate': y_test.mean()
                }
                
                results.append(result)
                logger.info(f"æœŸé–“ {result['period']}: ç²¾åº¦={accuracy:.4f}, é©åˆç‡={precision:.4f}")
                
            except Exception as e:
                logger.warning(f"æœŸé–“ {i} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return results
    
    def train_final_model(self, df: pd.DataFrame) -> dict:
        """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ¤– æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")
        
        # å…¨é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å»ƒæ­¢ï¼‰
        df_sampled = df.copy()
        logger.info(f"æœ€çµ‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_sampled):,}ä»¶ï¼ˆå…¨é‡ä½¿ç”¨ï¼‰")
        
        # ç‰¹å¾´é‡æº–å‚™
        feature_cols = [col for col in df_sampled.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       str(df_sampled[col].dtype) in ['int64', 'float64', 'int32', 'float32']]
        
        # ç‰¹å¾´é‡æ•°åˆ¶é™
        if len(feature_cols) > 25:
            non_null_counts = df_sampled[feature_cols].count()
            feature_cols = non_null_counts.nlargest(25).index.tolist()
        
        X = df_sampled[feature_cols].fillna(method='ffill').fillna(0)
        y = df_sampled['Target']
        
        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœ€å¾Œ20%ã‚’ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        df_sorted = df_sampled.sort_values('Date')
        split_idx = int(len(df_sorted) * 0.8)
        
        X_train = X[:split_idx]
        X_test = X[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆ78.5%ç²¾åº¦æ™‚ã«å¾©å…ƒï¼‰
        selector = SelectKBest(score_func=f_classif, k=min(30, len(feature_cols)))
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆ78.5%ç²¾åº¦æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¾©å…ƒï¼‰
        model = lgb.LGBMClassifier(
            objective='binary',
            n_estimators=300,  # å¾©å…ƒ
            max_depth=8,       # å¾©å…ƒ
            learning_rate=0.03,  # å¾©å…ƒ
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbose=-1
        )
        
        model.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        logger.info(f"ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        logger.info(f"  ç²¾åº¦: {accuracy:.4f}")
        logger.info(f"  é©åˆç‡: {precision:.4f}")
        logger.info(f"  å†ç¾ç‡: {recall:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_data = {
            'model': model,
            'scaler': scaler,
            'selector': selector,
            'feature_cols': feature_cols,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'train_size': len(X_train),
            'test_size': len(X_test)
        }
        
        model_file = self.output_dir / f"enhanced_model_v3_{accuracy:.4f}acc_{timestamp}.joblib"
        joblib.dump(model_data, model_file)
        logger.info(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_data
    
    def run_enhanced_system(self):
        """æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        logger.info("ğŸš€ Enhanced Precision System V3 å®Ÿè¡Œé–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            df = self.load_and_integrate_data()
            
            # ç‰¹å¾´é‡ä½œæˆ
            enhanced_df = self.create_enhanced_features(df)
            
            # ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–
            wfo_results = self.walk_forward_optimization(enhanced_df)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            final_model = self.train_final_model(enhanced_df)
            
            # çµæœçµ±è¨ˆ
            if wfo_results:
                wfo_accuracies = [r['accuracy'] for r in wfo_results]
                wfo_mean_acc = np.mean(wfo_accuracies)
                wfo_std_acc = np.std(wfo_accuracies)
                
                logger.info(f"\nğŸ“Š ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–çµæœ:")
                logger.info(f"  æœŸé–“æ•°: {len(wfo_results)}")
                logger.info(f"  å¹³å‡ç²¾åº¦: {wfo_mean_acc:.4f} Â± {wfo_std_acc:.4f}")
                logger.info(f"  æœ€é«˜ç²¾åº¦: {max(wfo_accuracies):.4f}")
                logger.info(f"  æœ€ä½ç²¾åº¦: {min(wfo_accuracies):.4f}")
            
            # çµæœä¿å­˜
            results = {
                'final_model_accuracy': final_model['accuracy'],
                'wfo_mean_accuracy': wfo_mean_acc if wfo_results else 0,
                'wfo_std_accuracy': wfo_std_acc if wfo_results else 0,
                'wfo_results': wfo_results,
                'data_size': len(enhanced_df),
                'feature_count': len(final_model['feature_cols']),
                'external_data_integrated': os.path.exists(self.external_file)
            }
            
            results_file = self.output_dir / f"enhanced_results_v3_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
            joblib.dump(results, results_file)
            
            logger.info(f"ğŸ‰ Enhanced Precision System V3 å®Œäº†!")
            logger.info(f"æœ€çµ‚ç²¾åº¦: {final_model['accuracy']:.4f}")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {'âœ…' if results['external_data_integrated'] else 'âŒ'}")
            logger.info(f"çµæœä¿å­˜: {results_file}")
            
            return results
            
        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    system = EnhancedPrecisionSystemV3()
    results = system.run_enhanced_system()
    
    if results:
        print(f"\nâœ… Enhanced Precision System V3 å®Ÿè¡Œå®Œäº†!")
        print(f"ğŸ“Š æœ€çµ‚ç²¾åº¦: {results['final_model_accuracy']:.4f}")
        if results['wfo_mean_accuracy'] > 0:
            print(f"ğŸ“ˆ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å¹³å‡ç²¾åº¦: {results['wfo_mean_accuracy']:.4f}")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {'æˆåŠŸ' if results['external_data_integrated'] else 'å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãªã—'}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿é‡: {results['data_size']:,}ä»¶")
        print(f"ğŸ”§ ç‰¹å¾´é‡æ•°: {results['feature_count']}å€‹")
    else:
        print("\nâŒ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
