#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Close-to-Close Precision System V1
å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ + å³å¯†ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹æ”¹å–„ç‰ˆ

æ”¹å–„ç‚¹ï¼š
1. å¤–éƒ¨çµŒæ¸ˆæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆï¼ˆUSD/JPY, VIX, æ—¥çµŒ225æŒ‡æ•°ç­‰ï¼‰
2. ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–ã«ã‚ˆã‚‹å³å¯†ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
3. è¤‡é›‘æ€§ã‚’æŠ‘ãˆãŸã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
import joblib
from datetime import datetime, timedelta
import logging
import argparse
import json
import warnings
import os
from pathlib import Path
from typing import Optional, Tuple

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CloseReturnPrecisionSystemV1:
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ + å³å¯†ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç‰ˆ"""
    
    def __init__(
        self,
        stock_file: str = None,
        external_file: str = None,
        target_return: float = 0.01,
        imbalance_boost: float = 1.0,
        imbalance_strategy: str = "scale_pos",
        focal_gamma: float = 2.0,
        positive_oversample_ratio: float = 1.0,
    ):
        """åˆæœŸåŒ–"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå‹•çš„ã«æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼‰
        if stock_file is None:
            stock_file = self._find_latest_stock_file()
        if external_file is None:
            external_file = self._find_latest_external_file()
            
        self.stock_file = stock_file
        self.external_file = external_file
        self.target_return = target_return
        self.imbalance_boost = imbalance_boost
        self.imbalance_strategy = imbalance_strategy
        self.focal_gamma = focal_gamma
        self.positive_oversample_ratio = max(positive_oversample_ratio, 1.0)
        self._rng = np.random.default_rng(42)
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("models/enhanced_close_v1")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.model_params = self._load_model_params()

        logger.info("ğŸ¯ Close-to-Close Precision System V1 åˆæœŸåŒ–å®Œäº†")
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {self.stock_file}")
        logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {self.external_file}")
        logger.info(f"ã‚¯ãƒ©ã‚¹é‡ã¿ãƒ–ãƒ¼ã‚¹ãƒˆä¿‚æ•°: {self.imbalance_boost:.3f}")
        logger.info(f"ä¸å‡è¡¡æˆ¦ç•¥: {self.imbalance_strategy}")
        if self.imbalance_strategy == "focal":
            logger.info(f"Focal Gamma: {self.focal_gamma:.2f}")
        if self.positive_oversample_ratio > 1.0:
            logger.info(f"Positive oversample ratio: {self.positive_oversample_ratio:.2f}")

    def _load_model_params(self) -> dict:
        params_path = Path('config/close_model_params.json')
        if params_path.exists():
            with params_path.open() as f:
                return json.load(f)
        return {
            'n_estimators': 300,
            'learning_rate': 0.03,
            'max_depth': 8,
            'num_leaves': 63,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'min_child_samples': 20
        }

    def _find_latest_stock_file(self) -> str:
        """æœ€æ–°ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        import glob
        
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            "data/processed/nikkei225_complete_*.parquet",
            "data/real_jquants_data/nikkei225_real_data_*.pkl",
            "data/processed/nikkei225_*.parquet"
        ]
        
        latest_file = None
        latest_time = 0
        
        for pattern in patterns:
            files = glob.glob(pattern)
            for file in files:
                try:
                    file_time = os.path.getmtime(file)
                    if file_time > latest_time:
                        latest_time = file_time
                        latest_file = file
                except:
                    continue
        
        if latest_file is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«å
            latest_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
            logger.warning(f"æœ€æ–°æ ªä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
        else:
            logger.info(f"æœ€æ–°æ ªä¾¡ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—: {latest_file}")
        
        return latest_file
    
    def _find_latest_external_file(self) -> str:
        """æœ€æ–°ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        import glob
        
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            "data/external_extended/external_integrated_*.parquet",
            "data/processed/enhanced_integrated_data.parquet",
            "data/processed/external_*.parquet"
        ]
        
        latest_file = None
        latest_time = 0
        
        for pattern in patterns:
            files = glob.glob(pattern)
            for file in files:
                try:
                    file_time = os.path.getmtime(file)
                    if file_time > latest_time:
                        latest_time = file_time
                        latest_file = file
                except:
                    continue
        
        if latest_file is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«å
            latest_file = "data/external_extended/external_integrated_10years_20250909_231815.parquet"
            logger.warning(f"æœ€æ–°å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
        else:
            logger.info(f"æœ€æ–°å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—: {latest_file}")
        
        return latest_file
    
    def load_and_integrate_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨çµ±åˆ"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        stock_df = pd.read_parquet(self.stock_file)
        logger.info(f"æ ªä¾¡ãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶, {stock_df['Code'].nunique()}éŠ˜æŸ„")

        # èª¿æ•´å¾Œçµ‚å€¤ãƒ»é«˜å€¤ãƒ»å®‰å€¤ãƒ»å‡ºæ¥é«˜ã‚’å„ªå…ˆåˆ©ç”¨
        if 'AdjustmentClose' in stock_df.columns:
            stock_df['Close_raw'] = stock_df['Close']
            stock_df['High_raw'] = stock_df['High']
            stock_df['Low_raw'] = stock_df['Low']
            stock_df['Volume_raw'] = stock_df['Volume']
            stock_df['Close'] = stock_df['AdjustmentClose']
            stock_df['High'] = stock_df.get('AdjustmentHigh', stock_df['High'])
            stock_df['Low'] = stock_df.get('AdjustmentLow', stock_df['Low'])
            stock_df['Volume'] = stock_df.get('AdjustmentVolume', stock_df['Volume'])
            logger.info('Adjusted price columns applied')
            stock_df['TargetReturn'] = getattr(self, 'target_return', 0.01)

        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        external_df = None
        if os.path.exists(self.external_file):
            try:
                external_df = pd.read_parquet(self.external_file)
                logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿: {len(external_df):,}ä»¶, {len(external_df.columns)}ã‚«ãƒ©ãƒ ")
            except Exception as e:
                logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                external_df = None
        else:
            logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.external_file}")
        
        # æ—¥ä»˜å‹çµ±ä¸€
        stock_df['Date'] = pd.to_datetime(stock_df['Date']).dt.tz_localize(None)
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆã‚µã‚¤ã‚ºã«é–¢ã‚ã‚‰ãšé‡è¦æŒ‡æ¨™ã®ã¿ã‚’å–ã‚Šè¾¼ã‚€ï¼‰
        if external_df is not None:
            try:
                external_df['Date'] = pd.to_datetime(external_df['Date']).dt.tz_localize(None)
                
                # é‡è¦æŒ‡æ¨™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒãƒƒãƒã™ã‚‹åˆ—ã‚’å„ªå…ˆçš„ã«é¸æŠ
                keyword_candidates = ['usdjpy', 'vix', 'nikkei', 'sp500', 'topix', 'dow', 'nasdaq', 'commodity', 'yield']
                selected_cols = [
                    col for col in external_df.columns
                    if any(key in col.lower() for key in keyword_candidates)
                ]

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ•°å€¤åˆ—ã®ã†ã¡æœ€å¤šéæ¬ æãƒˆãƒƒãƒ— N ã‚’æ¡ç”¨
                if not selected_cols:
                    numeric_cols = external_df.select_dtypes(include=[np.number]).columns.tolist()
                    non_null_counts = external_df[numeric_cols].count().sort_values(ascending=False)
                    selected_cols = non_null_counts.head(25).index.tolist()

                # çµ±åˆã™ã‚‹åˆ—æ•°ã‚’ä¸Šé™åŒ–ã—ã¦å­¦ç¿’è² è·ã‚’æŠ‘ãˆã‚‹
                max_external_features = 30
                if len(selected_cols) > max_external_features:
                    selected_cols = selected_cols[:max_external_features]

                external_selected = external_df[['Date'] + selected_cols].copy()

                # é‡è¤‡æ—¥ä»˜ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯æœ€çµ‚è¡Œã‚’æ¡ç”¨
                if external_selected['Date'].duplicated().any():
                    external_selected = external_selected.sort_values('Date')
                    external_selected = external_selected.groupby('Date').tail(1)

                stock_df = pd.merge(stock_df, external_selected, on='Date', how='left')
                logger.info(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(selected_cols)}æŒ‡æ¨™")

            except Exception as e:
                logger.warning(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        else:
            logger.info("å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿æœªæ¤œå‡ºï¼‰")
        
        logger.info(f"çµ±åˆå¾Œãƒ‡ãƒ¼ã‚¿: {len(stock_df):,}ä»¶, {len(stock_df.columns)}ã‚«ãƒ©ãƒ ")
        return stock_df
    
    def create_enhanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‹¡å¼µç‰¹å¾´é‡ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ”¥ æ‹¡å¼µç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        
        # å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆæ—¢å­˜æ§‹æˆã‚’è¸è¥²ï¼‰
        df_recent = df.copy()
        
        logger.info(f"å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {len(df_recent):,}ä»¶ï¼ˆç´„10å¹´é–“ï¼‰")
        
        # å‡¦ç†ç”¨ã®DataFrameã‚’æº–å‚™ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’é‡è¦–ï¼‰
        enhanced_df = df_recent.copy()
        
        # å…¨éŠ˜æŸ„ã‚’ä¸€æ‹¬å‡¦ç†ï¼ˆæ—¢å­˜æ§‹æˆã‚’è¸è¥²ï¼‰
        unique_codes = enhanced_df['Code'].unique()
        logger.info(f"å…¨éŠ˜æŸ„ä¸€æ‹¬å‡¦ç†: {len(unique_codes)}éŠ˜æŸ„")
        
        for code in unique_codes:
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy().sort_values('Date')
            
            if len(code_data) < 50:  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„éŠ˜æŸ„ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆå¿…è¦æœ€å°é™ï¼‰
            code_data['Returns'] = code_data['Close'].pct_change()
            code_data['High_Low_Ratio'] = code_data['High'] / code_data['Low']
                
            # ç§»å‹•å¹³å‡ï¼ˆé‡è¦ãªæœŸé–“ã®ã¿ï¼‰
            for window in [5, 20]:
                code_data[f'MA_{window}'] = code_data['Close'].rolling(window).mean()
                code_data[f'MA_{window}_ratio'] = code_data['Close'] / code_data[f'MA_{window}']
                
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ1ã¤ã®windowã®ã¿ï¼‰
            code_data['Volatility_20'] = code_data['Returns'].rolling(20).std()
                
            # RSIï¼ˆ1ã¤ã®windowã®ã¿ï¼‰
            window = 14
            delta = code_data['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
            rs = gain / loss
            code_data['RSI_14'] = 100 - (100 / (1 + rs))
                
            # MACDï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
            exp1 = code_data['Close'].ewm(span=12).mean()
            exp2 = code_data['Close'].ewm(span=26).mean()
            code_data['MACD'] = exp1 - exp2
                
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡
            code_data['Volume_MA_20'] = code_data['Volume'].rolling(20).mean()
            code_data['Volume_ratio'] = code_data['Volume'] / code_data['Volume_MA_20']
            code_data['Volume_zscore_20'] = (code_data['Volume'] - code_data['Volume_MA_20']) / code_data['Volume'].rolling(20).std()

            # çµ‚å€¤ãƒ™ãƒ¼ã‚¹ã®è¿½åŠ ç‰¹å¾´é‡
            code_data['Close_gap_prev'] = code_data['Open'] / code_data['Close'].shift(1) - 1
            code_data['Returns_3d'] = code_data['Close'].pct_change(periods=3)
            code_data['Volatility_5'] = code_data['Returns'].rolling(5).std()
            if 'nikkei_change' in code_data.columns:
                code_data['Relative_to_index'] = code_data['Returns'] - code_data['nikkei_change']
            else:
                code_data['Relative_to_index'] = code_data['Returns']

            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã¯æœ€å°é™ã«
            for col in code_data.columns:
                if any(key in col.lower() for key in ['usdjpy', 'vix']):
                    if code_data[col].notna().sum() > 50:
                        code_data[f'{col}_change'] = code_data[col].pct_change()
            
            enhanced_df.loc[mask] = code_data
        
        # ç›®çš„å¤‰æ•°ä½œæˆ
        logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
        enhanced_df['Target'] = 0
        threshold = 1.0 + float(self.target_return)
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy().sort_values('Date')
            next_close = code_data['Close'].shift(-1)
            current_close = code_data['Close']
            target = (next_close / current_close >= threshold).astype(int)
            enhanced_df.loc[code_data.index, 'Target'] = target
        
        # ç„¡é™å€¤ãƒ»æ¬ æå€¤å‡¦ç†
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
        
        logger.info(f"ğŸ”¥ ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶")
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(enhanced_df.columns)}ã‚«ãƒ©ãƒ ")
        logger.info(f"æ­£ä¾‹ç‡: {enhanced_df['Target'].mean():.3f}")
        
        return enhanced_df

    def _compute_scale_pos_weight(self, y: pd.Series) -> float:
        """LightGBMç”¨ã®scale_pos_weightã‚’è¨ˆç®—ï¼ˆãƒ–ãƒ¼ã‚¹ãƒˆä¿‚æ•°ã«å¯¾å¿œï¼‰"""
        # yã¯0/1ã®Seriesã‚’æƒ³å®š
        if not isinstance(y, pd.Series):
            y = pd.Series(y)
        pos_count = float(y.sum())
        total = len(y)
        neg_count = total - pos_count
        if pos_count <= 0 or neg_count <= 0:
            base_weight = 1.0
        else:
            base_weight = neg_count / pos_count

        boost = max(float(self.imbalance_boost), 1e-3)
        adjusted = max(base_weight * boost, 1e-3)
        logger.debug(
            "scale_pos_weightè¨ˆç®—: æ­£ä¾‹=%d, è² ä¾‹=%d, base=%.4f, boost=%.3f, adjusted=%.4f",
            int(pos_count),
            int(neg_count),
            base_weight,
            boost,
            adjusted
        )
        return adjusted

    def _compute_sample_weights(self, y: pd.Series) -> Optional[np.ndarray]:
        """ä¸å‡è¡¡æˆ¦ç•¥ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒ«é‡ã¿ã‚’è¨ˆç®—"""
        strategy = (self.imbalance_strategy or "").lower()
        if strategy in ("", "none", "scale_pos"):
            return None

        if not isinstance(y, pd.Series):
            y = pd.Series(y)

        positives = float(y.sum())
        total = len(y)
        negatives = total - positives
        epsilon = 1e-9

        if strategy == "balanced":
            if positives == 0 or negatives == 0:
                return np.ones(total)
            pos_weight = total / (2 * positives)
            neg_weight = total / (2 * negatives)
            return np.where(y == 1, pos_weight, neg_weight)

        if strategy == "focal":
            gamma = max(self.focal_gamma, 0.0)
            if positives == 0 or negatives == 0:
                return np.ones(total)
            pos_rate = positives / total
            neg_rate = max(1.0 - pos_rate, epsilon)
            pos_weight = (neg_rate) ** gamma
            neg_weight = (pos_rate + epsilon) ** gamma
            # æ­£ä¾‹ã‚’å¼·èª¿ã™ã‚‹ãŸã‚ãƒ–ãƒ¼ã‚¹ãƒˆã‚‚æ›ã‘åˆã‚ã›
            pos_weight *= self.imbalance_boost
            return np.where(y == 1, pos_weight, neg_weight)

        return None

    def _apply_positive_oversample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """æ­£ä¾‹ã‚’å˜ç´”ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§æ°´å¢—ã—"""
        ratio = float(self.positive_oversample_ratio)
        if ratio <= 1.0 or y.sum() == 0:
            return X, y

        positives = y[y == 1]
        current_pos = len(positives)
        target_pos = int(min(len(y) * ratio, current_pos * ratio))
        additional = target_pos - current_pos
        if additional <= 0:
            return X, y

        pos_indices = positives.index.to_numpy()
        sampled_idx = self._rng.choice(pos_indices, size=additional, replace=True)
        X_extra = X.loc[sampled_idx].copy()
        y_extra = y.loc[sampled_idx].copy()

        X_aug = pd.concat([X, X_extra], axis=0, ignore_index=True)
        y_aug = pd.concat([y, y_extra], axis=0, ignore_index=True)
        return X_aug, y_aug
    
    def walk_forward_optimization(self, df: pd.DataFrame, initial_train_size: int = 252*2) -> list:
        """ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ“ˆ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–é–‹å§‹...")
        
        # å…¨é‡ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼ï¼ˆå†ç¾æ€§ã¨é™½æ€§ã‚µãƒ³ãƒ—ãƒ«ã‚’æœ€å¤§é™æ´»ç”¨ï¼‰
        df_sampled = df.copy()
        logger.info(f"ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {len(df_sampled):,}ä»¶ï¼ˆå…¨é‡ä½¿ç”¨ï¼‰")
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df_sampled.sort_values(['Date', 'Code']).copy()
        unique_dates = sorted(df_sorted['Date'].unique())
        
        results = []
        step_size = 21  # ç´„1ãƒ¶æœˆãƒªãƒãƒ©ãƒ³ã‚¹ã§æœŸé–“è§£åƒåº¦ã‚’å‘ä¸Š
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ é¸æŠï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰
        feature_cols = [col for col in df_sorted.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       str(df_sorted[col].dtype) in ['int64', 'float64', 'int32', 'float32']]
        
        # ç‰¹å¾´é‡æ•°åˆ¶é™
        if len(feature_cols) > 30:
            # æ¬ æå€¤ãŒå°‘ãªã„ç‰¹å¾´é‡ã‚’å„ªå…ˆé¸æŠ
            non_null_counts = df_sorted[feature_cols].count()
            top_features = non_null_counts.nlargest(30).index.tolist()
            feature_cols = top_features
        
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
        logger.info(f"è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—å¹…: {step_size}å–¶æ¥­æ—¥ã”ã¨")
        
        # åˆæœŸã‚µã‚¤ã‚ºã‚’å°ã•ãè¨­å®š
        initial_train_size = min(initial_train_size, len(unique_dates) // 3)
        
        for i in range(initial_train_size, len(unique_dates) - step_size, step_size):
            try:
                # æœŸé–“è¨­å®š
                train_end_idx = i
                test_start_idx = i
                test_end_idx = min(i + step_size, len(unique_dates))
                
                train_dates = unique_dates[:train_end_idx]
                test_dates = unique_dates[test_start_idx:test_end_idx]
                
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
                train_df = df_sorted[df_sorted['Date'].isin(train_dates)]
                test_df = df_sorted[df_sorted['Date'].isin(test_dates)]
                
                if len(train_df) == 0 or len(test_df) == 0:
                    continue
                
                # ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°åˆ†é›¢
                X_train = train_df[feature_cols].copy()
                y_train = train_df['Target'].copy()
                X_test = test_df[feature_cols].copy()
                y_test = test_df['Target'].copy()

                # æ¬ æå€¤å‡¦ç†
                X_train = X_train.fillna(method='ffill').fillna(0)
                X_test = X_test.fillna(method='ffill').fillna(0)

                # ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                X_train, y_train = self._apply_positive_oversample(X_train, y_train)
                sample_weight = self._compute_sample_weights(y_train)

                # ç‰¹å¾´é‡é¸æŠï¼ˆæ—¢å­˜æ§‹æˆã‚’è¸è¥²ï¼‰
                selector = SelectKBest(score_func=f_classif, k=min(30, len(feature_cols)))
                X_train_selected = selector.fit_transform(X_train, y_train)
                X_test_selected = selector.transform(X_test)
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train_selected)
                X_test_scaled = scaler.transform(X_test_selected)
                
                # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è»½é‡åŒ–ï¼‰
                scale_pos_weight = self._compute_scale_pos_weight(y_train)
                if self.imbalance_strategy and self.imbalance_strategy.lower() not in ("", "none", "scale_pos"):
                    scale_pos_weight = 1.0

                params = self.model_params.copy()
                params.update({
                    'objective': 'binary',
                    'random_state': 42,
                    'verbose': -1,
                    'scale_pos_weight': scale_pos_weight
                })
                model = lgb.LGBMClassifier(**params)
                
                model.fit(X_train_scaled, y_train, sample_weight=sample_weight)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                
                # è©•ä¾¡
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                
                result = {
                    'period': f"{train_dates[-1].strftime('%Y-%m-%d')} to {test_dates[-1].strftime('%Y-%m-%d')}",
                    'train_size': len(train_df),
                    'test_size': len(test_df),
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'positive_rate': y_test.mean()
                }
                
                results.append(result)
                logger.info(f"æœŸé–“ {result['period']}: ç²¾åº¦={accuracy:.4f}, é©åˆç‡={precision:.4f}")
                
            except Exception as e:
                logger.warning(f"æœŸé–“ {i} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return results
    
    def train_final_model(self, df: pd.DataFrame) -> dict:
        """æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ¤– æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹...")
        
        # å…¨é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å»ƒæ­¢ï¼‰
        df_sampled = df.copy()
        logger.info(f"æœ€çµ‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_sampled):,}ä»¶ï¼ˆå…¨é‡ä½¿ç”¨ï¼‰")
        
        # ç‰¹å¾´é‡æº–å‚™
        feature_cols = [col for col in df_sampled.columns 
                       if col not in ['Date', 'Code', 'Target'] and 
                       str(df_sampled[col].dtype) in ['int64', 'float64', 'int32', 'float32']]
        
        # ç‰¹å¾´é‡æ•°åˆ¶é™
        if len(feature_cols) > 25:
            non_null_counts = df_sampled[feature_cols].count()
            feature_cols = non_null_counts.nlargest(25).index.tolist()
        
        X = df_sampled[feature_cols].fillna(method='ffill').fillna(0)
        y = df_sampled['Target']
        X, y = self._apply_positive_oversample(X, y)

        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæœ€å¾Œ20%ã‚’ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        df_sorted = df_sampled.sort_values('Date')
        split_idx = int(len(df_sorted) * 0.8)

        X_train = X[:split_idx]
        X_test = X[split_idx:]
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆæ—¢å­˜æ§‹æˆã‚’è¸è¥²ï¼‰
        selector = SelectKBest(score_func=f_classif, k=min(30, len(feature_cols)))
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆæ—¢å­˜æ§‹æˆã‚’è¸è¥²ï¼‰
        scale_pos_weight = self._compute_scale_pos_weight(y_train)
        if self.imbalance_strategy and self.imbalance_strategy.lower() not in ("", "none", "scale_pos"):
            scale_pos_weight = 1.0
        sample_weight = self._compute_sample_weights(y_train)

        params = self.model_params.copy()
        params.update({
            'objective': 'binary',
            'random_state': 42,
            'verbose': -1,
            'scale_pos_weight': scale_pos_weight
        })
        model = lgb.LGBMClassifier(**params)
        
        logger.info(f"ã‚¯ãƒ©ã‚¹é‡ã¿ (scale_pos_weight): {scale_pos_weight:.2f}")

        model.fit(X_train_scaled, y_train, sample_weight=sample_weight)
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

        calibrator = LogisticRegression(solver='lbfgs')
        calibrator.fit(y_pred_proba.reshape(-1, 1), y_test)
        calibrated_proba = calibrator.predict_proba(y_pred_proba.reshape(-1, 1))[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        logger.info(f"ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        logger.info(f"  ç²¾åº¦: {accuracy:.4f}")
        logger.info(f"  é©åˆç‡: {precision:.4f}")
        logger.info(f"  å†ç¾ç‡: {recall:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_data = {
            'model': model,
            'scaler': scaler,
            'selector': selector,
            'feature_cols': feature_cols,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'target_return': self.target_return,
            'model_params': self.model_params,
            'imbalance_boost': self.imbalance_boost,
            'imbalance_strategy': self.imbalance_strategy,
            'focal_gamma': self.focal_gamma,
            'positive_oversample_ratio': self.positive_oversample_ratio,
            'calibration': {
                'coef': calibrator.coef_[0][0],
                'intercept': calibrator.intercept_[0]
            }
        }
        
        model_file = self.output_dir / f"close_model_v1_{accuracy:.4f}acc_{timestamp}.joblib"
        joblib.dump(model_data, model_file)
        logger.info(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_data
    
    def run_enhanced_system(self):
        """æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        logger.info("ğŸš€ Close-to-Close Precision System V1 å®Ÿè¡Œé–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            df = self.load_and_integrate_data()
            
            # ç‰¹å¾´é‡ä½œæˆ
            enhanced_df = self.create_enhanced_features(df)
            
            # ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–
            wfo_results = self.walk_forward_optimization(enhanced_df)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            final_model = self.train_final_model(enhanced_df)
            
            # çµæœçµ±è¨ˆ
            if wfo_results:
                wfo_accuracies = [r['accuracy'] for r in wfo_results]
                wfo_mean_acc = np.mean(wfo_accuracies)
                wfo_std_acc = np.std(wfo_accuracies)
                
                logger.info(f"\nğŸ“Š ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–çµæœ:")
                logger.info(f"  æœŸé–“æ•°: {len(wfo_results)}")
                logger.info(f"  å¹³å‡ç²¾åº¦: {wfo_mean_acc:.4f} Â± {wfo_std_acc:.4f}")
                logger.info(f"  æœ€é«˜ç²¾åº¦: {max(wfo_accuracies):.4f}")
                logger.info(f"  æœ€ä½ç²¾åº¦: {min(wfo_accuracies):.4f}")
            
            # çµæœä¿å­˜
            results = {
                'final_model_accuracy': final_model['accuracy'],
                'wfo_mean_accuracy': wfo_mean_acc if wfo_results else 0,
                'wfo_std_accuracy': wfo_std_acc if wfo_results else 0,
                'wfo_results': wfo_results,
                'data_size': len(enhanced_df),
                'feature_count': len(final_model['feature_cols']),
                'target_return': self.target_return,
                'model_params': self.model_params,
                'external_data_integrated': os.path.exists(self.external_file),
                'imbalance_strategy': self.imbalance_strategy,
                'positive_oversample_ratio': self.positive_oversample_ratio
            }
            
            results_file = self.output_dir / f"close_results_v1_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
            joblib.dump(results, results_file)
            
            logger.info(f"ğŸ‰ Close-to-Close Precision System V1 å®Œäº†!")
            logger.info(f"æœ€çµ‚ç²¾åº¦: {final_model['accuracy']:.4f}")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {'âœ…' if results['external_data_integrated'] else 'âŒ'}")
            logger.info(f"çµæœä¿å­˜: {results_file}")
            
            return results
            
        except Exception as e:
            logger.error(f"ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description='Close-to-Close Precision System V1')
    parser.add_argument('--target-return', type=float, default=0.01, help='çµ‚å€¤ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šé–¾å€¤ (ä¾‹: 0.01=+1%)')
    parser.add_argument('--imbalance-boost', type=float, default=1.0, help='ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã¨ã—ã¦ scale_pos_weight ã«æ›ã‘ã‚‹å€ç‡')
    parser.add_argument('--imbalance-strategy', type=str, default='scale_pos', choices=['scale_pos', 'balanced', 'focal', 'none'], help='è¿½åŠ ã®ã‚µãƒ³ãƒ—ãƒ«é‡ã¿æˆ¦ç•¥')
    parser.add_argument('--focal-gamma', type=float, default=2.0, help='focalæˆ¦ç•¥ç”¨ã‚¬ãƒ³ãƒå€¤ (imbalance-strategy=focal ã®ã¿ä½¿ç”¨)')
    parser.add_argument('--positive-oversample-ratio', type=float, default=1.0, help='æ­£ä¾‹ã®å˜ç´”ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å€ç‡ (>1ã§å¢—å¹…)')
    args = parser.parse_args()

    system = CloseReturnPrecisionSystemV1(
        target_return=args.target_return,
        imbalance_boost=args.imbalance_boost,
        imbalance_strategy=args.imbalance_strategy,
        focal_gamma=args.focal_gamma,
        positive_oversample_ratio=args.positive_oversample_ratio,
    )
    results = system.run_enhanced_system()
    
    if results:
        print(f"\nâœ… Close-to-Close Precision System V1 å®Ÿè¡Œå®Œäº†!")
        print(f"ğŸ“Š æœ€çµ‚ç²¾åº¦: {results['final_model_accuracy']:.4f}")
        if results['wfo_mean_accuracy'] > 0:
            print(f"ğŸ“ˆ ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å¹³å‡ç²¾åº¦: {results['wfo_mean_accuracy']:.4f}")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {'æˆåŠŸ' if results['external_data_integrated'] else 'å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãªã—'}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿é‡: {results['data_size']:,}ä»¶")
        print(f"ğŸ”§ ç‰¹å¾´é‡æ•°: {results['feature_count']}å€‹")
    else:
        print("\nâŒ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
