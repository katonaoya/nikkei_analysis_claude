#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç©¶æ¥µã®é«˜ç²¾åº¦å®Ÿç¾ã‚·ã‚¹ãƒ†ãƒ  - ã‚ã‚‰ã‚†ã‚‹è©¦è¡ŒéŒ¯èª¤ã‚’å®Ÿè£…
å®Œå…¨å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®æœ€é«˜ç²¾åº¦ã‚’ç›®æŒ‡ã™åŒ…æ‹¬çš„æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import os
import joblib
import warnings
warnings.filterwarnings('ignore')

# MLé–¢é€£
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel
from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from lightgbm import LGBMClassifier
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier, BaggingClassifier
from sklearn.calibration import CalibratedClassifierCV
import optuna

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UltimatePrecisionSystem:
    """ç©¶æ¥µã®é«˜ç²¾åº¦å®Ÿç¾ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, data_file: str = None):
        """åˆæœŸåŒ–"""
        if data_file is None:
            data_file = "data/processed/nikkei225_complete_225stocks_20250909_230649.parquet"
        
        self.data_file = data_file
        self.df = None
        self.models = {}
        self.best_model = None
        self.best_score = 0.0
        self.experiment_results = []
        
        logger.info("ğŸ¯ ç©¶æ¥µã®é«˜ç²¾åº¦å®Ÿç¾ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file}")
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        self.df = pd.read_parquet(self.data_file)
        self.df['Date'] = pd.to_datetime(self.df['Date'])
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df):,}ä»¶, {self.df['Code'].nunique()}éŠ˜æŸ„")
        return True
    
    def create_ultimate_features(self):
        """ğŸ”¥ ç©¶æ¥µã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        logger.info("ğŸ”¥ ç©¶æ¥µã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        
        enhanced_df = self.df.copy()
        result_dfs = []
        
        for code in enhanced_df['Code'].unique():
            code_df = enhanced_df[enhanced_df['Code'] == code].copy()
            code_df = code_df.sort_values('Date')
            
            # ğŸ†• åŸºæœ¬ãƒªã‚¿ãƒ¼ãƒ³ç³»ç‰¹å¾´é‡ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            for period in [1, 2, 3, 5, 10, 20, 30]:
                code_df[f'Returns_{period}d'] = code_df['Close'].pct_change(period)
                code_df[f'LogReturns_{period}d'] = np.log(code_df['Close'] / code_df['Close'].shift(period))
            
            # ğŸ†• æ‹¡å¼µç§»å‹•å¹³å‡ï¼ˆ11ç¨®é¡ï¼‰
            windows = [3, 5, 7, 10, 15, 20, 25, 30, 50, 75, 100]
            for window in windows:
                code_df[f'MA_{window}'] = code_df['Close'].rolling(window).mean()
                code_df[f'MA_{window}_ratio'] = code_df['Close'] / code_df[f'MA_{window}']
                code_df[f'MA_{window}_slope'] = code_df[f'MA_{window}'].diff(5)
                code_df[f'MA_{window}_distance'] = (code_df['Close'] - code_df[f'MA_{window}']) / code_df['Close']
            
            # ğŸ†• MAäº¤å·®ã‚·ã‚°ãƒŠãƒ«
            code_df['MA_5_20_cross'] = np.where(code_df['MA_5'] > code_df['MA_20'], 1, 0)
            code_df['MA_10_30_cross'] = np.where(code_df['MA_10'] > code_df['MA_30'], 1, 0)
            code_df['MA_20_50_cross'] = np.where(code_df['MA_20'] > code_df['MA_50'], 1, 0)
            
            # ğŸ†• æ‹¡å¼µEMA
            for window in [5, 10, 20, 30, 50]:
                code_df[f'EMA_{window}'] = code_df['Close'].ewm(span=window).mean()
                code_df[f'EMA_{window}_ratio'] = code_df['Close'] / code_df[f'EMA_{window}']
            
            # ğŸ†• ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»ï¼ˆ7ç¨®é¡ï¼‰
            for window in [5, 10, 15, 20, 30, 60, 120]:
                code_df[f'Volatility_{window}'] = code_df['Returns_1d'].rolling(window).std()
                code_df[f'VolatilityRank_{window}'] = code_df[f'Volatility_{window}'].rolling(252).rank() / 252
            
            # ğŸ†• ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆï¼ˆZ-Score, Percentileï¼‰
            for window in [10, 20, 50, 100]:
                returns_rolling = code_df['Returns_1d'].rolling(window)
                code_df[f'Returns_zscore_{window}'] = (code_df['Returns_1d'] - returns_rolling.mean()) / returns_rolling.std()
                code_df[f'Returns_percentile_{window}'] = code_df['Returns_1d'].rolling(window).rank() / window
            
            # ğŸ†• RSIå¤‰ç¨®ï¼ˆ5ç¨®é¡ï¼‰
            for window in [5, 9, 14, 21, 28]:
                delta = code_df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
                rs = gain / loss
                code_df[f'RSI_{window}'] = 100 - (100 / (1 + rs))
                
                # RSIæ´¾ç”ŸæŒ‡æ¨™
                code_df[f'RSI_{window}_oversold'] = (code_df[f'RSI_{window}'] < 30).astype(int)
                code_df[f'RSI_{window}_overbought'] = (code_df[f'RSI_{window}'] > 70).astype(int)
            
            # ğŸ†• MACDå¤‰ç¨®ï¼ˆ3ç¨®é¡ï¼‰
            macd_configs = [(8, 21, 5), (12, 26, 9), (19, 39, 9)]
            for fast, slow, signal in macd_configs:
                exp1 = code_df['Close'].ewm(span=fast).mean()
                exp2 = code_df['Close'].ewm(span=slow).mean()
                code_df[f'MACD_{fast}_{slow}'] = exp1 - exp2
                code_df[f'MACD_signal_{fast}_{slow}'] = code_df[f'MACD_{fast}_{slow}'].ewm(span=signal).mean()
                code_df[f'MACD_histogram_{fast}_{slow}'] = code_df[f'MACD_{fast}_{slow}'] - code_df[f'MACD_signal_{fast}_{slow}']
                code_df[f'MACD_cross_{fast}_{slow}'] = np.where(code_df[f'MACD_{fast}_{slow}'] > code_df[f'MACD_signal_{fast}_{slow}'], 1, 0)
            
            # ğŸ†• ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰å¤‰ç¨®
            for window in [10, 20, 30]:
                for std_mult in [1, 1.5, 2, 2.5, 3]:
                    rolling_mean = code_df['Close'].rolling(window).mean()
                    rolling_std = code_df['Close'].rolling(window).std()
                    code_df[f'BB_upper_{window}_{std_mult}'] = rolling_mean + (rolling_std * std_mult)
                    code_df[f'BB_lower_{window}_{std_mult}'] = rolling_mean - (rolling_std * std_mult)
                    code_df[f'BB_ratio_{window}_{std_mult}'] = (code_df['Close'] - code_df[f'BB_lower_{window}_{std_mult}']) / (code_df[f'BB_upper_{window}_{std_mult}'] - code_df[f'BB_lower_{window}_{std_mult}'])
                    code_df[f'BB_squeeze_{window}_{std_mult}'] = ((code_df[f'BB_upper_{window}_{std_mult}'] - code_df[f'BB_lower_{window}_{std_mult}']) / rolling_mean).rolling(20).min()
            
            # ğŸ†• ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹å¤‰ç¨®
            for window in [9, 14, 21, 28]:
                low_min = code_df['Low'].rolling(window).min()
                high_max = code_df['High'].rolling(window).max()
                code_df[f'Stoch_K_{window}'] = 100 * (code_df['Close'] - low_min) / (high_max - low_min)
                code_df[f'Stoch_D_{window}'] = code_df[f'Stoch_K_{window}'].rolling(3).mean()
                code_df[f'Stoch_cross_{window}'] = np.where(code_df[f'Stoch_K_{window}'] > code_df[f'Stoch_D_{window}'], 1, 0)
            
            # ğŸ†• ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
            code_df['High_Low_ratio'] = code_df['High'] / code_df['Low']
            code_df['Open_Close_ratio'] = code_df['Open'] / code_df['Close']
            code_df['Close_Open_ratio'] = code_df['Close'] / code_df['Open']
            code_df['Upper_shadow'] = (code_df['High'] - np.maximum(code_df['Open'], code_df['Close'])) / code_df['Close']
            code_df['Lower_shadow'] = (np.minimum(code_df['Open'], code_df['Close']) - code_df['Low']) / code_df['Close']
            code_df['Body_size'] = abs(code_df['Close'] - code_df['Open']) / code_df['Close']
            code_df['Doji'] = (abs(code_df['Close'] - code_df['Open']) / code_df['Close'] < 0.01).astype(int)
            
            # ğŸ†• ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆ†æ
            code_df['Volume_MA_10'] = code_df['Volume'].rolling(10).mean()
            code_df['Volume_MA_20'] = code_df['Volume'].rolling(20).mean()
            code_df['Volume_ratio_10'] = code_df['Volume'] / code_df['Volume_MA_10']
            code_df['Volume_ratio_20'] = code_df['Volume'] / code_df['Volume_MA_20']
            code_df['Price_Volume_Trend'] = (code_df['Returns_1d'] * code_df['Volume']).rolling(10).sum()
            
            # ğŸ†• OBVå¤‰ç¨®
            obv_volume = code_df['Volume'] * np.where(code_df['Close'] > code_df['Close'].shift(1), 1, 
                                                     np.where(code_df['Close'] < code_df['Close'].shift(1), -1, 0))
            code_df['OBV'] = obv_volume.cumsum()
            for window in [10, 20, 30]:
                code_df[f'OBV_MA_{window}'] = code_df['OBV'].rolling(window).mean()
                code_df[f'OBV_ratio_{window}'] = code_df['OBV'] / code_df[f'OBV_MA_{window}']
            
            # ğŸ†• ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹
            for window in [20, 50, 100]:
                code_df[f'Support_{window}'] = code_df['Low'].rolling(window).min()
                code_df[f'Resistance_{window}'] = code_df['High'].rolling(window).max()
                code_df[f'Support_distance_{window}'] = (code_df['Close'] - code_df[f'Support_{window}']) / code_df['Close']
                code_df[f'Resistance_distance_{window}'] = (code_df[f'Resistance_{window}'] - code_df['Close']) / code_df['Close']
            
            # ğŸ†• ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ãƒãƒ£ãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
            for window in [5, 10, 15]:
                code_df[f'Local_max_{window}'] = (code_df['High'] == code_df['High'].rolling(window, center=True).max()).astype(int)
                code_df[f'Local_min_{window}'] = (code_df['Low'] == code_df['Low'].rolling(window, center=True).min()).astype(int)
            
            # ğŸ†• ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
            for period in [5, 10, 20, 30]:
                code_df[f'Momentum_{period}'] = code_df['Close'] / code_df['Close'].shift(period) - 1
                code_df[f'ROC_{period}'] = (code_df['Close'] - code_df['Close'].shift(period)) / code_df['Close'].shift(period)
            
            # ğŸ†• æ™‚ç³»åˆ—ç‰¹å¾´é‡
            code_df['DayOfWeek'] = code_df['Date'].dt.dayofweek
            code_df['Month'] = code_df['Date'].dt.month
            code_df['Quarter'] = code_df['Date'].dt.quarter
            code_df['IsMonthEnd'] = (code_df['Date'].dt.day > 25).astype(int)
            code_df['IsQuarterEnd'] = ((code_df['Date'].dt.month % 3 == 0) & (code_df['Date'].dt.day > 25)).astype(int)
            
            # ğŸ†• å¸‚å ´æ§‹é€ ç‰¹å¾´é‡
            code_df['Gap'] = (code_df['Open'] - code_df['Close'].shift(1)) / code_df['Close'].shift(1)
            code_df['Gap_up'] = (code_df['Gap'] > 0.01).astype(int)
            code_df['Gap_down'] = (code_df['Gap'] < -0.01).astype(int)
            
            # ğŸ†• é€£ç¶šæ€§ç‰¹å¾´é‡
            for period in [2, 3, 5]:
                code_df[f'Consecutive_up_{period}'] = (code_df['Returns_1d'] > 0).rolling(period).sum()
                code_df[f'Consecutive_down_{period}'] = (code_df['Returns_1d'] < 0).rolling(period).sum()
            
            result_dfs.append(code_df)
        
        # çµåˆ
        enhanced_df = pd.concat(result_dfs, ignore_index=True)
        
        # ç›®çš„å¤‰æ•°ä½œæˆï¼ˆ95.45%ç²¾åº¦ã¨åŒã˜å®šç¾©ï¼‰
        logger.info("ç›®çš„å¤‰æ•°ä½œæˆ...")
        enhanced_df['Target'] = 0
        
        for code in enhanced_df['Code'].unique():
            mask = enhanced_df['Code'] == code
            code_data = enhanced_df[mask].copy()
            next_high = code_data['High'].shift(-1)
            prev_close = code_data['Close'].shift(1)
            enhanced_df.loc[mask, 'Target'] = (next_high / prev_close > 1.01).astype(int)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
        enhanced_df = enhanced_df.dropna(subset=['Close', 'Date', 'Code', 'Target'])
        enhanced_df = enhanced_df.fillna(method='ffill').fillna(method='bfill').dropna()
        
        self.df = enhanced_df
        logger.info(f"ğŸ”¥ ç©¶æ¥µç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(enhanced_df):,}ä»¶")
        logger.info(f"ç‰¹å¾´é‡æ•°: {len(enhanced_df.columns)}ã‚«ãƒ©ãƒ ")
        
        positive_rate = enhanced_df['Target'].mean()
        logger.info(f"æ­£ä¾‹ç‡: {positive_rate:.3f} ({positive_rate:.1%})")
        
        return enhanced_df
    
    def get_features_and_target(self):
        """ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™"""
        exclude_cols = ['Date', 'Code', 'CompanyName', 'MatchMethod', 'ApiCode', 'Target']
        feature_cols = [col for col in self.df.columns if col not in exclude_cols]
        numeric_cols = self.df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        X = self.df[numeric_cols].replace([np.inf, -np.inf], np.nan).fillna(0)
        y = self.df['Target']
        
        return X, y, numeric_cols
    
    def experiment_1_advanced_lightgbm(self):
        """å®Ÿé¨“1: é«˜åº¦LightGBMæœ€é©åŒ–"""
        logger.info("ğŸ§ª å®Ÿé¨“1: é«˜åº¦LightGBMæœ€é©åŒ–é–‹å§‹...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # Optunaæœ€é©åŒ–
        def objective(trial):
            params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'n_estimators': trial.suggest_int('n_estimators', 500, 1500),
                'max_depth': trial.suggest_int('max_depth', 6, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'subsample': trial.suggest_float('subsample', 0.6, 0.95),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.3),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 0.3),
                'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
                'random_state': 42,
                'verbose': -1
            }
            
            # ç‰¹å¾´é‡é¸æŠ
            selector = SelectKBest(score_func=f_classif, k=50)
            X_train_selected = selector.fit_transform(X_train, y_train)
            X_test_selected = selector.transform(X_test)
            
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model = LGBMClassifier(**params)
            model.fit(X_train_selected, y_train)
            
            # äºˆæ¸¬
            pred_proba = model.predict_proba(X_test_selected)[:, 1]
            
            # æ—¥åˆ¥ç²¾åº¦è©•ä¾¡
            test_df_sample = df_sorted[test_mask].copy()
            test_df_sample['PredProba'] = pred_proba
            
            daily_precisions = []
            for date in test_df_sample['Date'].unique():
                daily_data = test_df_sample[test_df_sample['Date'] == date]
                if len(daily_data) >= 3:
                    top3 = daily_data.nlargest(3, 'PredProba')
                    precision = top3['Target'].mean()
                    daily_precisions.append(precision)
            
            return np.mean(daily_precisions) if daily_precisions else 0.0
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=20)  # 20å›è©¦è¡Œ
        
        best_params = study.best_params
        best_score = study.best_value
        
        logger.info(f"ğŸ¯ å®Ÿé¨“1çµæœ: æœ€é«˜ç²¾åº¦ = {best_score:.4f} ({best_score:.2%})")
        
        return {
            'name': 'Advanced LightGBM',
            'score': best_score,
            'params': best_params,
            'model_type': 'lightgbm'
        }
    
    def experiment_2_ensemble_voting(self):
        """å®Ÿé¨“2: å¤šæ§˜æ€§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨"""
        logger.info("ğŸ§ª å®Ÿé¨“2: å¤šæ§˜æ€§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨é–‹å§‹...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # ç‰¹å¾´é‡é¸æŠ
        selector = SelectKBest(score_func=f_classif, k=40)
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # å¤šæ§˜ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
        base_models = [
            ('lgbm', LGBMClassifier(n_estimators=400, max_depth=8, learning_rate=0.05, random_state=42, verbose=-1)),
            ('xgb', xgb.XGBClassifier(n_estimators=400, max_depth=7, learning_rate=0.05, random_state=42)),
            ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)),
            ('et', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)),
            ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42))
        ]
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ
        ensemble = VotingClassifier(estimators=base_models, voting='soft')
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # å­¦ç¿’
        ensemble.fit(X_train_scaled, y_train)
        
        # äºˆæ¸¬
        pred_proba = ensemble.predict_proba(X_test_scaled)[:, 1]
        
        # æ—¥åˆ¥ç²¾åº¦è©•ä¾¡
        test_df_sample = df_sorted[test_mask].copy()
        test_df_sample['PredProba'] = pred_proba
        
        daily_precisions = []
        for date in test_df_sample['Date'].unique():
            daily_data = test_df_sample[test_df_sample['Date'] == date]
            if len(daily_data) >= 3:
                top3 = daily_data.nlargest(3, 'PredProba')
                precision = top3['Target'].mean()
                daily_precisions.append(precision)
        
        score = np.mean(daily_precisions) if daily_precisions else 0.0
        
        logger.info(f"ğŸ¯ å®Ÿé¨“2çµæœ: ç²¾åº¦ = {score:.4f} ({score:.2%})")
        
        return {
            'name': 'Ensemble Voting',
            'score': score,
            'model': ensemble,
            'scaler': scaler,
            'selector': selector
        }
    
    def experiment_3_stacking_ensemble(self):
        """å®Ÿé¨“3: ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
        logger.info("ğŸ§ª å®Ÿé¨“3: ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é–‹å§‹...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # ç‰¹å¾´é‡é¸æŠ
        selector = SelectKBest(score_func=f_classif, k=45)
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)
        
        # ãƒ¬ãƒ™ãƒ«1ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šæ§˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
        level1_models = {
            'lgbm1': LGBMClassifier(n_estimators=300, max_depth=7, learning_rate=0.05, subsample=0.8, random_state=42, verbose=-1),
            'lgbm2': LGBMClassifier(n_estimators=500, max_depth=9, learning_rate=0.03, subsample=0.9, random_state=123, verbose=-1),
            'xgb': xgb.XGBClassifier(n_estimators=350, max_depth=6, learning_rate=0.04, random_state=42),
            'rf': RandomForestClassifier(n_estimators=250, max_depth=8, min_samples_split=10, random_state=42, n_jobs=-1),
            'et': ExtraTreesClassifier(n_estimators=200, max_depth=12, min_samples_split=8, random_state=42, n_jobs=-1)
        }
        
        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ¬ãƒ™ãƒ«1äºˆæ¸¬ã‚’ä½œæˆ
        tscv = TimeSeriesSplit(n_splits=3)
        level1_train_preds = np.zeros((len(X_train_scaled), len(level1_models)))
        level1_test_preds = np.zeros((len(X_test_scaled), len(level1_models)))
        
        for i, (name, model) in enumerate(level1_models.items()):
            model_train_preds = np.zeros(len(X_train_scaled))
            
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬
            for train_idx, val_idx in tscv.split(X_train_scaled):
                model.fit(X_train_scaled[train_idx], y_train.iloc[train_idx])
                model_train_preds[val_idx] = model.predict_proba(X_train_scaled[val_idx])[:, 1]
            
            level1_train_preds[:, i] = model_train_preds
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’ï¼‰
            model.fit(X_train_scaled, y_train)
            level1_test_preds[:, i] = model.predict_proba(X_test_scaled)[:, 1]
        
        # ãƒ¬ãƒ™ãƒ«2ãƒ¡ã‚¿å­¦ç¿’å™¨
        meta_learner = LogisticRegression(random_state=42, max_iter=1000)
        meta_learner.fit(level1_train_preds, y_train)
        
        # æœ€çµ‚äºˆæ¸¬
        pred_proba = meta_learner.predict_proba(level1_test_preds)[:, 1]
        
        # æ—¥åˆ¥ç²¾åº¦è©•ä¾¡
        test_df_sample = df_sorted[test_mask].copy()
        test_df_sample['PredProba'] = pred_proba
        
        daily_precisions = []
        for date in test_df_sample['Date'].unique():
            daily_data = test_df_sample[test_df_sample['Date'] == date]
            if len(daily_data) >= 3:
                top3 = daily_data.nlargest(3, 'PredProba')
                precision = top3['Target'].mean()
                daily_precisions.append(precision)
        
        score = np.mean(daily_precisions) if daily_precisions else 0.0
        
        logger.info(f"ğŸ¯ å®Ÿé¨“3çµæœ: ç²¾åº¦ = {score:.4f} ({score:.2%})")
        
        return {
            'name': 'Stacking Ensemble',
            'score': score,
            'level1_models': level1_models,
            'meta_learner': meta_learner,
            'scaler': scaler,
            'selector': selector
        }
    
    def experiment_4_calibrated_models(self):
        """å®Ÿé¨“4: ç¢ºç‡æ ¡æ­£ãƒ¢ãƒ‡ãƒ«"""
        logger.info("ğŸ§ª å®Ÿé¨“4: ç¢ºç‡æ ¡æ­£ãƒ¢ãƒ‡ãƒ«é–‹å§‹...")
        
        X, y, feature_cols = self.get_features_and_target()
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        df_sorted = self.df.sort_values('Date')
        latest_date = df_sorted['Date'].max()
        test_start = latest_date - timedelta(days=35)
        
        train_mask = df_sorted['Date'] < test_start
        test_mask = df_sorted['Date'] >= test_start
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # ç‰¹å¾´é‡é¸æŠ
        selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold='median')
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_test_selected = selector.transform(X_test)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
        base_model = LGBMClassifier(
            n_estimators=600,
            max_depth=10,
            learning_rate=0.03,
            subsample=0.85,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbose=-1
        )
        
        # ç¢ºç‡æ ¡æ­£ï¼ˆIsotonicå›å¸°ï¼‰
        calibrated_model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)
        
        # å­¦ç¿’
        calibrated_model.fit(X_train_selected, y_train)
        
        # äºˆæ¸¬
        pred_proba = calibrated_model.predict_proba(X_test_selected)[:, 1]
        
        # æ—¥åˆ¥ç²¾åº¦è©•ä¾¡
        test_df_sample = df_sorted[test_mask].copy()
        test_df_sample['PredProba'] = pred_proba
        
        daily_precisions = []
        for date in test_df_sample['Date'].unique():
            daily_data = test_df_sample[test_df_sample['Date'] == date]
            if len(daily_data) >= 3:
                top3 = daily_data.nlargest(3, 'PredProba')
                precision = top3['Target'].mean()
                daily_precisions.append(precision)
        
        score = np.mean(daily_precisions) if daily_precisions else 0.0
        
        logger.info(f"ğŸ¯ å®Ÿé¨“4çµæœ: ç²¾åº¦ = {score:.4f} ({score:.2%})")
        
        return {
            'name': 'Calibrated Model',
            'score': score,
            'model': calibrated_model,
            'selector': selector
        }
    
    def run_all_experiments(self):
        """å…¨å®Ÿé¨“å®Ÿè¡Œ"""
        logger.info("ğŸš€ ç©¶æ¥µã®é«˜ç²¾åº¦å®Ÿç¾: å…¨å®Ÿé¨“é–‹å§‹!")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            self.load_data()
            self.create_ultimate_features()
            
            # å…¨å®Ÿé¨“å®Ÿè¡Œ
            experiments = [
                self.experiment_1_advanced_lightgbm,
                self.experiment_2_ensemble_voting,
                self.experiment_3_stacking_ensemble,
                self.experiment_4_calibrated_models
            ]
            
            results = []
            for i, experiment in enumerate(experiments, 1):
                logger.info(f"\n{'='*50}")
                logger.info(f"å®Ÿé¨“ {i}/{len(experiments)} å®Ÿè¡Œä¸­...")
                logger.info(f"{'='*50}")
                
                result = experiment()
                results.append(result)
                
                if result['score'] > self.best_score:
                    self.best_score = result['score']
                    self.best_model = result
                    logger.info(f"ğŸ‰ æ–°è¨˜éŒ²æ›´æ–°! æœ€é«˜ç²¾åº¦: {self.best_score:.4f} ({self.best_score:.2%})")
            
            # çµæœã¾ã¨ã‚
            logger.info(f"\n{'='*60}")
            logger.info("ğŸ† å…¨å®Ÿé¨“çµæœ:")
            logger.info(f"{'='*60}")
            
            for result in results:
                logger.info(f"{result['name']}: {result['score']:.4f} ({result['score']:.2%})")
            
            logger.info(f"\nğŸ¥‡ æœ€é«˜ç²¾åº¦: {self.best_model['name']} = {self.best_score:.4f} ({self.best_score:.2%})")
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            self.save_best_model()
            
            return results
            
        except Exception as e:
            logger.error(f"å®Ÿé¨“å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_best_model(self):
        """æœ€é«˜ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        if self.best_model is None:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        precision_str = f"{self.best_score:.4f}".replace('.', '')
        
        os.makedirs("models/ultimate", exist_ok=True)
        
        model_file = f"models/ultimate/ultimate_model_{precision_str}precision_{timestamp}.joblib"
        
        model_data = {
            'best_model': self.best_model,
            'best_score': self.best_score,
            'data_info': {
                'total_records': len(self.df),
                'n_companies': self.df['Code'].nunique(),
                'feature_count': len(self.df.columns),
                'date_range': f"{self.df['Date'].min()} - {self.df['Date'].max()}"
            },
            'experiment_type': 'ultimate_precision_system'
        }
        
        joblib.dump(model_data, model_file)
        logger.info(f"ğŸ¯ æœ€é«˜ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_file}")
        
        return model_file

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    system = UltimatePrecisionSystem()
    results = system.run_all_experiments()
    
    if results:
        print(f"\nğŸ‰ ç©¶æ¥µã®é«˜ç²¾åº¦å®Ÿç¾ã‚·ã‚¹ãƒ†ãƒ å®Œäº†!")
        print(f"ğŸ¥‡ æœ€é«˜é”æˆç²¾åº¦: {system.best_score:.2%}")
        print(f"ğŸ† æœ€å„ªç§€æ‰‹æ³•: {system.best_model['name']}")
        print(f"ğŸ“Š å®Ÿé¨“ç·æ•°: {len(results)}å®Ÿé¨“")
    else:
        print("\nâŒ ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå¤±æ•—")

if __name__ == "__main__":
    main()