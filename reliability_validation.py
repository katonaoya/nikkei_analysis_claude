#!/usr/bin/env python3
"""
å®Ÿé‹ç”¨ä¿¡é ¼æ€§æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
83.33%ç²¾åº¦ã®å®Ÿé‹ç”¨å†ç¾æ€§ã‚’å¾¹åº•æ¤œè¨¼
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import lightgbm as lgb
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import classification_report, confusion_matrix
from yahoo_market_data import YahooMarketData
import warnings
warnings.filterwarnings('ignore')

from loguru import logger

class ReliabilityValidator:
    """ä¿¡é ¼æ€§æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_data_file = "data/processed/integrated_with_external.parquet"
        self.validation_results = {}
        
    def load_validation_data(self) -> pd.DataFrame:
        """æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        logger.info("ğŸ” ä¿¡é ¼æ€§æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
        df = pd.read_parquet(self.base_data_file)
        
        if 'date' in df.columns:
            df['Date'] = pd.to_datetime(df['date'])
        if 'code' in df.columns:
            df['Stock'] = df['code'].astype(str)
        
        # é«˜å“è³ªéŠ˜æŸ„é¸æŠï¼ˆå®Ÿé‹ç”¨ç›¸å½“ï¼‰
        stock_counts = df['Stock'].value_counts()
        reliable_stocks = stock_counts[stock_counts >= 400].head(150).index.tolist()
        df = df[df['Stock'].isin(reliable_stocks)].copy()
        
        logger.info(f"æ¤œè¨¼å¯¾è±¡éŠ˜æŸ„: {len(reliable_stocks)}éŠ˜æŸ„")
        
        # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆæˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³å†ç¾ï¼‰
        market_data = YahooMarketData()
        data_dict = market_data.get_all_market_data(period="2y")
        
        if data_dict:
            market_features = market_data.calculate_market_features(data_dict)
            if not market_features.empty:
                # æ—¥ä»˜çµ±ä¸€ï¼ˆUTCã§å¤‰æ›ã—ã¦ã‹ã‚‰dateã«å¤‰æ›ï¼‰
                df['Date'] = pd.to_datetime(df['Date']).dt.date
                market_features['Date'] = pd.to_datetime(market_features['Date'], utc=True).dt.date
                
                try:
                    df = df.merge(market_features, on='Date', how='left')
                    market_cols = [col for col in market_features.columns if col != 'Date']
                    df[market_cols] = df[market_cols].fillna(method='ffill').fillna(0)
                    logger.success("âœ… ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
                except:
                    logger.warning("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        return df
    
    def create_validation_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´é‡å†ç¾"""
        logger.info("ğŸ”§ æ¤œè¨¼ç”¨ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        
        enhanced_df = df.copy()
        enhanced_df = enhanced_df.sort_values(['Stock', 'Date'])
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå…ƒã®æ¡ä»¶ï¼šç¿Œæ—¥1%ä»¥ä¸Šä¸Šæ˜‡ï¼‰
        enhanced_df['next_high'] = enhanced_df.groupby('Stock')['high'].shift(-1)
        enhanced_df['Target'] = (enhanced_df['next_high'] > enhanced_df['close'] * 1.01).astype(int)
        
        # æˆåŠŸæ™‚ã®ç‰¹å¾´é‡å†ç¾
        for stock, stock_df in enhanced_df.groupby('Stock'):
            stock_mask = enhanced_df['Stock'] == stock
            stock_data = enhanced_df[stock_mask].sort_values('Date')
            
            if len(stock_data) < 50:
                continue
            
            # RSI
            delta = stock_data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss.replace(0, 1)
            enhanced_df.loc[stock_mask, 'Enhanced_RSI'] = 100 - (100 / (1 + rs))
            
            # ç§»å‹•å¹³å‡
            enhanced_df.loc[stock_mask, 'MA_Cross_Signal'] = (
                stock_data['close'].rolling(5).mean() > stock_data['close'].rolling(20).mean()
            ).astype(int)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            returns = stock_data['close'].pct_change()
            enhanced_df.loc[stock_mask, 'Enhanced_Volatility'] = returns.rolling(20).std()
            
            # å‡ºæ¥é«˜
            volume_ma = stock_data['volume'].rolling(20).mean()
            enhanced_df.loc[stock_mask, 'Enhanced_Volume_Ratio'] = stock_data['volume'] / volume_ma
        
        enhanced_df = enhanced_df.fillna(method='ffill').fillna(0)
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        logger.success("âœ… æ¤œè¨¼ç”¨ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
        return enhanced_df
    
    def time_series_validation(self, df: pd.DataFrame) -> dict:
        """æ™‚ç³»åˆ—ä¿¡é ¼æ€§æ¤œè¨¼ï¼ˆå®Ÿé‹ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        logger.info("ğŸ“Š æ™‚ç³»åˆ—ä¿¡é ¼æ€§æ¤œè¨¼å®Ÿè¡Œä¸­...")
        
        df_sorted = df.sort_values(['Stock', 'Date'])
        unique_dates = sorted(df_sorted['Date'].unique())
        
        # è¤‡æ•°æœŸé–“ã§ã®æ¤œè¨¼
        validation_periods = [
            ('æœ€æ–°10æ—¥', unique_dates[-10:]),
            ('æœ€æ–°20æ—¥', unique_dates[-20:]),
            ('æœ€æ–°30æ—¥', unique_dates[-30:]),
            ('æœ€æ–°60æ—¥', unique_dates[-60:])
        ]
        
        results = {}
        
        # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´é‡
        feature_cols = [col for col in df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']]
        feature_cols = [col for col in feature_cols if df[col].dtype in ['int64', 'float64']][:20]  # ä¸Šä½20ç‰¹å¾´é‡
        
        for period_name, test_dates in validation_periods:
            logger.info(f"  æ¤œè¨¼ä¸­: {period_name}")
            
            daily_precisions = []
            all_predictions = []
            all_actuals = []
            confidence_scores = []
            
            for test_date in test_dates:
                # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆæ—¥ã‚ˆã‚Šå‰ï¼‰
                train_data = df_sorted[df_sorted['Date'] < test_date]
                test_data = df_sorted[df_sorted['Date'] == test_date]
                
                train_clean = train_data.dropna(subset=['Target'] + feature_cols)
                test_clean = test_data.dropna(subset=['Target'] + feature_cols)
                
                if len(train_clean) < 500 or len(test_clean) < 3:
                    continue
                
                X_train = train_clean[feature_cols]
                y_train = train_clean['Target']
                X_test = test_clean[feature_cols]
                y_test = test_clean['Target']
                
                # ç‰¹å¾´é‡é¸æŠï¼ˆæˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³å†ç¾ï¼‰
                selector = SelectKBest(score_func=f_classif, k=min(12, len(feature_cols)))
                X_train_selected = selector.fit_transform(X_train, y_train)
                X_test_selected = selector.transform(X_test)
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train_selected)
                X_test_scaled = scaler.transform(X_test_selected)
                
                # æˆåŠŸãƒ¢ãƒ‡ãƒ«å†ç¾
                model = lgb.LGBMClassifier(
                    n_estimators=150,
                    max_depth=4,
                    learning_rate=0.08,
                    subsample=0.85,
                    colsample_bytree=0.85,
                    random_state=42,
                    verbose=-1
                )
                
                model.fit(X_train_scaled, y_train)
                probs = model.predict_proba(X_test_scaled)[:, 1]
                
                # ä¸Šä½3éŠ˜æŸ„é¸æŠï¼ˆæˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                n_select = min(3, len(probs))
                top_indices = np.argsort(probs)[-n_select:]
                
                selected_actuals = y_test.iloc[top_indices].values
                selected_probs = probs[top_indices]
                
                all_predictions.extend([1] * len(selected_actuals))
                all_actuals.extend(selected_actuals)
                confidence_scores.extend(selected_probs)
                
                # æ—¥åˆ¥ç²¾åº¦
                if len(selected_actuals) > 0:
                    daily_precision = sum(selected_actuals) / len(selected_actuals)
                    daily_precisions.append(daily_precision)
            
            # æœŸé–“åˆ¥çµæœ
            if all_predictions:
                overall_precision = sum(all_actuals) / len(all_actuals)
                precision_std = np.std(daily_precisions) if daily_precisions else 0
                avg_confidence = np.mean(confidence_scores)
                consistency = (np.array(daily_precisions) >= 0.5).mean() if daily_precisions else 0
                
                results[period_name] = {
                    'precision': overall_precision,
                    'precision_std': precision_std,
                    'avg_confidence': avg_confidence,
                    'consistency_rate': consistency,
                    'total_selections': len(all_predictions),
                    'daily_precisions': daily_precisions
                }
        
        return results
    
    def robustness_validation(self, df: pd.DataFrame) -> dict:
        """é ‘å¥æ€§æ¤œè¨¼ï¼ˆå¸‚å ´ç’°å¢ƒåˆ¥æ€§èƒ½ï¼‰"""
        logger.info("ğŸ›¡ï¸ é ‘å¥æ€§æ¤œè¨¼å®Ÿè¡Œä¸­...")
        
        df_sorted = df.sort_values(['Stock', 'Date'])
        
        # VIXæ°´æº–åˆ¥æ€§èƒ½æ¤œè¨¼
        vix_scenarios = []
        if 'vix_close' in df.columns:
            df['vix_regime'] = pd.cut(df['vix_close'], 
                                    bins=[0, 20, 30, 100], 
                                    labels=['ä½VIX', 'ä¸­VIX', 'é«˜VIX'])
            vix_scenarios = ['ä½VIX', 'ä¸­VIX', 'é«˜VIX']
        
        # å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¥æ€§èƒ½æ¤œè¨¼  
        market_scenarios = []
        if 'nikkei225_return_1d' in df.columns:
            df['market_trend'] = pd.cut(df['nikkei225_return_1d'], 
                                      bins=[-np.inf, -0.01, 0.01, np.inf], 
                                      labels=['ä¸‹è½', 'æ¨ªã°ã„', 'ä¸Šæ˜‡'])
            market_scenarios = ['ä¸‹è½', 'æ¨ªã°ã„', 'ä¸Šæ˜‡']
        
        scenarios = vix_scenarios + market_scenarios
        scenario_results = {}
        
        feature_cols = [col for col in df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']]
        feature_cols = [col for col in feature_cols if df[col].dtype in ['int64', 'float64']][:15]
        
        for scenario in scenarios:
            if scenario in ['ä½VIX', 'ä¸­VIX', 'é«˜VIX']:
                scenario_data = df[df['vix_regime'] == scenario]
            else:
                scenario_data = df[df['market_trend'] == scenario]
            
            if len(scenario_data) < 100:
                continue
            
            logger.info(f"  æ¤œè¨¼ä¸­: {scenario}ç’°å¢ƒ")
            
            # ç›´è¿‘ãƒ†ã‚¹ãƒˆ
            unique_dates = sorted(scenario_data['Date'].unique())
            test_dates = unique_dates[-min(10, len(unique_dates)):]
            
            all_preds = []
            all_actuals = []
            
            for test_date in test_dates:
                train_data = scenario_data[scenario_data['Date'] < test_date]
                test_data = scenario_data[scenario_data['Date'] == test_date]
                
                train_clean = train_data.dropna(subset=['Target'] + feature_cols)
                test_clean = test_data.dropna(subset=['Target'] + feature_cols)
                
                if len(train_clean) < 100 or len(test_clean) < 2:
                    continue
                
                X_train = train_clean[feature_cols]
                y_train = train_clean['Target']
                X_test = test_clean[feature_cols]
                y_test = test_clean['Target']
                
                try:
                    scaler = RobustScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    
                    model = lgb.LGBMClassifier(
                        n_estimators=100,
                        max_depth=4,
                        learning_rate=0.08,
                        random_state=42,
                        verbose=-1
                    )
                    
                    model.fit(X_train_scaled, y_train)
                    probs = model.predict_proba(X_test_scaled)[:, 1]
                    
                    # ä¸Šä½2éŠ˜æŸ„
                    n_select = min(2, len(probs))
                    top_indices = np.argsort(probs)[-n_select:]
                    
                    selected_actuals = y_test.iloc[top_indices].values
                    all_preds.extend([1] * len(selected_actuals))
                    all_actuals.extend(selected_actuals)
                    
                except Exception as e:
                    continue
            
            if all_preds:
                scenario_precision = sum(all_actuals) / len(all_actuals)
                scenario_results[scenario] = {
                    'precision': scenario_precision,
                    'sample_size': len(all_preds)
                }
        
        return scenario_results
    
    def statistical_validation(self, df: pd.DataFrame) -> dict:
        """çµ±è¨ˆçš„ä¿¡é ¼æ€§æ¤œè¨¼"""
        logger.info("ğŸ“ˆ çµ±è¨ˆçš„ä¿¡é ¼æ€§æ¤œè¨¼å®Ÿè¡Œä¸­...")
        
        # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ¤œè¨¼ï¼ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        df_sorted = df.sort_values(['Stock', 'Date'])
        unique_dates = sorted(df_sorted['Date'].unique())
        test_dates = unique_dates[-20:]  # æœ€æ–°20æ—¥
        
        bootstrap_precisions = []
        feature_cols = [col for col in df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']]
        feature_cols = [col for col in feature_cols if df[col].dtype in ['int64', 'float64']][:15]
        
        # 10å›ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—
        for bootstrap_iter in range(10):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¾©å…ƒæŠ½å‡ºï¼‰
            sampled_dates = np.random.choice(test_dates, size=len(test_dates), replace=True)
            
            all_preds = []
            all_actuals = []
            
            for test_date in sampled_dates:
                train_data = df_sorted[df_sorted['Date'] < test_date]
                test_data = df_sorted[df_sorted['Date'] == test_date]
                
                train_clean = train_data.dropna(subset=['Target'] + feature_cols)
                test_clean = test_data.dropna(subset=['Target'] + feature_cols)
                
                if len(train_clean) < 500 or len(test_clean) < 2:
                    continue
                
                try:
                    X_train = train_clean[feature_cols]
                    y_train = train_clean['Target']
                    X_test = test_clean[feature_cols]
                    y_test = test_clean['Target']
                    
                    scaler = RobustScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    
                    model = lgb.LGBMClassifier(
                        n_estimators=150,
                        max_depth=4,
                        learning_rate=0.08,
                        subsample=0.85,
                        colsample_bytree=0.85,
                        random_state=42 + bootstrap_iter,
                        verbose=-1
                    )
                    
                    model.fit(X_train_scaled, y_train)
                    probs = model.predict_proba(X_test_scaled)[:, 1]
                    
                    n_select = min(3, len(probs))
                    top_indices = np.argsort(probs)[-n_select:]
                    
                    selected_actuals = y_test.iloc[top_indices].values
                    all_preds.extend([1] * len(selected_actuals))
                    all_actuals.extend(selected_actuals)
                    
                except Exception as e:
                    continue
            
            if all_preds:
                bootstrap_precision = sum(all_actuals) / len(all_actuals)
                bootstrap_precisions.append(bootstrap_precision)
        
        # çµ±è¨ˆåˆ†æ
        if bootstrap_precisions:
            mean_precision = np.mean(bootstrap_precisions)
            precision_std = np.std(bootstrap_precisions)
            confidence_95_lower = np.percentile(bootstrap_precisions, 2.5)
            confidence_95_upper = np.percentile(bootstrap_precisions, 97.5)
            
            return {
                'bootstrap_mean': mean_precision,
                'bootstrap_std': precision_std,
                'confidence_95_lower': confidence_95_lower,
                'confidence_95_upper': confidence_95_upper,
                'precision_distribution': bootstrap_precisions
            }
        
        return {}
    
    def run_comprehensive_validation(self):
        """åŒ…æ‹¬çš„ä¿¡é ¼æ€§æ¤œè¨¼å®Ÿè¡Œ"""
        logger.info("ğŸ” åŒ…æ‹¬çš„ä¿¡é ¼æ€§æ¤œè¨¼é–‹å§‹")
        print("ğŸ¯ 83.33%ç²¾åº¦ã®å®Ÿé‹ç”¨ä¿¡é ¼æ€§æ¤œè¨¼")
        print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df = self.load_validation_data()
        if df.empty:
            print("âŒ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—")
            return False
        
        # ç‰¹å¾´é‡ç”Ÿæˆ
        df_enhanced = self.create_validation_features(df)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
        target_rate = df_enhanced['Target'].mean()
        print(f"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
        print(f"  ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_enhanced):,}")
        print(f"  å¯¾è±¡éŠ˜æŸ„æ•°: {df_enhanced['Stock'].nunique()}")
        print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé™½æ€§ç‡: {target_rate:.2%}")
        print()
        
        # 1. æ™‚ç³»åˆ—ä¿¡é ¼æ€§æ¤œè¨¼
        print("ğŸ“Š 1. æ™‚ç³»åˆ—ä¿¡é ¼æ€§æ¤œè¨¼")
        print("-"*40)
        time_results = self.time_series_validation(df_enhanced)
        
        baseline_precision = 0.8333  # æˆåŠŸæ™‚ã®ç²¾åº¦
        reliable_periods = 0
        
        for period, result in time_results.items():
            precision = result['precision']
            precision_std = result['precision_std']
            consistency = result['consistency_rate']
            
            # ä¿¡é ¼æ€§åˆ¤å®š
            if precision >= 0.75:  # 75%ä»¥ä¸Š
                reliability = "ğŸŸ¢ é«˜ä¿¡é ¼æ€§"
                if precision >= baseline_precision * 0.9:  # 90%ä»¥ä¸Šã®å†ç¾ç‡
                    reliable_periods += 1
            elif precision >= 0.65:  # 65%ä»¥ä¸Š
                reliability = "ğŸŸ¡ ä¸­ä¿¡é ¼æ€§"
            else:
                reliability = "ğŸ”´ ä½ä¿¡é ¼æ€§"
            
            print(f"{period:>8}: {precision:6.1%} (Â±{precision_std:.1%}) "
                  f"ä¸€è²«æ€§:{consistency:4.1%} {reliability}")
        
        # 2. é ‘å¥æ€§æ¤œè¨¼
        print(f"\\nğŸ›¡ï¸ 2. å¸‚å ´ç’°å¢ƒåˆ¥é ‘å¥æ€§æ¤œè¨¼")
        print("-"*40)
        robustness_results = self.robustness_validation(df_enhanced)
        
        robust_scenarios = 0
        for scenario, result in robustness_results.items():
            precision = result['precision']
            sample_size = result['sample_size']
            
            if precision >= 0.70:
                robustness = "ğŸŸ¢ é ‘å¥"
                robust_scenarios += 1
            elif precision >= 0.60:
                robustness = "ğŸŸ¡ ã‚„ã‚„é ‘å¥"
            else:
                robustness = "ğŸ”´ ä¸å®‰å®š"
            
            print(f"{scenario:>8}: {precision:6.1%} (n={sample_size:3d}) {robustness}")
        
        # 3. çµ±è¨ˆçš„ä¿¡é ¼æ€§æ¤œè¨¼
        print(f"\\nğŸ“ˆ 3. çµ±è¨ˆçš„ä¿¡é ¼æ€§æ¤œè¨¼ï¼ˆãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼‰")
        print("-"*40)
        stats_results = self.statistical_validation(df_enhanced)
        
        if stats_results:
            mean_precision = stats_results['bootstrap_mean']
            precision_std = stats_results['bootstrap_std']
            conf_lower = stats_results['confidence_95_lower']
            conf_upper = stats_results['confidence_95_upper']
            
            print(f"å¹³å‡ç²¾åº¦: {mean_precision:.1%} (Â±{precision_std:.1%})")
            print(f"95%ä¿¡é ¼åŒºé–“: [{conf_lower:.1%}, {conf_upper:.1%}]")
            
            # ä¿¡é ¼åŒºé–“ãŒ60%ä»¥ä¸Šã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            confidence_60_above = conf_lower >= 0.60
            if confidence_60_above:
                statistical_reliability = "ğŸŸ¢ çµ±è¨ˆçš„ã«é«˜ä¿¡é ¼"
            else:
                statistical_reliability = "ğŸŸ¡ çµ±è¨ˆçš„ã«è¦æ³¨æ„"
            
            print(f"çµ±è¨ˆçš„ä¿¡é ¼æ€§: {statistical_reliability}")
        
        # ç·åˆåˆ¤å®š
        print(f"\\nğŸ¯ ç·åˆä¿¡é ¼æ€§è©•ä¾¡")
        print("="*60)
        
        # è©•ä¾¡åŸºæº–
        total_checks = 4
        passed_checks = 0
        
        # ãƒã‚§ãƒƒã‚¯1: æ™‚ç³»åˆ—å®‰å®šæ€§
        stable_ratio = reliable_periods / len(time_results) if time_results else 0
        if stable_ratio >= 0.5:  # 50%ä»¥ä¸Šã®æœŸé–“ã§å®‰å®š
            print("âœ… æ™‚ç³»åˆ—å®‰å®šæ€§: åˆæ ¼ï¼ˆè¤‡æ•°æœŸé–“ã§75%ä»¥ä¸Šç¶­æŒï¼‰")
            passed_checks += 1
        else:
            print("âŒ æ™‚ç³»åˆ—å®‰å®šæ€§: ä¸åˆæ ¼ï¼ˆå®‰å®šæ€§ä¸è¶³ï¼‰")
        
        # ãƒã‚§ãƒƒã‚¯2: å¸‚å ´ç’°å¢ƒè€æ€§
        robust_ratio = robust_scenarios / len(robustness_results) if robustness_results else 0
        if robust_ratio >= 0.6:  # 60%ä»¥ä¸Šã®ç’°å¢ƒã§é ‘å¥
            print("âœ… å¸‚å ´ç’°å¢ƒè€æ€§: åˆæ ¼ï¼ˆå¤šæ§˜ãªç’°å¢ƒã§70%ä»¥ä¸Šï¼‰")
            passed_checks += 1
        else:
            print("âŒ å¸‚å ´ç’°å¢ƒè€æ€§: ä¸åˆæ ¼ï¼ˆç’°å¢ƒä¾å­˜æ€§å¤§ï¼‰")
        
        # ãƒã‚§ãƒƒã‚¯3: çµ±è¨ˆçš„ä¿¡é ¼æ€§
        if stats_results and stats_results.get('confidence_95_lower', 0) >= 0.65:
            print("âœ… çµ±è¨ˆçš„ä¿¡é ¼æ€§: åˆæ ¼ï¼ˆ95%ä¿¡é ¼åŒºé–“ã§65%ä»¥ä¸Šï¼‰")
            passed_checks += 1
        else:
            print("âŒ çµ±è¨ˆçš„ä¿¡é ¼æ€§: ä¸åˆæ ¼ï¼ˆä¿¡é ¼åŒºé–“ãŒä½ã„ï¼‰")
        
        # ãƒã‚§ãƒƒã‚¯4: å®Ÿç”¨æ€§
        if time_results:
            recent_precision = time_results.get('æœ€æ–°10æ—¥', {}).get('precision', 0)
            if recent_precision >= 0.70:
                print("âœ… å®Ÿç”¨æ€§: åˆæ ¼ï¼ˆç›´è¿‘10æ—¥ã§70%ä»¥ä¸Šï¼‰")
                passed_checks += 1
            else:
                print("âŒ å®Ÿç”¨æ€§: ä¸åˆæ ¼ï¼ˆç›´è¿‘æ€§èƒ½ä½ä¸‹ï¼‰")
        
        # æœ€çµ‚åˆ¤å®š
        reliability_score = passed_checks / total_checks
        
        print(f"\\nğŸ“Š ã€æœ€çµ‚ä¿¡é ¼æ€§è©•ä¾¡ã€‘")
        print(f"ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢: {passed_checks}/{total_checks} ({reliability_score:.1%})")
        
        if reliability_score >= 0.75:
            final_verdict = "ğŸŸ¢ é«˜ä¿¡é ¼æ€§ - å®Ÿé‹ç”¨æ¨å¥¨"
            recommendation = "å®Ÿé‹ç”¨ã«é©ç”¨å¯èƒ½ã§ã™"
        elif reliability_score >= 0.5:
            final_verdict = "ğŸŸ¡ ä¸­ä¿¡é ¼æ€§ - è¦æ³¨æ„é‹ç”¨"
            recommendation = "ãƒªã‚¹ã‚¯ç®¡ç†ã‚’å¼·åŒ–ã—ã¦é‹ç”¨"
        else:
            final_verdict = "ğŸ”´ ä½ä¿¡é ¼æ€§ - é‹ç”¨éæ¨å¥¨"
            recommendation = "ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦"
        
        print(f"ç·åˆåˆ¤å®š: {final_verdict}")
        print(f"æ¨å¥¨äº‹é …: {recommendation}")
        
        # ä¿¡é ¼æ€§ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open('reliability_validation_report.txt', 'w') as f:
            f.write("å®Ÿé‹ç”¨ä¿¡é ¼æ€§æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ\\n")
            f.write("="*50 + "\\n\\n")
            f.write(f"æ¤œè¨¼æ—¥æ™‚: {datetime.now()}\\n")
            f.write(f"å¯¾è±¡ç²¾åº¦: 83.33%\\n")
            f.write(f"ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢: {reliability_score:.1%}\\n")
            f.write(f"æœ€çµ‚åˆ¤å®š: {final_verdict}\\n")
            f.write(f"æ¨å¥¨äº‹é …: {recommendation}\\n\\n")
            
            f.write("è©³ç´°çµæœ:\\n")
            f.write(f"æ™‚ç³»åˆ—å®‰å®šæ€§: {stable_ratio:.1%}ã®æœŸé–“ã§å®‰å®š\\n")
            f.write(f"å¸‚å ´ç’°å¢ƒè€æ€§: {robust_ratio:.1%}ã®ç’°å¢ƒã§é ‘å¥\\n")
            if stats_results:
                f.write(f"çµ±è¨ˆçš„ä¿¡é ¼åŒºé–“: [{stats_results['confidence_95_lower']:.1%}, {stats_results['confidence_95_upper']:.1%}]\\n")
        
        print("\\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: reliability_validation_report.txt")
        
        return reliability_score >= 0.5

# å®Ÿè¡Œ
if __name__ == "__main__":
    validator = ReliabilityValidator()
    reliable = validator.run_comprehensive_validation()
    
    if reliable:
        print("\\nğŸ‰ å®Ÿé‹ç”¨ã«é©ã—ãŸä¿¡é ¼æ€§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸï¼")
    else:
        print("\\nâš ï¸ å®Ÿé‹ç”¨å‰ã«ã•ã‚‰ãªã‚‹æ¤œè¨¼ãƒ»æ”¹å–„ãŒæ¨å¥¨ã•ã‚Œã¾ã™")