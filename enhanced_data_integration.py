#!/usr/bin/env python3
"""
æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ + ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ« + ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
60%ç²¾åº¦é”æˆã‚’ç›®æŒ‡ã™
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

from jquants_auth import JQuantsAuth
from jquants_fundamental import JQuantsFundamental
from yahoo_market_data import YahooMarketData

class EnhancedDataIntegration:
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_data_file = "data/processed/integrated_with_external.parquet"
        self.output_file = "data/processed/enhanced_integrated_data.parquet"
        
    def load_base_data(self) -> pd.DataFrame:
        """æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.base_data_file).exists():
                df = pd.read_parquet(self.base_data_file)
                logger.success(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}ä»¶")
                
                # ã‚«ãƒ©ãƒ çµ±ä¸€
                if 'date' in df.columns and 'Date' not in df.columns:
                    df['Date'] = pd.to_datetime(df['date'])
                if 'code' in df.columns and 'Stock' not in df.columns:
                    df['Stock'] = df['code'].astype(str)
                
                return df
            else:
                logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.base_data_file}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def get_stock_list(self, base_df: pd.DataFrame, limit: int = 200) -> list:
        """å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆä¸»è¦éŠ˜æŸ„ã«é™å®šï¼‰"""
        if base_df.empty:
            return []
        
        # éŠ˜æŸ„åˆ¥ã®ãƒ‡ãƒ¼ã‚¿é‡ã‚’ç¢ºèª
        stock_counts = base_df['Stock'].value_counts()
        
        # ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã«ã‚ã‚‹éŠ˜æŸ„ã‚’é¸æŠï¼ˆæœ€ä½100æ—¥ä»¥ä¸Šï¼‰
        valid_stocks = stock_counts[stock_counts >= 100].head(limit).index.tolist()
        
        logger.info(f"å¯¾è±¡éŠ˜æŸ„é¸æŠ: {len(valid_stocks)}éŠ˜æŸ„ï¼ˆãƒ‡ãƒ¼ã‚¿ååˆ†ãªéŠ˜æŸ„ã‹ã‚‰é¸æŠï¼‰")
        return valid_stocks
    
    def integrate_fundamental_data(self, base_df: pd.DataFrame, stock_list: list) -> pd.DataFrame:
        """ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        logger.info("ğŸ”„ ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        # J-Quantsèªè¨¼
        auth = JQuantsAuth()
        
        if not auth.test_auth():
            logger.warning("J-Quantsèªè¨¼å¤±æ•—ã€ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ãªã—ã§ç¶™ç¶š")
            return base_df
        
        # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
        fundamental = JQuantsFundamental(auth)
        
        # æœŸé–“è¨­å®šï¼ˆéå»2å¹´ï¼‰
        to_date = datetime.now().strftime('%Y-%m-%d')
        from_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')
        
        try:
            # è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆåˆ¶é™ä»˜ãï¼‰
            limited_stocks = stock_list[:50]  # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦50éŠ˜æŸ„ã«åˆ¶é™
            logger.info(f"ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—å¯¾è±¡: {len(limited_stocks)}éŠ˜æŸ„")\n            \n            financial_df = fundamental.get_bulk_financial_data(limited_stocks, from_date, to_date)\n            \n            if financial_df.empty:\n                logger.warning("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")\n                return base_df\n            \n            # ç‰¹å¾´é‡ç”Ÿæˆ\n            features_df = fundamental.process_fundamental_features(financial_df)\n            \n            if features_df.empty:\n                logger.warning("ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´é‡ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")\n                return base_df\n            \n            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸\n            logger.info("ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆä¸­...")\n            \n            # æ—¥ä»˜ç¯„å›²ã‚’èª¿æ•´ï¼ˆãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯å››åŠæœŸå˜ä½ãªã®ã§å‰æ–¹è£œå®Œï¼‰\n            enhanced_df = base_df.copy()\n            \n            for _, fund_row in features_df.iterrows():\n                stock = fund_row['Stock']\n                fund_date = fund_row['Date']\n                \n                # è©²å½“éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n                stock_mask = (enhanced_df['Stock'] == stock) & (enhanced_df['Date'] >= fund_date)\n                \n                if stock_mask.sum() > 0:\n                    # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´é‡ã‚’è¿½åŠ \n                    for col in features_df.columns:\n                        if col not in ['Stock', 'Date']:\n                            enhanced_df.loc[stock_mask, col] = fund_row[col]\n            \n            # ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´é‡ã®å‰æ–¹è£œå®Œ\n            fund_cols = [col for col in features_df.columns if col not in ['Stock', 'Date']]\n            \n            for stock in enhanced_df['Stock'].unique():\n                stock_mask = enhanced_df['Stock'] == stock\n                enhanced_df.loc[stock_mask, fund_cols] = enhanced_df.loc[stock_mask, fund_cols].fillna(method='ffill')\n            \n            # æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ\n            for col in fund_cols:\n                if col in enhanced_df.columns:\n                    enhanced_df[col] = enhanced_df[col].fillna(enhanced_df[col].median())\n            \n            logger.success(f"âœ… ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(fund_cols)}ç‰¹å¾´é‡è¿½åŠ ")\n            return enhanced_df\n            \n        except Exception as e:\n            logger.error(f"âŒ ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆå¤±æ•—: {e}")\n            return base_df\n    \n    def integrate_market_data(self, base_df: pd.DataFrame) -> pd.DataFrame:\n        """ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""\n        logger.info("ğŸ”„ ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")\n        \n        try:\n            # Yahoo Financeã‹ã‚‰ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—\n            market_data = YahooMarketData()\n            data_dict = market_data.get_all_market_data(period="2y")\n            \n            if not data_dict:\n                logger.warning("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")\n                return base_df\n            \n            # ãƒãƒ¼ã‚±ãƒƒãƒˆç‰¹å¾´é‡ç”Ÿæˆ\n            market_features = market_data.calculate_market_features(data_dict)\n            \n            if market_features.empty:\n                logger.warning("ãƒãƒ¼ã‚±ãƒƒãƒˆç‰¹å¾´é‡ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")\n                return base_df\n            \n            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸\n            logger.info("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆä¸­...")\n            \n            # æ—¥ä»˜ã§ãƒãƒ¼ã‚¸\n            enhanced_df = base_df.merge(market_features, on='Date', how='left')\n            \n            # å‰æ–¹è£œå®Œã§æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹\n            market_cols = [col for col in market_features.columns if col != 'Date']\n            enhanced_df[market_cols] = enhanced_df[market_cols].fillna(method='ffill')\n            \n            # æ®‹ã‚Šã®æ¬ æå€¤ã‚’å¾Œæ–¹è£œå®Œ\n            enhanced_df[market_cols] = enhanced_df[market_cols].fillna(method='bfill')\n            \n            # ãã‚Œã§ã‚‚æ®‹ã‚‹æ¬ æå€¤ã‚’0ã§è£œå®Œ\n            enhanced_df[market_cols] = enhanced_df[market_cols].fillna(0)\n            \n            logger.success(f"âœ… ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(market_cols)}ç‰¹å¾´é‡è¿½åŠ ")\n            return enhanced_df\n            \n        except Exception as e:\n            logger.error(f"âŒ ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆå¤±æ•—: {e}")\n            return base_df\n    \n    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        """ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""\n        logger.info("ğŸ”§ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆä¸­...")\n        \n        enhanced_df = df.copy()\n        \n        try:\n            # 1. ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ« Ã— ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«\n            if 'PER' in df.columns and 'RSI' in df.columns:\n                enhanced_df['PER_RSI_interaction'] = df['PER'] * (100 - df['RSI']) / 100  # å‰²å®‰ Ã— å£²ã‚‰ã‚Œéã\n            \n            if 'ROE' in df.columns and 'Price_vs_MA20' in df.columns:\n                enhanced_df['ROE_Momentum_interaction'] = df['ROE'] * df['Price_vs_MA20']  # åç›Šæ€§ Ã— ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ \n            \n            # 2. ãƒãƒ¼ã‚±ãƒƒãƒˆ Ã— å€‹åˆ¥æ ª\n            if 'nikkei225_return_1d' in df.columns and 'Volume_Ratio' in df.columns:\n                enhanced_df['Market_Volume_interaction'] = df['nikkei225_return_1d'] * df['Volume_Ratio']  # å¸‚å ´å‹•å‘ Ã— å‡ºæ¥é«˜\n            \n            if 'vix_close' in df.columns and 'Volatility' in df.columns:\n                enhanced_df['VIX_Stock_Vol_ratio'] = df['vix_close'] / (df['Volatility'] * 100 + 1)  # å¸‚å ´ææ€– / å€‹åˆ¥ãƒœãƒ©\n            \n            # 3. ã‚»ã‚¯ã‚¿ãƒ¼ç›¸å¯¾å¼·åº¦ï¼ˆä»®æƒ³ï¼‰\n            if 'PBR' in df.columns and 'topix_return_1d' in df.columns:\n                enhanced_df['Value_Market_sync'] = (1 / (df['PBR'] + 0.1)) * df['topix_return_1d']  # ãƒãƒªãƒ¥ãƒ¼ Ã— å¸‚å ´\n            \n            # 4. ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚©ãƒ­ãƒ¼å¼·åº¦\n            if 'Momentum_5' in df.columns and 'nikkei225_ma20_slope' in df.columns:\n                enhanced_df['Trend_Alignment'] = np.sign(df['Momentum_5']) * np.sign(df['nikkei225_ma20_slope'])  # ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è‡´\n            \n            # 5. ãƒªã‚¹ã‚¯èª¿æ•´ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬\n            if 'ROE' in df.columns and 'vix_close' in df.columns:\n                enhanced_df['Risk_Adjusted_Quality'] = df['ROE'] * np.exp(-df['vix_close'] / 100)  # å“è³ª Ã— ãƒªã‚¹ã‚¯èª¿æ•´\n            \n            interaction_cols = len([col for col in enhanced_df.columns if 'interaction' in col.lower() or 'sync' in col.lower() or 'alignment' in col.lower() or 'adjusted' in col.lower()])\n            \n            logger.success(f"âœ… ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {interaction_cols}ç‰¹å¾´é‡è¿½åŠ ")\n            \n        except Exception as e:\n            logger.warning(f"ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {e}")\n        \n        return enhanced_df\n    \n    def finalize_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        """æœ€çµ‚çš„ãªç‰¹å¾´é‡èª¿æ•´"""\n        logger.info("ğŸ¯ æœ€çµ‚ç‰¹å¾´é‡èª¿æ•´ä¸­...")\n        \n        # å¿…è¦ãªåˆ—ã®ç¢ºèª\n        required_base_cols = ['Date', 'Stock', 'close', 'high', 'low', 'open', 'volume']\n        missing_cols = [col for col in required_base_cols if col not in df.columns]\n        \n        if missing_cols:\n            logger.error(f"å¿…è¦ãªåŸºæœ¬åˆ—ãŒä¸è¶³: {missing_cols}")\n            return df\n        \n        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ç”Ÿæˆ\n        df = df.sort_values(['Stock', 'Date'])\n        \n        # ç¿Œæ—¥ã®é«˜å€¤ãŒå½“æ—¥çµ‚å€¤ã‚ˆã‚Š1%ä»¥ä¸Šé«˜ã„å ´åˆã‚’äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ\n        df['next_high'] = df.groupby('Stock')['high'].shift(-1)\n        df['Target'] = (df['next_high'] > df['close'] * 1.01).astype(int)\n        \n        # ç•°å¸¸å€¤å‡¦ç†\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        \n        for col in numeric_cols:\n            if col not in ['Target', 'Date']:\n                # 99%ã¨1%ã§ç•°å¸¸å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—\n                q99 = df[col].quantile(0.99)\n                q01 = df[col].quantile(0.01)\n                df[col] = df[col].clip(q01, q99)\n        \n        # ç„¡é™å¤§å€¤ã‚’NaNã«å¤‰æ›\n        df = df.replace([np.inf, -np.inf], np.nan)\n        \n        # æœ€çµ‚çš„ãªæ¬ æå€¤å‡¦ç†\n        df = df.fillna(method='ffill').fillna(0)\n        \n        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯\n        total_features = len([col for col in df.columns if col not in ['Date', 'Stock', 'Target', 'next_high']]) \n        target_count = df['Target'].sum()\n        target_rate = target_count / len(df) * 100\n        \n        logger.success(f"âœ… æœ€çµ‚ç‰¹å¾´é‡èª¿æ•´å®Œäº†:")\n        logger.info(f"  ç·ç‰¹å¾´é‡æ•°: {total_features}")\n        logger.info(f"  ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}")\n        logger.info(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé™½æ€§ç‡: {target_rate:.2f}%")\n        \n        return df\n    \n    def run_integration(self) -> pd.DataFrame:\n        """çµ±åˆå‡¦ç†å®Ÿè¡Œ"""\n        logger.info("ğŸš€ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")\n        \n        # 1. ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n        base_df = self.load_base_data()\n        if base_df.empty:\n            logger.error("ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")\n            return pd.DataFrame()\n        \n        # 2. å¯¾è±¡éŠ˜æŸ„é¸æŠ\n        stock_list = self.get_stock_list(base_df)\n        if not stock_list:\n            logger.error("å¯¾è±¡éŠ˜æŸ„ãŒé¸æŠã§ãã¾ã›ã‚“ã§ã—ãŸ")\n            return pd.DataFrame()\n        \n        # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¯¾è±¡éŠ˜æŸ„ã«é™å®š\n        base_df = base_df[base_df['Stock'].isin(stock_list)].copy()\n        \n        # 3. ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆ\n        enhanced_df = self.integrate_fundamental_data(base_df, stock_list)\n        \n        # 4. ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿çµ±åˆ\n        enhanced_df = self.integrate_market_data(enhanced_df)\n        \n        # 5. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ç”Ÿæˆ\n        enhanced_df = self.create_interaction_features(enhanced_df)\n        \n        # 6. æœ€çµ‚èª¿æ•´\n        final_df = self.finalize_features(enhanced_df)\n        \n        if not final_df.empty:\n            # ä¿å­˜\n            try:\n                final_df.to_parquet(self.output_file)\n                logger.success(f"âœ… æ‹¡å¼µçµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {self.output_file}")\n                \n                # çµ±è¨ˆã‚µãƒãƒªãƒ¼\n                feature_cols = [col for col in final_df.columns if col not in ['Date', 'Stock', 'Target']]\n                logger.info("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")\n                logger.info(f"  æœŸé–“: {final_df['Date'].min()} ï½ {final_df['Date'].max()}")\n                logger.info(f"  éŠ˜æŸ„æ•°: {final_df['Stock'].nunique()}")\n                logger.info(f"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}")\n                logger.info(f"  ãƒ‡ãƒ¼ã‚¿å“è³ª: {(1 - final_df.isnull().sum().sum() / (len(final_df) * len(final_df.columns))) * 100:.1f}%")\n                \n            except Exception as e:\n                logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: {e}")\n        \n        return final_df

# å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    integrator = EnhancedDataIntegration()
    enhanced_data = integrator.run_integration()
    
    if not enhanced_data.empty:
        logger.success("ğŸ‰ æ‹¡å¼µãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†ï¼60%ç²¾åº¦å‘ä¸Šã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ")
    else:
        logger.error("âš ï¸ çµ±åˆå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")